{"version":"1","records":[{"hierarchy":{"lvl1":"About me"},"type":"lvl1","url":"/about","position":0},{"hierarchy":{"lvl1":"About me"},"content":"I’m a physicist turned open-source software engineer. I work on projects that make it easier to analyse and share massive scientific datasets.\n\nI strongly feel that progress in most fields of computational science is bottlenecked by how uneccessarily hard that is right now. There are \n\ncrucial missing pieces of global scientific infrastructure.\n\nClimate science, meteorology, and all downstream impact analyses are similarly hamstrung, and I hope my work contributes in some small way to addressing the global climate crisis by helping enable them.","type":"content","url":"/about","position":1},{"hierarchy":{"lvl1":"About me","lvl2":"What I do"},"type":"lvl2","url":"/about#what-i-do","position":2},{"hierarchy":{"lvl1":"About me","lvl2":"What I do"},"content":"Open Source Software and Infrastructure for Science\n\nI contribute to a number of open-source software projects that support this aim, often as part of the \n\nPangeo Community.\n\nA few projects I’m particularly proud of or excited about, and my role in them:\n\nXarray - N-D labeled arrays and datasets in Python (core maintainer).\n\nVirtualiZarr - Cloud-Optimizes your Scientific Data as Virtual Zarr stores, using Xarray syntax (original author and lead developer).\n\nCubed - Scalable array processing with bounded memory (cheerleader, and author of the \n\nCubed-Xarray integration).\n\nFROST - Federated registry of all scientific datasets (originator - I’m trying to make this a thing).\n\nCheck out my \n\nmy GitHub page for more details.\n\nScientific Research Dilettante\n\nI have at some point somehow been involved in research on or written peer-reviewed pieces on a wide range of topics, including:\n\nNuclear Fusion Plasma Physics\n\nEconomics of Fusion Reactors\n\nPhysical Oceanography\n\nOcean-based Carbon Dioxide Removal\n\nClimate Science\n\nSuperconductivity\n\nHypersonic Aerothermodynamics (Spacecraft Re-entry)\n\nSeismology\n\nI also regularly interact with researchers in all sort of fields of science, from biology to social science to machine learning.\n\nIn every field I see the same kinds of pain around doing computational work, which motivates my software projects.\n\nFor a list of publications and scholarly artifacts in which I’ve been involved,\ncheck out \n\nmy ORCID page or \n\nmy Google Scholar page.","type":"content","url":"/about#what-i-do","position":3},{"hierarchy":{"lvl1":"About me","lvl2":"About this website"},"type":"lvl2","url":"/about#about-this-website","position":4},{"hierarchy":{"lvl1":"About me","lvl2":"About this website"},"content":"This website is my fork of \n\nChris Holdgraf’s experiment in hosting a personal website and blog via Sphinx extensions instead of using Jekyll. All credit for the website should go to him.","type":"content","url":"/about#about-this-website","position":5},{"hierarchy":{"lvl1":"About me","lvl2":"A rough timeline"},"type":"lvl2","url":"/about#about-timeline","position":6},{"hierarchy":{"lvl1":"About me","lvl2":"A rough timeline"},"content":"Below is a rough timeline of my working life so far.\n\n2025- : Engineer at Earthmover\n\nI joined \n\nEarthmover to be able to work full-time on improving tools for science. The open-sourcing of \n\nIcechunk was a major catalyst for this move.\n\nI’m hoping we can build something like my \n\nvision of a social network for sharing scientific data.\n\n2023-2025: Staff Scientist at [C]Worthy\n\nAfter Ryan wound his lab down, I looked for places to apply my open-source skills in the climate space, and found \n\n[C]Worthy. Their non-profit “Focused Research Organisation” status is a very cool model.\n\nI helped build some \n\ncool stuff, but I felt generally frustrated by the lack of powerful tools for working with scientific data in an operational context.\n\n2021-2023: Oceanographer at Columbia University\n\nAfter meeting Ryan Abernathey through the Xarray core development team, he invited me to come work with him at Columbia. This seemed like an ideal way to continue doing open-source development whilst pivoting towards more climate-ish stuff.\n\nThis meant a move to New York, and a pivot to physical oceanography research. Luckily it turned out ocean turbulence is surprisingly similar to plasma turbulence!\n\n2019: Became worried about Climate Change\n\nThere was a clear moment when I realized exactly how big and urgent the climate crisis is.\nMany people describe such a “penny drop” moment, and for me it came while watching lectures organized by the \n\nOxford Climate Society.\n\nThrough this excellent student society I had the privilege of meeting and interrogating many climate experts of all kinds, which only cemented my decision to somehow work on climate issues.\n\n2018: First contributions to Xarray\n\nI first heard about \n\nXarray during my PhD, and immediately started \n\nusing it to analyse my plasma physics simulation data.\n\nTo get this to work I began making upstream contributions. One of my \n\nfirst big contributions was generalizing xarray.open_mfdataset to work on N-dimensional grids of files, which still sees a lot of use via the combine='by_coords'/'nested' options.\n\n2016-2021: PhD at Culham Centre for Fusion Energy\n\nI did big simulations of turbulent plasmas inside magnetically-confinement fusion experiments (particularly MAST-U). I was a student of the University of York as part of the excellent \n\nFusion CDT, but worked at \n\nCulham Centre for Fusion Energy. The simulations generated a lot of netCDF files on HPC...\n\nMy proudest work during my PhD wasn’t physics, but economics: a paper about the \n\n(lack of) future market for fusion power.\n\n2012-2016 Studied Physics at Oxford\n\nGraduated from Oxford University with an MPhys in Physics, specializing in Theoretical and Condensed Matter Physics. Did my Master’s thesis on modelling and data analysis of certain types of novel superconducting materials.","type":"content","url":"/about#about-timeline","position":7},{"hierarchy":{"lvl1":"Blog"},"type":"lvl1","url":"/blog","position":0},{"hierarchy":{"lvl1":"Blog"},"content":"Below are a few of the latest posts in my blog.\nYou can see a full list by year to the left.\n\nFundamentals: What is Cloud-Optimized Scientific Data?\n\nThe article I wish I could have read back when I first heard of Zarr in 2018. Explains how object storage and conventional filesystems are different, and the key properties that make Zarr work so well in cloud object storage.\n\n\nDate: April 17, 2025 | Author: Tom Nicholas | Tags: cloud, zarr, netcdf, earthmover, open-science\n\nScience needs a social network for sharing big data\n\nImagine being able to visit one website, search for any scientific dataset from any institution in the world, preview it, then stream the data out at high speed, in the format you prefer. We have the technology - here's what we should build.\n\n\nDate: January 18, 2025 | Author: Tom Nicholas | Tags: open-science, frost\n\nXarray x NASA: xarray.DataTree for hierarchical data structures\n\nHow xarray's new DataTree feature came about, and thoughts on how public agencies can support the open-source scientific software that they depend on.\n\n\nDate: December 19, 2024 | Author: Tom Nicholas | Tags: code, python, xarray, open-science\n\nCubed: Bounded-memory serverless array processing in xarray\n\nCubed was designed to address the main problems with Dask, so I integrated it with Xarray. \n\n\nDate: June 01, 2023 | Author: Tom Nicholas | Tags: code, python, xarray, dask, cubed, open-science\n\nDask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration\n\nDask's distributed scheduler algorithm got a major fix after we tested its' limits on a huge oceanography analysis problem.\n\n\nDate: January 04, 2023 | Author: Tom Nicholas | Tags: code, python, dask, open-science\n\nUnit-aware arithmetic in Xarray, via pint\n\nAll scientific computations involve units, so let's make our analysis software aware of them.\n\n\nDate: August 30, 2022 | Author: Tom Nicholas | Tags: code, python, xarray, pint, open-science, physical-units\n\nEasy IPCC part 1: Multi-Model Datatree\n\nAnalysing CMPI6 data as a motivation for xarray DataTree.\n\n\nDate: August 29, 2022 | Author: Tom Nicholas | Tags: code, python, xarray, open-science, climate-science, datatree\n\nThe faulty science, doomism, and flawed conclusions of Deep Adaptation\n\nCalling out pseudoscience claiming that societal collapse due to climate change is inevitable. \n\n\nDate: July 14, 2020 | Author: Tom Nicholas | Tags: climate-science, misinformation\n\nOxford University Divestment Explained\n\nThought's on Oxford University's fossil fuel divestment motion.\n\n\nDate: April 22, 2020 | Author: Tom Nicholas | Tags: climate, oxford\n\nCoronavirus: The Simplest Model\n\nSolving the simplest possible epidemiological model of the spread of COVID-19.\n\n\nDate: March 20, 2020 | Author: Tom Nicholas | Tags: code, python, modelling, covid-19\n\nNuclear Fusion: too late for the climate\n\nDon't let politicians use funding for nuclear fusion research as greenwashing.\n\n\nDate: September 30, 2019 | Author: Tom Nicholas | Tags: fusion, energy, climate","type":"content","url":"/blog","position":1},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate"},"type":"lvl1","url":"/blog/2019/theconversation-fusion","position":0},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate"},"content":"","type":"content","url":"/blog/2019/theconversation-fusion","position":1},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate","lvl2":"Preface"},"type":"lvl2","url":"/blog/2019/theconversation-fusion#preface","position":2},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate","lvl2":"Preface"},"content":"The post below was originally published on \n\ntheconversation.com, under the title \n\n“Conservatives’ ‘nuclear fusion by 2040’ pledge is wishful thinking”.\nIt came about after my \n\ntwitter rant was picked up. Apparently 15,000 people have read the piece, which is pretty cool.\n\nReally the point is not just about the UK Conservative Party’s policies, but about the trap of techno-optimism as a valid approach to decarbonisation in general.\n\nFusion is merely a clear example of this general problem, where the complexity of the science (and often sloppiness of science communication) allows non-experts to harbour  unjustified levels of optimism about development timescales.\n\nThe Gantt chart above from the \n\nEuroFusion Roadmap (the EU fusion research consortium, and main global effort) lays out the problem. Notice the conspicuous absence of projected dates...","type":"content","url":"/blog/2019/theconversation-fusion#preface","position":3},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate","lvl2":"Conservatives’ ‘nuclear fusion by 2040’ pledge is wishful thinking"},"type":"lvl2","url":"/blog/2019/theconversation-fusion#conservatives-nuclear-fusion-by-2040-pledge-is-wishful-thinking","position":4},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate","lvl2":"Conservatives’ ‘nuclear fusion by 2040’ pledge is wishful thinking"},"content":"The UK’s governing Conservative Party has announced a new package of climate policies, including £220m for research into nuclear fusion reactors to provide clean energy “by 2040”. Although additional funding is welcome news to fusion researchers like me, it isn’t an effective response to climate change.\n\nIt’s easy to see why such a pledge is appealing though. Nuclear fusion is the process that powers stars like our sun. Unlike current nuclear power plants – which split atoms in a process called fission – nuclear fusion binds atomic nuclei together. This releases much more energy than fission and produces no high-level nuclear waste.\n\nA fusion reactor would also produce zero carbon emissions and wouldn’t run the risk of a nuclear meltdown. Fusion could produce energy regardless of wind conditions or daylight hours, and wouldn’t require enriched uranium, which can be repurposed for nuclear weapons.\n\nAs good as this all sounds, nuclear fusion is unlikely to play a major role in fighting climate change. To understand why, we need only look at the current state of fusion research.","type":"content","url":"/blog/2019/theconversation-fusion#conservatives-nuclear-fusion-by-2040-pledge-is-wishful-thinking","position":5},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate","lvl2":"Fusion: for the stars, for now"},"type":"lvl2","url":"/blog/2019/theconversation-fusion#fusion-for-the-stars-for-now","position":6},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate","lvl2":"Fusion: for the stars, for now"},"content":"For decades, the performance of fusion devices has been improving – the capacity of scientists to confine hot hydrogen plasma has improved by a factor of 10,000. This plasma has to be over 100,000,000°C in order for the hydrogen nuclei to fuse and generate energy.\n\nThe next big step is ITER, a huge project involving 35 nations, under construction in the south of France. ITER’s purpose is to test our ability to confine plasma for long enough and at a high enough density and temperature. Its goal is to be the first fusion device to produce more energy from the plasma than is put into it.\n\nBut ITER isn’t a power station, it’s an experiment. The plan is to build a demonstration fusion power plant – called “DEMO” – after ITER shows it’s possible for the plasma to generate a net gain in energy. But ITER isn’t likely to reach this goal until 2035.\n\nThe EU fusion roadmap assumes DEMO comes some time afterwards, likely around 2050 or later. The Tory promise of 2040 is extremely optimistic. A DEMO power plant needs to solve several problems which ITER won’t address. ITER’s power is produced in the form of neutrons which hit the reactor’s internal walls, but DEMO needs to actually turn that power into electricity.\n\nOne of the biggest challenges is that a fusion reactor has to generate some of its own fuel. Two types of hydrogen are needed – deuterium, which is abundant in seawater, and tritium, which is rare on Earth because it decays into helium with a half-life of only 12 years. DEMO will need to combine the neutrons from the plasma with lithium to produce new tritium fuel, in a process called “tritium breeding”.\n\nUnfortunately, prototypes of this crucial technology can’t even be tested until ITER starts producing copious high-energy neutrons, in 2035. The UK has a solid research plan working towards solving all of these problems, but there isn’t much that can be done to accelerate this timeline.","type":"content","url":"/blog/2019/theconversation-fusion#fusion-for-the-stars-for-now","position":7},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate","lvl2":"Failing leadership on climate change"},"type":"lvl2","url":"/blog/2019/theconversation-fusion#failing-leadership-on-climate-change","position":8},{"hierarchy":{"lvl1":"Nuclear Fusion: too late for the climate","lvl2":"Failing leadership on climate change"},"content":"In 2018, the IPCC released their 1.5°C report, which explained that the world must reach net-zero greenhouse gas emissions by 2050 in order to limit future warming to 1.5°C. It’s unlikely that commercial fusion power plants will exist in time for that, and even once a first-of-its-kind DEMO power plant is operational, hundreds would still need to be built to seriously dent global emissions. None of this sits well with the 2040 date the Conservatives have promised.\n\nEven if a new green energy technology like fusion is realised before 2050, that’s far too late for the 1.5°C target anyway. “Net-zero by 2050” assumes that emissions have been constantly decreasing from now until 2050. As it’s the total amount of carbon dioxide in the atmosphere that sets the level of eventual global warming, it’s cumulative emissions that matter.\n\nEven if we could snap our fingers on December 31, 2049 and replace all fossil fuel plants, the world would have already emitted twice as much carbon as the budget allows. Sound climate policy involves cutting emissions as soon as possible, and any further delay makes the task even harder.\n\nAs with any research, nuclear fusion might not work. Despite the best efforts of researchers, it’s still possible that there’s an unforeseen roadblock. But the climate won’t wait for us if we trip up.\n\nAny convincing climate policy requires huge investments across many sectors, from domestic heating to road transport to agriculture. Even a complete decarbonisation of the electricity grid is only one part of the solution. The Committee on Climate Change’s net-zero report laid this all out starkly for policymakers – only mentioning nuclear fusion once in 227 pages. The idea of a single panacea is simplistic fantasy.\n\nSo why fund fusion?\nThe likely role for fusion would be as an energy source in a post-carbon society. It’s well worth funding it for that reason alone, and it’s possible that an unexpected breakthrough will ensure it can reduce emissions after all. In fact, research into all sorts of new technologies could help make reducing emissions easier, such as industrial carbon capture and storage.\n\nThe allure of fusion makes it a good distraction from the failures of the current government’s science and climate policy. The Committee on Climate Change has set 25 targets that need to be met to ensure the UK is a net-zero society by 2050. The government is currently only on track to meet one of them.\n\nThis funding is also of little consolation to laboratories who are worried about the impact of Brexit on UK science, and their international staff who rely on freedom of movement. In fact, the government has yet to commit to paying to participate in the EU DEMO programme in the case of a no-deal Brexit.\n\nClimate policy should prioritise deploying proven technologies immediately, without relying on speculative solutions. Stopping climate change is too important to leave to the last minute.","type":"content","url":"/blog/2019/theconversation-fusion#failing-leadership-on-climate-change","position":9},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model"},"type":"lvl1","url":"/blog/2020/covid19-model","position":0},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model"},"content":"While the models used by actual epidemiologists can be complex, the essential dynamics of the spread of the novel coronavirus can be captured by models which are suprisingly simple.\nBy adding complexities to the base model we can then reproduce predictions from current major headlines.\n\n(This post is available as a \n\nJupyter notebook, which can be \n\ninteratively run through binder.)","type":"content","url":"/blog/2020/covid19-model","position":1},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"DISCLAIMER"},"type":"lvl2","url":"/blog/2020/covid19-model#disclaimer","position":2},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"DISCLAIMER"},"content":"I am not an epidemiologist! This model is for understanding and tutorial purposes only. It is not suitable for predictions or decision-making, and excludes many important considerations (some of which are listed below). It is intended primarily as an example of the power of basic programming in modelling a real-world problem, and an introduction to the mathematics of the spread of disease.\n\nWe will be able to use this basic model to reproduce the essence of some of the modelling results that appear in headlines and widely-shared articles at the moment, but this particular model is only meant to help you understand how those authors might have arrived at those conclusions. While the general trends indicated are indicative, the absolute numbers aren’t necessarily.\n\nIt’s also important to understand that most of the cases shown here assume that we do not manage to stamp out the virus in first month or so of infection, and so are worst-case scenarios. The model we’re using is only representative of situations which might occur after that initial chance to stop a widespread epidemic is missed.\n\nA similar but more detailed model created by an actual epidemiologist is \n\nhere.\nFor general information about the Coronavirus and advice for the public see the \n\nWorld Health Organisation website.","type":"content","url":"/blog/2020/covid19-model#disclaimer","position":3},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Susceptible-Infected-Recovered-Deceased model"},"type":"lvl2","url":"/blog/2020/covid19-model#susceptible-infected-recovered-deceased-model","position":4},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Susceptible-Infected-Recovered-Deceased model"},"content":"The simplest model appropriate for an influenza-type disease is called the SIRD model.\nIt breaks the population up into those who are Susceptible to becoming infected, those who are Infected, those who are Recovered, and those who are Deceased.\n\nThe SIRD model is suitable for the novel coronavirus if we assume that those who have recovered from the infection are henceforth immune to becoming infected again (otherwise they would return to the Suceptible population).","type":"content","url":"/blog/2020/covid19-model#susceptible-infected-recovered-deceased-model","position":5},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl3":"Equations of the SIRD model","lvl2":"Susceptible-Infected-Recovered-Deceased model"},"type":"lvl3","url":"/blog/2020/covid19-model#equations-of-the-sird-model","position":6},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl3":"Equations of the SIRD model","lvl2":"Susceptible-Infected-Recovered-Deceased model"},"content":"We follow the description of the SIRD model laid out in \n\nHethcote (1989).\n\nThe susceptible population NS(t) decreases through contact with infected, at a rate given by λ:\\frac{d(NS)}{dt} = -\\lambda S N I\n\nλ is the “daily contact rate”, which means that if everyone were suceptible and uninfected, one infected person would infect λ people per day. So the average number of susceptibles infected by an infectious person per day is \\lambda S.\n\nThe infected population NI(t) increases through contact with infected (at a rate set by λ), decreases through recovery, at a rate γ, and also decreases through death, at a rate μ.\\frac{d(NI)}{dt} = \\lambda S N I - \\gamma N I - \\mu N I\n\nWhat does γ mean? Individuals recover and are removed from the infective class at a rate proportional to the number of infectives with proportionality constant γ, called the daily recovery removal rate.\n\nThe latent period is zero (it is defined as the period between the time of exposure and the time when infectiousness begins).\nThus the proportion of individuals exposed (and immediately infective) at time t_0 who are still infective at time t_0 + t is \\exp(-\\gamma t), and the average period of infectivity is P=1/\\gamma.\n\nThe recovered population NR(t) increases when members of the infected population recover. We assume (for now) that all recovered people are henceforth immune to being infected a second time.\\frac{d(NR)}{dt} = \\gamma I N\n\nThe deceased population ND(t) increases through death at a rate μ\\frac{d(ND)}{dt} = \\mu I N\n\nMembers of the infected population leave I(t) either by recovering (at a rate γ), or dying (at a rate μ).\nTherefore the chance of an infected individual dying (the average mortality) is M=\\mu/\\gamma.\n\nThe “basic reproductive number” (also known as “contact number”) R_0 is defined as R_0=\\lambda / (\\gamma + \\mu). This comes from the fact that accounting for the deaths of infected gives a death-adjusted average period of infectivity of 1/(\\gamma + \\mu), and multiplying that time period by the daily contact rate λ.\n\nThe fact that the membership of all groups together must add up to the total population is expressed asNS + NI + NR + ND = N\n\nWe’ve also assumed that the total population is constant (including deceased), and that the epidemic occurs in a timeframe much shorter than that over which natural births and deaths significantly affect any of the populations.\n\nNow we can simplify our equations by dividing through by N\\begin{align}\n\\frac{dS}{dt} & = -\\lambda S I \\\\\n\\frac{dI}{dt} & = \\lambda S I - \\gamma I - \\mu I \\\\\n\\frac{dR}{dt} & = \\gamma I \\\\\n\\frac{dD}{dt} & = \\mu I \n\\end{align}\n\nand we also have the boundary conditionS + I + R + D = 1\n\nDiagrammatically, these equations reqpresent a set of states (S, I, R & D), connected by transfer rates (\\lambda SI, \\gamma I, \\mu I):","type":"content","url":"/blog/2020/covid19-model#equations-of-the-sird-model","position":7},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl3":"Exponential growth","lvl2":"Susceptible-Infected-Recovered-Deceased model"},"type":"lvl3","url":"/blog/2020/covid19-model#exponential-growth","position":8},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl3":"Exponential growth","lvl2":"Susceptible-Infected-Recovered-Deceased model"},"content":"Before we even write any code we can already identify the main feature of these equations: the number of Infected will grow exponentially.\n\nWhy? Exponential growth is when the rate of change of a quantity is proportional to the size of that quantity. If we assume that the recovery rate γ and death rate μ are small compared to the infection rate \\lambda S, (and that the susceptible population is large enough that it is roughly constant to begin with) then at the start we have\\begin{align}\n\\frac{dI(t)}{dt} & = (\\lambda S - \\gamma - \\mu) I(t) \\\\\n                 & = \\lambda ( S - \\frac{(\\gamma + \\mu)}{\\lambda}) I(t) \\\\\n                 & = \\lambda ( S - \\frac{1}{R_0}) I(t)\n\\end{align}\n\nthe solution of which isI(t) \\propto e^{~ \\lambda ( S - \\frac{1}{R_0}) I}\n\nso as \\lambda>0 always, and at the beginning S \\sim 1, then the infection will grow exponentially if R_0 > 1.","type":"content","url":"/blog/2020/covid19-model#exponential-growth","position":9},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Solving the equations for the Coronavirus"},"type":"lvl2","url":"/blog/2020/covid19-model#solving-the-equations-for-the-coronavirus","position":10},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Solving the equations for the Coronavirus"},"content":"Okay let’s try and model the coronavirus by solving these equations.\n\nOur equations are a system of linear ordinary differential equations, and we will start from a set of known initial values, so we’re going to need scipy’s \n\nintegrate.solve_ivp function.\n\n{% highlight py %}\nfrom scipy.integrate import solve_ivp\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\nimport numpy as np\n{% endhighlight %}","type":"content","url":"/blog/2020/covid19-model#solving-the-equations-for-the-coronavirus","position":11},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl3":"Population parameters","lvl2":"Solving the equations for the Coronavirus"},"type":"lvl3","url":"/blog/2020/covid19-model#population-parameters","position":12},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl3":"Population parameters","lvl2":"Solving the equations for the Coronavirus"},"content":"We need to pick initial conditions, which are the number of people in each population (susceptible, infected etc.) to start from.\nHere we will model the UK, but we can get the up-to-date numbers for any country from \n\nthis website, which is updated daily.# Reported UK coronavirus numbers on 18th March 2020\n# cases means total reported cases including resolved\nuk_initial_conditions = {'population': 66440000,\n                         'cases': 2626,\n                         'deaths': 104,\n                         'recovered': 65}","type":"content","url":"/blog/2020/covid19-model#population-parameters","position":13},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl3":"Virus parameters","lvl2":"Solving the equations for the Coronavirus"},"type":"lvl3","url":"/blog/2020/covid19-model#virus-parameters","position":14},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl3":"Virus parameters","lvl2":"Solving the equations for the Coronavirus"},"content":"We also need parameters which describe the properties of the coronavirus.\n\nThe basic reproductive rate of the virus R_0 (in the absence of any social distancing measures) has been \n\nestimated to be between 2.0-2.6, and we’ll use 2.4.\n\nFor the mortality we’ll use an average of 1%, as a rough representation of the value \n\nof 1.4% case fatality rate given here.\n\nWe’ll use 14 days for the average infectious period.\n\nLet’s write out the equations for the derivatives we need to calculate, then create a class which will set up the problem, solve it, and plot the solution.class SIRD:\n    def __init__(self, R0, M, P):\n        # Model parameters\n        self.R0 = R0\n        self.M = M\n        self.P = P\n\n    def dSdt(self, S, I, lamb):\n        return -lamb*S*I\n\n    def dIdt(self, S, I, lamb, gamma, mu):\n        return lamb*S*I - gamma*I - mu*I\n\n    def dRdt(self, I, gamma):\n        return gamma*I\n\n    def dDdt(self, I, mu):\n        return mu*I\n        \n    def eqns(self, t, y, lamb, gamma, mu):\n        S, I, R, D = y\n        return [self.dSdt(S, I, lamb),\n                self.dIdt(S, I, lamb, gamma, mu), \n                self.dRdt(I, gamma),\n                self.dDdt(I, mu)]\n    \n    def setup(self, population, cases, recovered, deaths):\n        # Compute initial values\n        self.population = population\n        initial_S = (population - cases) / population\n        initial_R = recovered / population \n        initial_D = deaths / population\n        initial_I = 1 - initial_S - initial_R - initial_D\n        self.y0 = [initial_S, initial_I, initial_R, initial_D]\n\n        # Compute coefficients\n        self.gamma = 1 / self.P\n        self.mu = self.gamma * self.M\n        self.lamb = self.R0 * (self.gamma + self.mu)\n\n    def solve(self, initial_conditions, tf=300):\n        \n        self.setup(initial_conditions['population'],\n                   initial_conditions['cases'],\n                   initial_conditions['recovered'],\n                   initial_conditions['deaths'])\n        \n        t_span = (0, tf)  # tf is number of days to run simulation for, defaulting to 300\n    \n        self.soln = solve_ivp(self.eqns, t_span, self.y0,\n                              args=(self.lamb, self.gamma, self.mu),\n                              t_eval=np.linspace(0, tf, tf*2))\n        return self\n    \n    def plot(self, ax=None, susceptible=True):\n        S, I, R, D = self.soln.y\n        t = self.soln.t\n        N = self.population\n\n        print(f\"For a population of {N} people, after {t[-1]:.0f} days there were:\")\n        print(f\"{D[-1]*100:.1f}% total deaths, or {D[-1]*N:.0f} people.\")\n        print(f\"{R[-1]*100:.1f}% total recovered, or {R[-1]*N:.0f} people.\")\n        print(f\"At the virus' maximum {I.max()*100:.1f}% people were simultaneously infected, or {I.max()*N:.0f} people.\")\n        print(f\"After {t[-1]:.0f} days the virus was present in less than {I[-1]*N:.0f} individuals.\")\n\n        if ax is None:\n            fig, ax = plt.subplots()\n        \n        ax.set_title(\"Covid-19 spread\")\n        ax.set_xlabel(\"Time [days]\")\n        ax.set_ylabel(\"Number\")\n        if susceptible:\n            ax.plot(t, S*N, label=\"Susceptible\", linewidth=2, color='blue')\n        ax.plot(t, I*N, label=\"Infected\", linewidth=2, color='orange')\n        ax.plot(t, R*N, label=\"Recovered\", linewidth=2, color='green')\n        ax.plot(t, D*N, label=\"Deceased\", linewidth=2, color='black')\n        ax.legend()\n\n        return ax\n\nWe initialise this with our choice of model coefficientsR0 = 2.4   # Basic Reproductive Rate [people]\nM = 0.01   # Mortality ratio [fraction]\nP = 14     # Average infectious period [days] (should really be split up by case severity)\nsird = SIRD(R0, M, P)\n\nand now solve the model forward in timesird.solve(uk_initial_conditions)\nsird.plot()For a population of 66440000 people, after 300 days there were:\n0.9% total deaths, or 575261 people.\n86.6% total recovered, or 57515786 people.\nAt the virus' maximum 21.8% people were simultaneously infected, or 14503231 people.\nAfter 300 days the virus was present in less than 1814 individuals.","type":"content","url":"/blog/2020/covid19-model#virus-parameters","position":15},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"(Interactivity)"},"type":"lvl2","url":"/blog/2020/covid19-model#id-interactivity","position":16},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"(Interactivity)"},"content":"We can use the ipywidgets library to make the inputs interactive (will only display if you’re running the code in this notebook yourself).from ipywidgets import interact, fixed\nfrom ipywidgets import IntSlider, IntRangeSlider, FloatSlider, DropdownP_slider = IntSlider(min=1, max=30, step=1, value=14)\nR0_slider = FloatSlider(min=0.1, max=15, step=0.1, value=2.6)\nM_slider = FloatSlider(min=0.001, max=0.05, step=0.001, value=0.035)@interact(R0=R0_slider, M=M_slider, P=P_slider,\n          initial_conditions=fixed(uk_initial_conditions),\n          continuous_update=False)\ndef run_sird(R0, M, P, initial_conditions):\n    SIRD(R0, M, P).solve(initial_conditions).plot()interactive(children=(FloatSlider(value=2.6, description='R0', max=15.0, min=0.1), FloatSlider(value=0.035, de…","type":"content","url":"/blog/2020/covid19-model#id-interactivity","position":17},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Social distancing"},"type":"lvl2","url":"/blog/2020/covid19-model#social-distancing","position":18},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Social distancing"},"content":"How can social distancing help?\n\nSocial distancing effectively reduces R_0. If R_0=0 (everyone lives alone in a box) then there would be no new infections, and the disease would die out once all the infected had either recovered or died.\n\nIf R_0 < 1 then new people will still be infected, but not fast enough to sustain the virus, and the contagion will decay away.\n\nBut if R_0>1, even if only a bit greater, than the virus will still grow exponentially.\nIt will grow more slowly, but will still pick up pace, becoming faster and faster.\n\nSo why do social distancing? If we can decrease R_0, in other words we can #flattenthecurve.","type":"content","url":"/blog/2020/covid19-model#social-distancing","position":19},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Lockdown policies"},"type":"lvl2","url":"/blog/2020/covid19-model#lockdown-policies","position":20},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Lockdown policies"},"content":"But just changing R_0 overall isn’t realistic, it assumes that as soon as the virus was discovered, everyone who is near it practices social distancing immediately and indefinitely.\nWhat is more realistic is that once it’s realised that the disease could cause an epidemic, governments encourage (or enforce) varying degrees of social distancing through policies like quarantining, lockdowns, banning large events, and working from home.\n\nHow can we parameterise a change in the effectiveness of social distancing through policies like these?\nLet’s try adjusting R_0 using a “quarantine parameter” Q.\nWhen Q=0 then the disease spreads freely, but when Q=1 we are on total lockdown and the disease can’t spread at all.\n\nWe can represent this by changing our equations for the fraction of susceptible and infected people to\\begin{align}\n\\frac{dS}{dt} & = -\\lambda(1 - Q) S I \\\\\n\\frac{dI}{dt} & = \\lambda(1 - Q) S I - \\gamma I - \\mu I\n\\end{align}\n\nIf we make Q a function of time Q=Q(t), then we can simulate the effect of varying the level of social distancing in response to the virus.\n\nWe can model a sudden permanent lockdown by setting Q=Q_0H(t-t_Q), where H is the heaviside step functionH(t) = \n  \\begin{cases} \n    0 & t\\lt t_Q \\\\\n    1 & t\\geq t_Q \n  \\end{cases}\n\nThis just means there are zero social distancing measures in place before day t_Q, and then immediate distancing measures of effectiveness Q are enforced permanently from day t_Q.\n\nTo model this, we need \\frac{dS}{dt} = \\frac{d}{dt}S(Q(t-t_Q))class SIRDQ(SIRD):\n    def __init__(self, R0, M, P, Q0, tQ):\n        super().__init__(R0, M, P)\n        self.Q0 = Q0\n        self.tQ = tQ\n\n    def dSdt(self, S, I, lamb, Q):\n        return -lamb*(1-Q)*S*I\n\n    def dIdt(self, S, I, lamb, gamma, mu, Q):\n        return lamb*(1-Q)*S*I - gamma*I - mu*I\n    \n    def Q(self, t):\n        if t > self.tQ:\n            return self.Q0\n        else:\n            return 0\n\n    def eqns(self, t, y, lamb, gamma, mu):\n        S, I, R, D = y\n\n        return [self.dSdt(S, I, lamb, self.Q(t)),\n                self.dIdt(S, I, lamb, gamma, mu, self.Q(t)), \n                self.dRdt(I, gamma),\n                self.dDdt(I, mu)]\n    \n    def solve(self, initial_conditions, tf=300):\n        return super().solve(initial_conditions, tf)\n    \n    def plot(self, ax=None, susceptible=True):\n        ax = super().plot(ax, susceptible)\n        return plot_social_distancing_event(ax, self.Q0, self.tQ, 'Begin')def plot_social_distancing_event(ax, Q0, t_Q, limit):\n    print(f\"{limit} social distancing of effectiveness {Q0} on day {t_Q}.\")\n    \n    # Mark intervention point\n    ax.axvline(x=t_Q, linewidth=2, color='black', linestyle='--')\n    ax.text(x=t_Q+5, y=0.8*ax.get_ylim()[1], s=f\"{limit} \\nSocial \\nDistancing \\n\")\n    return axSIRDQ(R0=2.4, M=0.01, P=14, Q0=0.6, tQ=75).solve(uk_initial_conditions).plot()For a population of 66440000 people, after 300 days there were:\n0.3% total deaths, or 192619 people.\n29.0% total recovered, or 19251533 people.\nAt the virus' maximum 5.8% people were simultaneously infected, or 3870304 people.\nAfter 300 days the virus was present in less than 38013 individuals.\nBegin social distancing of effectiveness 0.6 on day 75.\n\nThese policies might be implemented only once the virus has been spreading for a while.Q0_slider = FloatSlider(min=0.0, max=1.0, step=0.01, value=0.5)\ntQ_slider = IntSlider(min=0, max=300, value=75)@interact(R0=R0_slider, M=M_slider, P=P_slider, Q0=Q0_slider, tQ=tQ_slider, \n          initial_conditions=fixed(uk_initial_conditions),\n          continuous_update=False)\ndef run_sirdq(R0, M, P, Q0, tQ, initial_conditions):\n    SIRDQ(R0, M, P, Q0, tQ).solve(initial_conditions).plot()interactive(children=(FloatSlider(value=2.6, description='R0', max=15.0, min=0.1), FloatSlider(value=0.035, de…\n\nOr perhaps these policies will only be ramped up when there are lots of people infected.","type":"content","url":"/blog/2020/covid19-model#lockdown-policies","position":21},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Second wave?"},"type":"lvl2","url":"/blog/2020/covid19-model#second-wave","position":22},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Second wave?"},"content":"The deadliest part of the 1918 Spanish Flu epidemic was the second wave that remerged in the winter.\n\nAlthough it’s yet not clear if Covid-19 is significantly affected by temperature, there are also fears that slacking off on isolation measures will trigger a second wave.\n\nCan we model this possibility?\n\nLet’s alter our Q(t) to also stop isolation measures at a certain point in time. Let’s call this time t_{UQ} (for “Un-Quarantine”). Then Q becomesQ(t) = \n  \\begin{cases} \n    0 & t\\lt t_Q \\\\\n    Q_0 & t_Q \\leq t \\lt t_{UQ} \\\\\n    0 & t\\geq t_{UQ}\n  \\end{cases}class SIRDUQ(SIRDQ):\n    def __init__(self, R0, M, P, Q0, tQ, tUQ):\n        super().__init__(R0, M, P, Q0, tQ)\n        self.tUQ = tUQ\n\n    def Q(self, t):\n        if self.tQ < t <= self.tUQ:\n            return self.Q0\n        else:\n            return 0\n\n    def plot(self, ax=None):\n        ax = super().plot(ax)\n        return plot_social_distancing_event(ax, self.Q0, self.tUQ, 'End')SIRDUQ(R0=2.4, M=0.01, P=14, Q0=0.7, tQ=75, tUQ=150).solve(uk_initial_conditions).plot()For a population of 66440000 people, after 300 days there were:\n0.8% total deaths, or 532994 people.\n80.2% total recovered, or 53289019 people.\nAt the virus' maximum 12.1% people were simultaneously infected, or 8052134 people.\nAfter 300 days the virus was present in less than 629758 individuals.\nBegin social distancing of effectiveness 0.7 on day 75.\nEnd social distancing of effectiveness 0.7 on day 150.","type":"content","url":"/blog/2020/covid19-model#second-wave","position":23},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Healthcare capacity"},"type":"lvl2","url":"/blog/2020/covid19-model#healthcare-capacity","position":24},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Healthcare capacity"},"content":"So far our model assumes that everyone who catches the virus continues to act normally, but has a L=\\mu/\\gamma chance of dying.\n\nThis is unrealistic for multiple important reasons, but one of those is that an individual’s chance of survival depends greatly on whether there are sufficient medical resources available to tend to them.\nAny hospital, and the healthcare system as a whole, has a certain capacity for patients.\nIf the demand for care greatly exceeds this capacity then patients will unfortunately recieve less of the care that they need.\nIn reality, this comes about because a patient’s chance of survival will depend on whether or not beds are available, how many patients doctors and nurses have to try and care for at once, and whether or not more specialist equipment such as intubators and Intensive Care Units are available.\n\nTrying to model the availability of doctors, nurses, beds, ICUs and so on is complicated, so here we will try and make the simplest model which still captures the essential idea of healthcare system capacity.\n\nWe’re going to use a simple, clear and constant metric to describe healthcare system capacity - the number of hospital beds available per person in the total population, B.\n\nWe are then going to relate the number of infected people to the death rate via B.\nWe’ll do this by assuming that if the number of severely ill patients exceeds the number of beds available, the mortality rate for those patients will increase from \\mu_B to a greater value \\mu_{NB}. We’ll also assume that a fraction f_S of infected cases are severe enough to require a bed in a hospital.\n\nSo we set \\mu=\\mu(I, B, \\beta, f_S), specifically\\mu(I) = \n  \\begin{cases} \n    \\mu_B & f_S I \\leq B \\\\\n    (\\mu_{NB}-\\mu_B)\\frac{(f_S I-B)}{f_S I} + \\mu_B & f_S I \\gt B\n  \\end{cases}\n\nThe reason for the complex expression when I>B is because we’re assuming that only those patients for whom there is no bed suffer the higher mortality rate.\n(If we didn’t do this and simply used \\mu_{NB} then exceeding the healthcare capacity by one person could dramatically increase the mortality for all patients, which is clearly not realistic, and could possibly also lead to weird oscillations in the model around I=B.)\n\nWe’ll parameterise this in the model by setting \\mu_{NB}=\\beta \\mu_B, where \\beta > 1. So if the mortality rate for a patient without a hospital bed were twice that for a patient with one, \\beta=2.class SIRDQB(SIRDQ):\n    def __init__(self, R0, M, P, Q0, tQ, B, fS, beta):\n        super().__init__(R0, M, P, Q0, tQ)\n        self.B = B\n        self.fS = fS\n        self.beta = beta\n        \n    # Locally redefine mu(I)\n    def mu_I(self, I):\n        if I > self.B:\n            return ((self.beta - 1)*(I - self.B)/I + 1) * self.mu\n        else:\n            return self.mu\n    \n    def eqns(self, t, y, lamb, gamma, mu):\n        S, I, R, D = y\n    \n        return [self.dSdt(S, I, lamb, self.Q(t)),\n                self.dIdt(S, I, lamb, gamma, self.mu_I(I*self.fS), self.Q(t)), \n                self.dRdt(I, gamma),\n                self.dDdt(I, self.mu_I(I*self.fS))]\n\n    def plot(self, ax=None):\n        ax = super().plot(ax)\n        return plot_bed_capacity(ax, self.B, self.population)\n        \ndef plot_bed_capacity(ax, B, N):\n    print(f\"Hospital bed capacity of {B*1000} beds per 1000 people.\")\n\n    ax.axhline(y=N*B, linewidth=2, color='red', linestyle='--')\n    ax.text(x=5, y=N*B, s=f\"Hospital \\nBed \\nCapacity \\n\")\n    return axB = 0.0024  # Hospital beds per person in the UK\nfS = 0.1   # Fraction of Covid-19 cases which are severe enough to require a bed\nbeta = 2    # How many times more likely a severely-ill patient is to die without a bed as opposed to with oneSIRDQB(R0=2.4, M=0.01, P=14, Q0=0.5, tQ=75, B=B, fS=fS, beta=beta).solve(uk_initial_conditions).plot()For a population of 66440000 people, after 300 days there were:\n0.6% total deaths, or 393849 people.\n43.2% total recovered, or 28690787 people.\nAt the virus' maximum 6.0% people were simultaneously infected, or 3984186 people.\nAfter 300 days the virus was present in less than 83810 individuals.\nBegin social distancing of effectiveness 0.5 on day 75.\nHospital bed capacity of 2.4 beds per 1000 people.","type":"content","url":"/blog/2020/covid19-model#healthcare-capacity","position":25},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Flattening the curve"},"type":"lvl2","url":"/blog/2020/covid19-model#flattening-the-curve","position":26},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Flattening the curve"},"content":"You’ve probably heard about how we should all be practicing social distancing to help “flatten the curve”.\nWe’re now in a position to model why flattening the curve helps.def plot_curve_flattening(initial_conditions, \n                          R0, M, P, tQ, Q_values):\n    \n    tf = 500  # plot over longer period of time\n    \n    fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(15, 5))\n    ax1.set_title(\"Flattening the curve\")\n    ax1.set_xlabel(\"Time [days]\")\n    ax1.set_ylabel(\"Number of severe cases\")\n    \n    N = initial_conditions['population']\n    \n    mortality = []\n    for Q0 in Q_values:\n        sirdqb = SIRDQB(R0, M, P, Q0, tQ, B, fS, beta)\n        sirdqb = sirdqb.solve(initial_conditions, tf)\n        \n        soln = sirdqb.soln\n        S, I, R, D = soln.y\n        t = soln.t\n    \n        # Flattened curves\n        ax1.plot(t, fS*I*N, linewidth=2)\n        ax1.text(x=t[np.argmax(I)]+7, y=N*fS*I.max(), s=f'Q={Q0}')\n        \n        mortality.append(D[-1]/R[-1])\n        \n    ax1 = plot_bed_capacity(ax1, B, N)\n    \n    # Mortality rates\n    ax2.set_title(\"Effect of flattening the curve on mortality rate\")\n    ax2.set_xlabel(\"Success of social distancing [Q]\")\n    ax2.set_ylabel(\"Overall mortality rate [deaths/cases]\")\n    ax2.plot(Q_values, mortality, linewidth=2, color='black')\n    \n    plt.show()plot_curve_flattening(uk_initial_conditions, R0=2.6, M=0.01, P=14, tQ=0, \n                      Q_values=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.55])Hospital bed capacity of 2.4 beds per 1000 people.\n\nWe can now see that the better our social distancing is, the fewer cases there are at once, minimising overload of the healthcare system, and so reducing the chance that those who contract Covid-19 die from it. Social distancing doesn’t just decrease the number of people who will get infected, it also gives those who do get infected a better chance!","type":"content","url":"/blog/2020/covid19-model#flattening-the-curve","position":27},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Protecting the vulnerable"},"type":"lvl2","url":"/blog/2020/covid19-model#protecting-the-vulnerable","position":28},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Protecting the vulnerable"},"content":"One of the biggest omissions of the this model so far is that there is no distinction made between vulnerable and not-so-vulnerable people.\nInstead the population is modelled as homogenous and evenly-mixed, with an overall average mortality.\n\nHowever, we know that Covid-19 is \n\nfar more dangerous to the elderly, to those with compromised immune systems, or with certain other pre-existing health conditions. We also know that the vast majority of cases in children and young people are mild.\n\nThis property of the disease could help us to save lives, if we put measures in place designed specifically to protect the vulnerable from infection (e.g. quarantining care homes).\n\nHow can we model this in the simplest possible way? We need to divide our susceptible S(t) population into at least two groups, the vulnerable S_{V}(t) and the non-vulnerable S_{NV}(t). We also need to divide up our infected population I(t) into I_V(t) and I_{NV}(t), because a demographically-averaged mortality rate is no longer applicable if most of the infected are not the vulnerable ones. Recovered and deceased populations do not need to be subdivided.\n\nTo be able to control this subdivision of infection, the model needs a way to represent decreasing the transmission rate to the vulnerable specifically.\nTherefore, we have to split our averaged λ up according to whether it represents the rate of transmission from vulnerable to non-vulnerable, vulnerable to vulnerable etc.\nWe’ll also need to split up the death rate μ, and the recovery rate γ.\\begin{align}\n\\frac{dS_V}{dt} & = -(\\lambda_{V-V}I_{V} + \\lambda_{NV-V}I_{NV}) S_V  \\\\\n\\frac{dS_{NV}}{dt} & = -(\\lambda_{V-NV}I_{V} + \\lambda_{NV-NV}I_{NV}) S_{NV} \\\\\n\\frac{dI_V}{dt} & = (\\lambda_{V-V} I_V + \\lambda_{NV-V} I_{NV}) S_V - \\gamma_V I_V - \\mu_V I_V \\\\\n\\frac{dI_{NV}}{dt} & = (\\lambda_{V-NV} I_V + \\lambda_{NV-NV} I_{NV}) S_{NV} - \\gamma_{NV} I_{NV} - \\mu_{NV} I_{NV} \\\\\n\\frac{dR}{dt} & = \\gamma_V I_V + \\gamma_{NV} I_{NV} \\\\\n\\frac{dD}{dt} & = \\mu_V I_V + \\mu_{NV} I_{NV}\n\\end{align}\n\nand the boundary condition becomesS_V + S_{NV} + I_V + I_{NV} + R + D = 1\n\nAs an aside, if we wanted to subdivide the populations further (for example by age bracket), we would instead write S(t) as a vector of subsceptible subpopulations \\mathbf{S}(t)=(S_0(t), S_1(t), ...), and promote λ to be matrix.\\begin{align}\n\\frac{dS_i}{dt} & = \\sum_{ij} -\\lambda_{ij} I_i S_j  \\\\\n\\frac{dI_i}{dt} & = \\sum_{ij} \\lambda_{ij} S_j I_i - \\sum_i (\\gamma_i I_i + \\mu_i I_i) \\\\\n\\frac{dR}{dt} & = \\sum_i \\gamma_i I_i \\\\\n\\frac{dD}{dt} & = \\sum_i \\mu_i I_i\n\\end{align}\n\nand the boundary condition would become\\sum_i \\left(S_i + I_i \\right) + R + D = 1\n\nWe have a lot of extra coefficients here. Still aiming for the simplest way to represent this additional complication, we ideally only want one extra parameter to quantify the extent to which we successfuly manage to isolate (and hence hopefully protect) the vulnerable.\n\nWe going to create a separation parameter α, which must correspond to the same model as before when \\alpha=0, and total separation when \\alpha=1. We’ll relate it to our original daily contact rate \\lambda_0 via\\begin{align}\n\\lambda_{NV-NV} & = \\lambda_{V-V} = \\lambda_0 \\\\\n\\lambda_{NV-V} & = \\lambda_{V-NV} = (1-\\alpha) \\lambda_0\n\\end{align}\n\nThis basically assumes that we have two populations, which each have the same internal transmission rate as before, but have a reduced transmission rate between populations.\n\n(Eagle-eyed readers will notice this change has altered the determinant of the tranmission matrix \\text{det}(\\lambda_{ij}), which means the total averaged rate of transmission in the whole population is now a function of α. That means our decoupling method isn’t pure, it has a side-effect of reducing overall transmission. I’m going to ignore that, because correcting for it would mean artificially increasing the transmission within each separate population when α is increased, which seems sillier.)class SIRDV(SIRD):\n    def __init__(self, R0, M_NV, M_V, P, alpha):\n        super().__init__(R0, None, P)\n        self.M_NV = M_NV\n        self.M_V = M_V\n        self.alpha = alpha\n\n    def eqns(self, t, y, lamb_0, gamma, mu_V, mu_NV, alpha):\n        S_V, S_NV, I_V, I_NV, R, D = y\n\n        def dSVdt(S_V, I_V, I_NV, lamb_VV, lamb_NVV):\n            return -(lamb_VV * I_V + lamb_NVV * I_NV) * S_V\n\n        def dSNVdt(S_NV, I_V, I_NV, lamb_NVNV, lamb_VNV):\n            return -(lamb_NVNV * I_NV + lamb_VNV * I_V) * S_NV\n\n        def dIVdt(S_V, I_V, I_NV, lamb_VV, lamb_NVV, gamma, mu_V):\n            return (lamb_VV * I_V + lamb_NVV * I_NV) * S_V - gamma*I_V - mu_V*I_V\n\n        def dINVdt(S_NV, I_V, I_NV, lamb_NVNV, lamb_VNV, gamma, mu_NV):\n            return (lamb_VNV * I_V + lamb_NVNV * I_NV) * S_NV - gamma*I_NV - mu_NV*I_NV\n\n        def dRdt(I_V, I_NV, gamma):\n            return gamma*(I_V + I_NV)\n\n        def dDdt(I_V, I_NV, mu_V, mu_NV):\n            return mu_V*I_V + mu_NV*I_NV\n\n        lamb_VV = lamb_0\n        lamb_NVNV = lamb_0\n        lamb_VNV = lamb_0 * (1 - alpha)\n        lamb_NVV = lamb_0 * (1 - alpha)\n\n        return [dSVdt(S_V, I_V, I_NV, lamb_VV, lamb_NVV),\n                dSNVdt(S_NV, I_V, I_NV, lamb_NVNV, lamb_VNV),\n                dIVdt(S_V, I_V, I_NV, lamb_VV, lamb_NVV, gamma, mu_V), \n                dINVdt(S_NV, I_V, I_NV, lamb_NVNV, lamb_VNV, gamma, mu_NV), \n                dRdt(I_V, I_NV, gamma),\n                dDdt(I_V, I_NV, mu_V, mu_NV)]\n    \n    def setup(self, population, cases, recovered, deaths, fV):\n        \n        self.population = population\n        self.fV = fV\n        initial_S = (population - cases) / population\n        initial_R = recovered / population \n        initial_D = deaths / population\n        initial_I = 1 - initial_S - initial_R - initial_D\n        \n        initial_S_V = (population - cases) / population * fV\n        initial_S_NV = (population - cases) / population * (1-fV)\n        \n        initial_I_V = initial_I * fV\n        initial_I_NV = initial_I * (1-fV)\n        \n        self.y0 = [initial_S_V, initial_S_NV, \n                   initial_I_V, initial_I_NV, \n                   initial_R, initial_D]\n\n        # Compute coefficients\n        self.gamma = 1 / self.P\n        self.mu_V = self.gamma * self.M_V\n        self.mu_NV = self.gamma * self.M_NV\n        mu_avg = self.mu_V * fV + self.mu_NV * (1 - self.fV) \n        self.lamb_0 = self.R0 * (self.gamma + mu_avg)\n\n    def solve(self, initial_conditions, tf=300):\n    \n        self.setup(initial_conditions['population'],\n                   initial_conditions['cases'],\n                   initial_conditions['recovered'],\n                   initial_conditions['deaths'],\n                   initial_conditions['vulnerable fraction'])\n    \n        t_span = (0, tf)  # tf is number of days to run simulation for, defaulting to 300\n    \n        self.soln = solve_ivp(self.eqns, t_span, self.y0, \n                              args=(self.lamb_0, self.gamma, self.mu_V, self.mu_NV, self.alpha),\n                              t_eval=np.linspace(0, tf, tf*2))\n        return self\n\n    def plot(self):\n\n        soln = self.soln\n        S_V, S_NV, I_V, I_NV, R, D = soln.y\n        t = soln.t\n        N = self.population\n        \n        print(f\"For a population of {N} people, after {t[-1]:.0f} days there were:\")\n        print(f\"{D[-1]*100:.1f}% total deaths, or {D[-1]*N:.0f} people.\")\n        print(f\"{R[-1]*100:.1f}% total recovered, or {R[-1]*N:.0f} people.\")\n        print(f\"At the virus' maximum {(I_V + I_NV).max()*100:.1f}% people were simultaneously infected, or {(I_V + I_NV).max()*N:.0f} people.\")\n        print(f\"After {t[-1]:.0f} days the virus was present in less than {(I_V + I_NV)[-1]*N:.0f} individuals.\")\n\n        fig, ax = plt.subplots()\n        ax.set_title(\"Covid-19 spread\")\n        ax.set_xlabel(\"Time [days]\")\n        ax.set_ylabel(\"Number\")\n        ax.plot(t, S_V*N, label=\"Susceptible and Vulnerable\", linewidth=2, color='blue')\n        ax.plot(t, S_NV*N, label=\"Susceptible and Non-Vulnerable\", linewidth=2, color='blue', linestyle='--')\n        ax.plot(t, I_V*N, label=\"Infected and Vulnerable\", linewidth=2, color='orange')\n        ax.plot(t, I_NV*N, label=\"Infected and Non-Vulnerable\", linewidth=2, color='orange', linestyle='--')\n        ax.plot(t, R*N, label=\"Recovered\", linewidth=2, color='green')\n        ax.plot(t, D*N, label=\"Deceased\", linewidth=2, color='black')\n        ax.legend()\n\n        return ax\n\nWe will roughly define the “vulnerable” population to be those aged >70, which for demographics similar to the UK means the vulnerable fraction f_V=0.12 (as \n\n11.7% of the UK is aged over 70).\n\nThe chance of an infected individual dying from the disease is M_i = \\mu_i / \\gamma_i, where i could be for V or NV.\nFor simplicity’s sake we’re going to assume that the rate of recovery is the same for both populations (so \\gamma_V = \\gamma_{NV} = \\gamma_0). However we will assume that the mortality is very different: M_{V} = 0.07 while M_{NV} = 0.002. These numbers are roughly chosen from table 1 of the \n\nImperial College Report, in such a way that the population-averaged mortality is still the same as in the previous modelM_\\text{avg} = M_V f_V + (1 - f_V) M_{NV}\n\nTo start our new model, we also need the initial conditions: we’ll assume that the fraction of infected cases so far is divided in the same way as the overall population.M_NV = 0.002  # Mortality ratio of the vulnerable population\nM_V = 0.07    # Mortality ratio of the non-vulnerable population\n\nuk_initial_conditions['vulnerable fraction'] = 0.12    # Fraction of the population considered vulnerable\n\nalpha = 0.5   # Effectiveness of isolating the vulnerable populationSIRDV(R0=2.4, M_V=M_V, M_NV=M_NV, P=14, alpha=0.4).solve(uk_initial_conditions).plot()For a population of 66440000 people, after 300 days there were:\n0.7% total deaths, or 478227 people.\n83.4% total recovered, or 55392681 people.\nAt the virus' maximum 19.0% people were simultaneously infected, or 12635921 people.\nAfter 300 days the virus was present in less than 5249 individuals.def plot_protecting_the_vulnerable(initial_conditions, \n                                   R0, M_V, M_NV, P, alpha_values):\n    \n    tf = 500  # plot over longer period of time\n    \n    fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(15, 5))\n    ax1.set_title(\"Reducing transmission to the vulnerable\")\n    ax1.set_xlabel(\"Time [days]\")\n    ax1.set_ylabel(\"Number of vulnerable cases\")\n    \n    ax2.set_title(\"Effect of reducing transmission to the vulnerable on mortality rate\")\n    ax2.set_xlabel(\"Success of reducing transmission to the vulnerable [alpha]\")\n    ax2.set_ylabel(\"Overall mortality rate [deaths/cases]\")\n    \n    mortality = []\n    for alpha in alpha_values:\n        sirdv = SIRDV(R0, M_NV, M_V, P, alpha).solve(initial_conditions, tf=300)\n        soln = sirdv.soln\n        \n        S_V, S_NV, I_V, I_NV, R, D = soln.y\n        t = soln.t\n        N = sirdv.population\n    \n        # Flattened curves\n        ax1.plot(t, I_V*N, linewidth=2)\n        ax1.text(x=t[np.argmax(I_V)]+7, y=N*I_V.max(), s=f'alpha={alpha}')\n        \n        mortality.append(D[-1]/R[-1])\n    \n    # Mortality rates\n    ax2.plot(alpha_values, mortality, linewidth=2, color='black')\n    \n    plt.show()plot_protecting_the_vulnerable(uk_initial_conditions, R0=2.4, M_V=0.07, M_NV=0.002, P=14,\n                               alpha_values=[0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 1.0])\n\nWe can see that the better we do at reducing transmission to the vulnerable population, the lower the mortality rate.","type":"content","url":"/blog/2020/covid19-model#protecting-the-vulnerable","position":29},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Immediate suppression"},"type":"lvl2","url":"/blog/2020/covid19-model#immediate-suppression","position":30},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Immediate suppression"},"content":"Even with these mitigation policies, the results are horrifying. What’s the best-case outcome?\n\nWe could instead go all-out to try and \n\nstop the virus in its tracks immediately.\n\nIf we could get the reproductive rate of the virus r<0, the infection would exponentially decay instead of growing.\n\nTherefore we need( S - \\frac{1}{R_0(1-Q)})  < 0\n\norR_0 (1-Q) < 1\n\nFor our typical coronavirus parameters, S\\sim 1, and R_0 \\approx 2.4, this implies we need Q>0.59.\n\nThis work suggests that with efficient contact tracing and isolation, we could get r down to ≈ 0.1.\nAsr = S - \\frac{1}{R_0(1-Q)},\n\nthen this can be represented in our model asQ = 1 - \\frac{1}{(S + r)R_0} = 0.62SIRDQ(R0=2.4, M=0.01, P=14, Q0=0.62, tQ=7).solve(uk_initial_conditions, tf=100).plot(susceptible=False)For a population of 66440000 people, after 100 days there were:\n0.0% total deaths, or 333 people.\n0.0% total recovered, or 22954 people.\nAt the virus' maximum 0.0% people were simultaneously infected, or 4353 people.\nAfter 100 days the virus was present in less than 2328 individuals.\nBegin social distancing of effectiveness 0.62 on day 7.\n\nThis suggests that with advanced methods for contact tracing and isolation, it is possible to prevent the epidemic growing further, which would save thousands of lives.\n\nHowever, the measures would likely still have to be in place for several months in order to reduce the number of cases.\n\nAt this point, more advanced models of virus propagation are really required.","type":"content","url":"/blog/2020/covid19-model#immediate-suppression","position":31},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Summary"},"type":"lvl2","url":"/blog/2020/covid19-model#summary","position":32},{"hierarchy":{"lvl1":"Coronavirus: The Simplest Model","lvl2":"Summary"},"content":"We have seen that:\n\nVery simple and intuitive mathematical models can reproduce the same overall trends as more complex epidemiological models.\n\nIf unchecked, the virus will spread exponentially through a large fraction of the population.\n\nIn the worst-case scenario, the total death toll in the UK alone could be in the hundreds of thousands.\n\nThe peak load on the healthcare system might not be reached for 2-3 months.\n\nWidespread social distancing can help reduce the number of cases, and limit the burden on the healthcare system.\n\nThe earlier social distancing begins the lower the number of cases will be.\n\nHowever, slacking off on the social distancing could lead to a re-emergence of the virus.\n\nSpecifically isolating the most vulnerable segments of the population can significantly reduce the overall mortality rate.\n\nAll of these conclusions agree with detailed \n\npublished studies.","type":"content","url":"/blog/2020/covid19-model#summary","position":33},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl1","url":"/blog/2020/deep-pseudoscience","position":0},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"","type":"content","url":"/blog/2020/deep-pseudoscience","position":1},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Preface"},"type":"lvl3","url":"/blog/2020/deep-pseudoscience#preface","position":2},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Preface"},"content":"This is a backup of an article I first \n\npublished on openDemocracy.\nThere is also a much shorter summary version \n\npublished in The Ecologist.\nIt got considerable \n\ntraction on twitter.\n\nA huge number of people contributed to this piece in some form or another, not just those listed.\nI’m very grateful to all of them, but especially to Galen and Colleen.\n\nJem Bendell wrote \n\na response here, addressed to members of the Deep Adaptation movement.\nHe also updated the original Deep Adaptation paper in response to this article.\nThe original to which this article responds can be found on this site \n\nhere, whilst the updated version is \n\nhere.","type":"content","url":"/blog/2020/deep-pseudoscience#preface","position":3},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl2","url":"/blog/2020/deep-pseudoscience#the-faulty-science-doomism-and-flawed-conclusions-of-deep-adaptation","position":4},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"As members of Extinction Rebellion and other climate movements, we have been overjoyed at the success of XR in ringing the alarm about climate and ecological breakdown, and in applying pressure to the UK government, as well as other governments worldwide. As members of the science community, we have also found comfort in a movement dedicated to telling a truth that has for decades been obscured by corporate public relations campaigns and misinformation.\n\nMany scientists support XR or are active members, lending some immediate authority to our message of climate and ecological emergency. The need for peaceful civil disobedience has been explicitly supported by over a thousand scientists. Arrested XR activists received support during their trials from high-profile scientists acting as expert witnesses. As scientists ourselves, we support our movement’s goal of halting greenhouse gas emissions and biodiversity loss rapidly and equitably, but we also know that doing so successfully requires clarity about what science can and cannot tell us. Such clarity is especially important now. In the past few years we have seen a troubling trend: a few figures in the climate movement using science — or what looks like science — to justify increasingly dire and prophetic, but ultimately unsupported, claims about the future.\n\nThe most influential example of such climate doomism is undoubtedly Professor Jem Bendell’s “Deep Adaptation,” a self-published 2018 paper which holds that accelerating climate change has guaranteed social collapse within the next few decades. Hundreds of thousands of people have downloaded “Deep Adaptation,” and the paper has significantly impacted the ideology and strategy of climate movement organizations like XR. People have changed their life plans based in large part on this paper’s predictions. It is therefore past time to show that Deep Adaptation is wrong — not least because Bendell’s brand of doomism relies heavily on misinterpreted climate science that undermines the credibility of his claims. In fact, Deep Adaptation consistently cherry-picks data, cites false experts, puts forward logical fallacies, and disregards robust scientific consensus. Bendell defends himself by offering unsupported reasons for activists and the public to distrust mainstream climate science. In all of these regards, Deep Adaptation mimics the practices that deniers of global warming have wielded for decades.\n\nWhy is it important to deconstruct Deep Adaptation now, in the midst of a global pandemic? In short, the fatal verdict handed down by Deep Adaptation brings with it a bundle of personal and strategic implications with the potential to cripple us as a movement. The flawed science of Deep Adaptation supports flawed socio-political conclusions. The pandemic makes the divergence between these flawed conclusions and the ones we ought to draw all the more apparent. Where Deep Adaptation implies that scientific understanding can no longer save us from catastrophe, COVID has shown the critical importance of science-based policy. Where Deep Adaptation backs away from questions of equity and distribution in the face of disaster, COVID has shown that (in)justice only becomes more important under such circumstances. The people who have suffered most under the coronavirus will also suffer disproportionately from climate change. Conversely, the same people who oppose climate justice and malign climate science also bear central responsibility for disastrous COVID responses in countries like the US, England and Brazil. The coronavirus pandemic may open a window for policy shifts to begin an equitable transition away from our carbon based economy — in which case we cannot allow a faulty quasi-ideology like Deep Adaptation to mislead us.\n\nTo be totally clear, we argue that all of the following are simultaneously true:\n\nThere is an unprecedented global climate and ecological emergency. If governments do not undertake enormous measures to mitigate climate change, then some form of “societal collapse” is plausible — albeit in varying forms and undoubtedly far worse for the poorest people.\n\nPolicymakers and society at large are not treating this grave threat with anything approaching sufficient urgency.\n\nThe climate crisis is dire enough in any case to justify urgent action, including mass sustained nonviolent disruption, to pressure governments to address it swiftly.\n\nHowever, neither social science nor the best available climate science support Deep Adaptation’s core premise: that near-term societal collapse due to climate change is inevitable.\n\nThis false belief undermines the environmental movement and could lead to harmful political decisions, overwhelming grief, and fading resolve for decisive action.\n\nRespecting the distinction between the coming hardships and unstoppable collapse clarifies our agency to minimise future harm by mitigating and adapting to climate change, whilst freeing us from moral and political blinkers.","type":"content","url":"/blog/2020/deep-pseudoscience#the-faulty-science-doomism-and-flawed-conclusions-of-deep-adaptation","position":5},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl3","url":"/blog/2020/deep-pseudoscience#deep-adaptation-unfounded-doomism","position":6},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"Deep Adaptation is just one prominent case of a stubborn class of doomist narratives. Doomism has always occupied an influential place within the western environmental movement. It was present during the first Earth Day, fifty years ago, in concern over the coming “population bomb.”  When one instance of doomism becomes discredited or disproven, another appears, generally following a re-examination of the state of environmental degradation. The resulting dire findings are then used to justify a fatalist ideology or response.\n\nThis most recent iteration of environmental doomism occurs as a widespread and growing scientific consensus confirms that we are in a planetary crisis. The climate movement has fought hard just to bring this science back into the public narrative. But a movement based on science that is opaque to anyone without relevant expertise will always depend on writers, journalists and academics in interpretive roles. Because of this dependence, the climate movement is also structurally vulnerable to doomist intellectuals who claim that science supports their ideas.\n\nDeep Adaptation exploits this vulnerability by appearing not only rigorous and scientific, but revelatory. The paper itself is framed as an act of academic rebellion: it was first published as a blog post after it was rejected for publication by the Sustainability Accounting, Management and Policy Journal, and the author claims it is one of the first academic articles to “conclude that climate-induced societal collapse is now inevitable in the near-term’'. His framing paid off. Deep Adaptation has since been downloaded over 450,000 times and prominently featured in XR media. In fact, Bendell contributed a chapter to the Extinction Rebellion handbook, and has spoken for the movement many other times. His paper received a boost from VICE’s favourable coverage, and from the Financial Times.","type":"content","url":"/blog/2020/deep-pseudoscience#deep-adaptation-unfounded-doomism","position":7},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Why is it so popular?","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#why-is-it-so-popular","position":8},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Why is it so popular?","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"Deep Adaptation has clearly resonated with many people, including professional scientists. Why is it so popular? We should separate this into two distinct questions. First, what immediate appeal does Deep Adaptation have to the first-time reader? Second, where does it get its significant and lasting support base?\n\nOne immediate appeal is the paper’s blunt language strikes a chord for people anguished at society’s obliviousness to the huge climate and ecological threat. Governments continue to massively subsidise fossil fuels without accompanying pollution controls, and the non-binding national commitments made under the Paris agreement are grossly inadequate even to limit global temperature rise to 2˚C. Once one appreciates the scale of this negligence it is difficult to see roads filled with petrol-burning cars, or new airport expansions, and not feel totally powerless. Deep Adaptation openly discusses the emotional anxiety induced by thinking about such colossal problems: the author writes that they “still make my jaw drop, eyes moisten, and air escape my lungs.” (pg. 21) These are familiar feelings for us and many others. Relatedly, Deep Adaptation talks about why many deny climate change’s implications and the necessity of action — a sensitive discussion too-often framed in solely political terms. Part of the paper’s value is its willingness to discuss the current, affective, and emotional impacts of the crisis.\n\nDeep Adaptation also correctly identifies many of the ideological barriers that have stymied environmental protection so far: “The West’s response to environmental issues has been restricted by the dominance of neoliberal economics since the 1970s. That led to hyper-individualist, market fundamentalist, incremental and atomistic approaches.” (pg. 24) We and many others agree with the thrust of this argument; permanently solving the ecological crisis requires much more fundamental societal shifts than merely trusting deregulated market forces, corporate social responsibility initiatives, or personal carbon footprinting.\n\nA crucial strength of the Deep Adaptation paper is the general idea that we need to brace for serious impacts from climate change. Already, many countries in the Global South have been suffering these impacts for years, and people in frontline communities in (for example) Louisiana, Puerto Rico and across the western U.S. face greater risks of climate-exacerbated disasters each year. The oceans’ heat uptake lags behind that of the atmosphere, meaning seas are expected to continue to rise past 2100 even in a low emissions scenario. Cutting emissions in line with the 1.5°C target would still lead to around a metre of global sea level rise over the next few hundred years, and cannot rule out the possibility of the West Antarctic Ice Sheet contributing an additional 0.2-1.7m by 2100, exceeding the hundreds of millions of people who will already be at risk from flooding. Therefore, even the most rapid emissions reductions programme must be accompanied by major adaptation efforts, as the effects of the climate crisis will only get worse.\n\nThe second question, why Deep Adaptation maintains strong support from a small and committed community, is harder to answer. In private conversations some have speculated that Deep Adaptation provides a kernel of shared grief about which shared identity coalesces. Bendell has encouraged this development and goes as far as to frame Deep Adaptation as something akin to a spiritual movement, while also sheltering online groups like the “Positive Deep Adaptation” Facebook page from dissenting views. (The page discourages any discussion of climate change mitigation.) We trust the good intentions of setting up a compassionate space for people to share feelings of grief and loss, as many of us share similar feelings. Nonetheless, the central premises shared by these Deep Adaptation groups are based fundamentally on faulty science.","type":"content","url":"/blog/2020/deep-pseudoscience#why-is-it-so-popular","position":9},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Exaggerated tipping points","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#exaggerated-tipping-points","position":10},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Exaggerated tipping points","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"Supporters of Deep Adaptation often claim that understanding social collapse requires a holistic understanding of physical science, economics, culture, food systems, and so forth. Bendell himself has argued that opinions about the effects of climate change on human society “are not climate science,” and we agree.\n\nYet at its base, Deep Adaptation rests entirely on a climate-science-based argument contained in the sections titled “Our Non-Linear World” and “Looking Ahead (pages 6-12). Bendell writes in a letter to the editor attached to Deep Adaptation that “the summary of science is the core of the paper as everything then flows from the conclusion of that analysis” (pg. 35). The science Bendell purports to summarize revolves around two concepts: so-called “climate tipping points” and “nonlinearity”. These concepts are crucial to understanding both the paper and the sub-movement it has inspired.\n\nWhat are tipping points? Any complex physical system like the climate can feature self-reinforcing feedback effects. Arctic ice melt is one example: as the earth warms, the highly reflective ice melts, and the darker surface below absorbs more heat, warms faster, and melts more ice. Climate feedback effects normally balance each other so that the climate stays relatively stable. In theory, a particular feedback can grow until the system crosses a key “tipping point,” after which the feedback becomes the dominant factor determining the rate of global warming, overpowering any human intervention. Passing tipping points is a worrying thought and a real potential problem, but it is one that must be considered in context.\n\nThe core of Deep Adaptation’s argument depends on two particular feedback loops: Arctic ice melt and methane release from permafrost. Although the discussion of these processes occupies only a small section of the paper, Bendell’s argument that societal collapse is now inevitable, and thus the basis for the Deep Adaptation philosophy, stands or falls depending on whether they are correct. They are not.","type":"content","url":"/blog/2020/deep-pseudoscience#exaggerated-tipping-points","position":11},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Arctic ice claims are overblown","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#arctic-ice-claims-are-overblown","position":12},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Arctic ice claims are overblown","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"As the atmosphere warms, more Arctic sea ice melts and less refreezes each year. The receding ice reveals more of the ocean and, as darker water reflects less sunlight than white ice, the surface absorbs more incoming energy from the sun. This “ice albedo effect” is a well-established part of climate modelling and, like any other positive feedback, a real cause for concern. But it is not, as Bendell claims, a near-term existential threat. A summary of the relevant research explained by Dr. David Armstrong McKay, a postdoctoral researcher on climate tipping points, shows that the overall warming expected as the result of ice-free summers is about 0.15°C globally, which would be primarily concentrated in the Arctic — a fraction of the goal set by the Paris Agreement of limiting global warming to 2°C.\n\nCompare this summary of multiple studies to Deep Adaptation’s treatment of the same topic:\n\n“One of the most eminent climate scientists in the world, Peter Wadhams, believes an ice-free Arctic will occur one summer in the next few years and that it will likely increase by 50% the warming caused by the CO2 produced by human activity (Wadhams, 2016). In itself, that renders the calculations of the IPCC redundant, along with the targets and proposals of the UNFCCC.” (pg. 7)","type":"content","url":"/blog/2020/deep-pseudoscience#arctic-ice-claims-are-overblown","position":13},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Methane claims are misleading","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#methane-claims-are-misleading","position":14},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Methane claims are misleading","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"The second tipping point often used to justify claims of near-term catastrophe is the release of methane from methane clathrate compounds buried in seabeds and seabed permafrost. The “clathrate gun hypothesis”, originally proposed in a 2003 paper, suggests that warming in the Arctic could release massive quantities of methane within a few years: up to 50 giga-tons, or a twelve-fold increase in atmospheric methane concentration, by one estimate. Methane is about 84 times more potent a greenhouse gas than carbon dioxide over the first twenty years after emission, which means that this release would cause a powerful feedback loop in which rising temperatures cause more methane to escape, and so on.\n\nBut multiple more recent reviews of the scientific literature have all come to an opposing conclusion: “it seems unlikely that catastrophic, widespread dissociation of marine clathrates will be triggered by continued climate warming at contemporary rates (0.2˚C per decade) during the twenty-first century.” Much of the methane released from these marine sources never reaches the atmosphere in the first place, in part because microbes in the ocean’s water and sediment digest it before it bubbles out of the ocean. A comprehensive review found little evidence that these sources are contributing to an increase in atmospheric methane. In fact, a scientist at NASA debunked similar claims back in 2013 — but Deep Adaptation has almost single-handedly resurrected them.\n\nDeep Adaptation revisits the methane question through suggestive allusions to real-time methane measurements. It references the “[methane] data published by scientists from the Arctic News website,” claiming that they are “consistent with this added methane coming from our oceans, which could in turn be from methane hydrates” (pg. 12). Arctic News is a blog which the science editor of the fact-checking website Climate Feedback highlighted for containing all sorts of unscientific claims as early as 2014. For example, the blog previously predicted a global temperature rise of a full 20 degrees Celsius by 2040 using a physically unjustified polynomial trendline one could fit in Excel.\nAside from confusing global with local arctic temperature increase (the polar regions experience much greater increases than the globe as a whole), this particular prediction was made by fitting an arbitrary curve to some arctic temperature data. These kinds of predictions are unscientific in the same way that the Trump administration’s recent “cubic curve fit” to COVID-19 cases was — they use no scientific principles to ground the prediction, so they aren’t accurate. (The COVID-19 fit predicted that mid-May would bring a negative number of virus deaths.)\n\nMultiple sources refute that recent increases in methane come from the Arctic, and deny that the world is near a methane tipping point. As the Tipping Points Project explains: although there has been an increase in methane since 2007, and at a faster rate since 2015, this is “mostly from either tropical or sub-tropical sources (e.g. from farming or wetlands), fossil fuels (e.g. natural gas leaks), and/or a slowdown in how quickly methane breaks down in the atmosphere. This modelling also indicates that after 2007 methane has mostly come from outside polar regions, which would rule out Arctic permafrost or methane hydrates as the driver of the recent increase.”\n\nThe most recent exhaustive review of this topic was the IPCC Oceans and Cryosphere report, published in September 2019. The IPCC found that humans still control methane-induced warming: “As with total carbon emissions, there is high confidence that mitigation of anthropogenic methane sources could help to dampen the impact of increased methane emissions from the Arctic and boreal regions.” (Chapter 3, page 253) In fact, one of the main papers the Cryosphere report cites concludes that, “Despite large uncertainties associated with future projections of Arctic natural methane emissions, our current best estimates of potential increases in natural emissions remain lower than anthropogenic emissions. In other words, claims of an apocalypse associated solely with Arctic natural methane emission feedbacks are misleading, since they guide attention away from the fact that the direction of atmospheric methane concentrations, and their effect on climate, largely remain the responsibility of anthropogenic GHG emissions.”\n\nLarge methane contributions from permafrost are not impossible, but it is a gross misrepresentation of the science to say that it is likely, let alone inevitable, that non-anthropogenic methane emissions have or will become the dominant warming factor regardless of our actions going forwards.","type":"content","url":"/blog/2020/deep-pseudoscience#methane-claims-are-misleading","position":15},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Tipping cascades to the rescue?","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#tipping-cascades-to-the-rescue","position":16},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Tipping cascades to the rescue?","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"This version is almost identical to the long-form article in openDemocracy, but contains a few extra footnotes which had to be dropped.daptation exaggerates tipping points and promotes false science on arctic sea ice melt and methane emissions to fuel its doomist narrative. We and others have shown that these claims are unfounded, and yet supporters double down on them, searching for evidence to confirm a position which had already been chosen.\n\nSince Deep Adaptation was published, a paper in PNAS introduced the concept of “tipping cascades.” Supporters of Deep Adaptation now regularly use it to support the idea of inevitable runaway heating. “Tipping cascades” refer to multiple tipping points, each one sequentially triggering the next, which could hypothetically take global warming out of human control. We should not ignore the possibility of positive feedback cycles such as this. But other climate scientists have pointed out several crucial caveats to this paper. Firstly, this was what is called a “perspectives” paper - a speculative think piece which made a suggestion, not a detailed numerical study backed up by new modelling or experimental evidence. It might well be that more detailed studies later show that no such cascade is possible. Secondly, the lowest temperature at which this cascade was hypothesised to begin was 2°C higher than pre-industrial times, a mark which we definitely have not yet reached. Further, this was a lower bound, meaning that uncertainty would likely place that limit higher, not lower. It is crucial to note that the timescale over which these changes were hypothesised to occur is centuries to millennia, not the few decades that would be required to support claims of near-term societal collapse.\n\nIt is important to clarify a few further aspects of tipping points. First, we must not confuse tipping points that cause further planetary warming with ones which cause isolated damage to a local ecosystem; the latter, while contributing to a crisis of their own, do not contribute significantly to the chance of a climate-driven global collapse. Second, a tipping point is not the same thing as a feedback loop. For one, feedbacks exist which don’t constitute tipping points--because they are too weak, for example, or can only provide a finite amount of additional warming before all the ice is melted. Additionally, some tipping points exist in systems which do not contribute to a global feedback loop. Recognizing the complexity within the broad concept of tipping points helps us to understand the scale and scope of these various threats.\n\nOverall the PNAS paper constitutes a thought-provoking argument: what we do now may matter indefinitely, so we should do all we can to avoid that long-term risk. It does not say that we might already be too late, rather that “decisions occurring over the next decade or two could significantly influence the trajectory of the Earth System for tens to hundreds of thousands of years.” We agree with this argument, but we’ve also shown that it lends no weight to claims of inevitable near-term societal collapse.","type":"content","url":"/blog/2020/deep-pseudoscience#tipping-cascades-to-the-rescue","position":17},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Non-linear but not unstoppable","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#non-linear-but-not-unstoppable","position":18},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Non-linear but not unstoppable","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"upport the idea that we have already crossed some tipping points, Deep Adaptation leans heavily on the concept of nonlinearity. While “nonlinear” has a strict mathematical definition, Deep Adaptation appears to use the term in the colloquial sense, which describes something whose change is not directly proportional to the input. We are currently receiving a brutal lesson on one kind of nonlinear relationship — exponential growth — from COVID-19.\n\nFortunately, “nonlinear” does not mean “unstoppable,” even by this casual definition. Coronavirus spread exponentially at first, but in most countries social distancing has now begun to reduce the rate of infection so that, with persistence and luck, the number of new infections will begin to reduce exponentially. The spread and containment of coronavirus has been a nonlinear process throughout, but, as countries like South Korea and New Zealand have shown, it can still be controlled. (Using the strict mathematical definition of nonlinear - differential equations whose coefficients are functions of the variables to be solved for - does not change our fundamental point: non-linear does not immediately imply uncontrollable runaway behaviour.)\n\nThe Deep Adaptation paper does not make the distinction between nonlinear and unstoppable. It regularly implies that no nonlinear process can be controlled, and appears to confuse nonlinear change with unstoppable exponential growth. For example, Deep Adaptation claims that observed temperature and sea level rise “are consistent with non-linear changes in our environment that then trigger uncontrollable impacts on human habitat and agriculture” (pg. 7), and that measurements of atmospheric methane “are consistent with a non-linear increase - potentially exponential” (pg. 12).\n\nBut exponential growth is only one possible type of nonlinear change. COVID-19’s spread remains nonlinear even while we constrain it below exponential levels. Similarly, the height of a person over time changes nonlinearly, because 40-year olds are not normally twice as tall as 20-year olds. Human growth is nonlinear, but it certainly isn’t exponential. The height example illustrates another key point: extrapolation from recent data without any understanding of the driving process will predict the wrong outcome. Someone with no understanding of human biology might watch the rapid growth of a newborn and conclude that an 80-year old person should be 100 feet tall, but we know better.\n\nClimate scientists like NASA’s Gavin Schmidt have already criticized Bendell for misunderstanding the nature of nonlinearity. Bendell responded by “emphasis[ing] the importance of real time observations” of quantities like methane release from permafrost. He also writes in the original paper that the IPCC incorrectly assumes linear as opposed to nonlinear increases in quantities like sea level rise (pg. 7).\n\nThis response is still wrong. Recent history is littered with faulty predictions made based on short-term observations and no mechanistic understanding. These predictions range from claims that global warming had stopped during the “global warming hiatus,” to others arguing that it had entered a runaway cycle, as we encountered in the Arctic sea ice section above. Climate science avoids these pitfalls because it models the underlying (and nonlinear) processes driving climate change by incorporating as much basic physics, chemistry and biology as possible, whilst also matching historical observations. This process can include many feedback effects between temperature changes and natural carbon stocks, so global climate models are fundamentally nonlinear. But they avoid the trap of lending too much importance to short fluctuations in the climate. As a result, a recent review of 15 different historical models used since 1970 found “no evidence that the climate models evaluated in this paper have systematically overestimated or underestimated warming over their projection period.”","type":"content","url":"/blog/2020/deep-pseudoscience#non-linear-but-not-unstoppable","position":19},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Tipping points are still less important than human emissions","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#tipping-points-are-still-less-important-than-human-emissions","position":20},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Tipping points are still less important than human emissions","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"The most important points we should take away from this discussion are that 1) the tipping points that Deep Adaptation refers to threaten us far less than it claims; 2) mainstream climate science has predicted warming fairly accurately; 3) Bendell’s criticisms generally misunderstand how mainstream climate science models work; and, most importantly, 4) human activity, rather than climate feedback processes, still comprises the dominant influence on climate change.\n\nFor example, Deep Adaptation mentions the danger posed by releases of methane from Arctic permafrost. But the magnitude of methane emissions from the Arctic is dwarfed by those from human activity: about 2-10 million tons per year from permafrost and hydrates versus around 100 million tons from fossil fuel use. We also have other evidence that recent increases likely originate from increased fossil fuel use, not coming from the arctic. Therefore the key takeaway is that we are still in control of the majority of warming, both present and future. We have primary agency in the outcome, it is not at all “too late.”\n\nDeep Adaptation contains many more scientific claims which we could question: for instance, it mentions dire predictions about biodiversity loss but cites only one paper, on the extreme end of those predictions, which has been strongly criticised, and we discuss this pattern of referencing below. But Deep Adaptation brings up so many spurious claims that it would be a massive undertaking to thoroughly refute them all. Instead, for now we should focus on the fact that almost all of the climate science claims underlying Deep Adaptation’s predictions of societal collapse are wrong. Such strong predictions require strong evidence, but that evidence just isn’t there. Bendell states that his conclusion about societal collapse “flows from the conclusion of [the scientific] analysis,” (pg. 35) but this crucial scientific analysis is wrong. The rest of the paper does not make up for this failing.","type":"content","url":"/blog/2020/deep-pseudoscience#tipping-points-are-still-less-important-than-human-emissions","position":21},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Societal collapse is not guaranteed","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#societal-collapse-is-not-guaranteed","position":22},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Societal collapse is not guaranteed","lvl3":"Deep Adaptation: unfounded doomism","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"Despite the above quote from Bendell, the author has rightly stated elsewhere that climate science alone cannot confirm or deny predictions about societal collapse. After all, societies have collapsed in the past for any number of reasons to do with both social practices and the environment. Besides being true, this claim is a good hedge: it leaves Deep Adaptation potentially immune to criticism that focuses only on climate science, because to refute the paper’s claims one also has to refute the proposed scenarios of collapse. Fortunately for all of us, those scenarios are also implausible.\n\nHere is one collapse scenario that Deep Adaptation seems to take seriously: “inevitable methane release from the seafloor leading to a rapid collapse of societies will trigger multiple meltdowns of some of the world’s 400 nuclear power-stations, leading to the extinction of the human race” (pg. 19). A grim picture. How does it hold up against accepted science? We have already seen that large-scale methane release is not inevitable, and does not promise the levels of warming Deep Adaptation seems to expect — a fact which by itself disproves the claim. Yet Bendell also fails to explain why societal collapse might cause any of these plants to spontaneously meltdown. Nuclear plants do plan for “Station Blackout” events in which contact with the outside world is cut off. For example the AP1000 design being built at the Vogtle site in the US includes planning, equipment, and supplies to “provide safety functions for an indefinite time” in the event of station blackout. Newer designs are intended to be “walk away safe”.\n\nEven if these plants did somehow collectively melt down, global nuclear meltdown would not come close to causing human extinction. There are 450 nuclear reactors currently operating. If each simultaneously released a Chernobyl-level amount of radioactive material, the average radiation dose per square mile over the earth would be far less than the dose per square mile which the whole of Ukraine suffered as a result of the actual Chernobyl accident. Clearly all human life was not wiped out in soviet-era Ukraine (the resulting mortality across the country was roughly 4000 people out of 51 million), so whilst this hypothetical scenario would be a profound tragedy, it would not come anywhere near causing human extinction. (In contrast to global nuclear war, which remains an existential threat. Warming will likely exacerbate the risk of nuclear war by inflaming geopolitical tensions - a strong argument against a passive, non-interventionist stance on climate mitigation. However, if the argument is really “climate change makes nuclear war more likely”, then that should be made clear, as disarmament would also be an effective preventative measure.)\n\nIn a paper using multiple unreliable sources, this claim about nuclear meltdowns still stands out. The apocalyptic prediction of “chain nuclear meltdowns” comes from Guy McPherson, a retired ecologist who spreads misinformation about climate science in order to package and sell a “near-term human extinction” narrative. In 2008, McPherson predicted the end of civilisation by 2018, and in 2012 he predicted that global warming would kill much of humanity by 2020.\n\nMcPherson made one of his most eyebrow-raising claims in 2012: “atmospheric oxygen levels are dropping to levels considered dangerous for humans, especially in cities.” This patently false statement appears to stem from a misrepresentation of both the effects of carbon particulate air pollution, and the famous Keeling Curves of atmospheric CO2 and oxygen concentrations. Oxygen levels are indeed dropping as we combust fossil fuels — but they are dropping at an average rate of about 20 parts per million, or 0.002%, per year, so we would have to wait several hundred years to see any noticeable difference. Air pollution in cities is a serious health issue, but due to high levels of carbon particulates, not low levels of oxygen. McPherson’s atmospheric oxygen claim was made on the same webpage that Deep Adaptation cited as the source for the implausible nuclear meltdown scenario. His claims are so ludicrous that this citation casts immediate doubt on the integrity of the rest of Deep Adaptation’s references. Not unrelatedly, these references also include far fewer peer-reviewed papers than one typically finds in an academic article.\n\nMore generally, Deep Adaptation gives strikingly little explanation about how collapse might happen, beyond scattered discredited sources like McPherson, and instead relies on emotionally compelling descriptions of what collapse means for us. Bendell writes that “the evidence before us suggests that we are set for disruptive and uncontrollable levels of climate change, bringing starvation, destruction, migration, disease and war.” Any serious prediction should back up such a statement with a line of reasoning. But this is not a serious prediction — rather, it’s a highly effective emotional appeal to fear, masquerading as science. We see as much when Bendell writes that “when I say starvation, destruction, migration, disease and war, I mean in your own life. With the power down, soon you wouldn’t have water coming out of your tap. You will depend on your neighbours for food and some warmth. You will become malnourished. You won’t know whether to stay or go. You will fear being violently killed before starving to death.” (pg. 13) This description of collapse is big on pathos, but low on explanations of how or why such a scenario is inevitable, or even plausible. A close reading of this section of Deep Adaptation reveals no such explanation.\n\nIntentionally or not, Deep Adaptation strikes a skilful balance between attempting to discredit mainstream scientific sources, postulating frightening tipping points, and appealing to our fear of the future and our own justified distrust of the institutions meant to protect us, all to conceal the lack of serious evidence for its own predictions.","type":"content","url":"/blog/2020/deep-pseudoscience#societal-collapse-is-not-guaranteed","position":23},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Patterns of misinformation","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl3","url":"/blog/2020/deep-pseudoscience#patterns-of-misinformation","position":24},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Patterns of misinformation","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"We wrote earlier that the climate movement bears structural vulnerabilities to people who misinterpret climate science for a public audience. Since the vast majority of people in the climate movement don’t have time to review the scientific literature for ourselves, we can at times be duped by narratives like the ones employed in Deep Adaptation. In this way, Deep Adaptation bears striking similarities to writing by climate change deniers. A report by Geoffery Supran and Naomi Oreskes on the #ExxonKnew scandal breaks down some of the different techniques used by fossil-fuel-funded contrarians to make junk science seem convincing. In Exxon’s case the mistakes are deliberate, intended to delay regulations which would hurt their profits, but they needn’t be. The report describes five techniques of science denial, each of which appears in some form in Deep Adaptation, albeit in support of the opposite message:\n\nFake experts - Promoting dissenting non-experts as highly qualified though they have not published any actual climate research and/or received any relevant education. Guy McPherson, who has no expertise in climate science yet periodically claims prophetic predictive powers, nevertheless receives a serious treatment in Deep Adaptation.\n\nLogical fallacies - Logically flawed arguments that lead to false conclusions. Deep Adaptation argues that nonlinearities in the climate system lead to accelerating impacts, and that, therefore, society will collapse within the next couple decades. But the therefore in the argument is unsubstantiated — Deep Adaptation simply provides no logical argument to back it up. (Societal collapse is not even defined anywhere in the text.) Arguing from the premise that you “feel that [collapse] is inevitable,” to the conclusion that it actually is inevitable, is a straightforward logical fallacy.\n\nImpossible standards - Demanding unrealistic standards of certainty before acting on the science. This technique can manifest as demanding that the IPCC’s predictions all be near-perfect. Deep Adaptation claims that “The observed phenomena, of actual temperatures and sea levels, are greater than what the climate models over the past decades were predicting for our current time.” While the observed warming has been slightly higher than the mean of the results of the model runs used by the IPCC, it has fallen well within their expected range.\n\nCherry picking - Selectively choosing data that supports a desired conclusion that differs from the conclusion arising from all the available data. As discussed above, Deep Adaptation dismisses the work of the entire IPCC related to methane by citing a single paper about methane clathrates, with no mention of the multiple papers which disagree with its conclusions.\n\nConspiracy theories - Proposing a secret plan among a number of people, generally to implement a nefarious scheme such as conspiring to hide a truth or perpetuate misinformation. Deep Adaptation invites its readers “to consider the value of leaving mainstream views behind,” as if those views are inherently flawed by virtue of being mainstream. The original blog post is titled “The study on collapse they thought you should not read – yet.”\n\nScience endeavours to guard against these dangers through several methods: peer review, logical positivism, consensus, and expertise. The process of peer review helps filter out mistakes, fallacies, and ignorance of existing work. Practicing logical positivism puts the burden of proof on the author making the claim. Consensus asks for agreement across multiple analyses, because the evidence presented in a single paper may be flawed or incomplete. Academic bodies require proof of a certain level of relevant subject expertise. Because of these methods, the IPCC synthesises the work of hundreds of authors, drawing on thousands of papers in order to make its conclusions. It is possible that a piece of scientific work that hasn’t used these safeguards is still correct, but it’s far less likely, especially if it implies that the peer-reviewed science is completely wrong.\n\nDeep Adaptation sidesteps the IPCC and in doing so ends up using each of the techniques above. The paper gives at least two justifications for conducting a separate analysis of the recent scientific literature. First, it claims that “one needs real-time data on the current situation and the trends that it may infer” in order to understand the real implications of recent warming — which requires going beyond the IPCC, since it generally synthesizes science published a few years previously. We have already seen how this claim gets it wrong. In large, chaotic systems like the climate system, erratic behavior can lead to short-lived trends that merely seem catastrophic when extrapolated further into the future. Real-time data is important in that it provides more information about the climate system and allows us to constantly refine our predictions. It does not demonstrate that recent non-linear changes will continue into the future.\n\nSecond, Deep Adaptation argues that the IPCC “has done useful work but has a track record of significantly underestimating the pace of change,” concluding that it is better to turn to individual “eminent climate scientists” whose predictions have been more accurate. We’ve already shown how many of those “eminent climate scientists” have made far worse predictions than the IPCC. But even if individual experts occasionally make better predictions than the IPCC, that does not mean that their methods are better. Because climate science studies a massive, complex, and chaotic system, there is always the chance that a lucky individual will make a prediction that turns out to be closer than those made by models. Most of the lone experts cited in Deep Adaptation turned out not to be so lucky, but even if they had made spot-on predictions at one time or another, that would not give reason to trust them above the entire IPCC.\n\nDeep Adaptation relies on three further claims to evidence the bias towards conservative predictions in “mainstream” climate science. First, it claims that “the information available to environmental professionals about the state of the climate is not as frightening as it could be” due to the tendency towards cautious understatement inherent in the scientific process. Deep Adaptation cites Brysse et al. (2012), who point to a tendency towards “moderation” within the scientific community which may have contributed to the IPCC’s underestimation of sea level rise, ice sheet deterioration, and the threat of methane leakage from thawing permafrost. We agree that there are structural forces for moderation in the science community, but this does not merit the logical leaps that Bendell makes. In other words, just because the IPCC has sometimes given (slightly!) conservative predictions on particular questions, that does not justify disregarding its overwhelming body of evidence and concluding that societal collapse is inevitable. There is also a legitimate critique to be made of the way that the IPCC’s maze of qualifiers and caveats undermines its severe message. However this is not the same thing as downplaying or underestimating the actual science.\n\nDeep Adaptation also argues that climate scientists, like all people, “avoid voicing certain thoughts when they go against the social norm around them and/or their social identity,” and that this social mechanism prevents sustainability experts from speaking the full truth on climate change. According to Deep Adaptation, they might be even more prone to such psychological blocks than the average person because their investment in achieving status within existing social structures makes them “more naturally inclined to imagine reform of those systems than their upending.” As scientists, we take some small satisfaction in seeing this faulty argument, so often used to portray scientists as alarmists, now used for the opposite end. Yet multiple climate scientists made grim public statements around the release of the IPCC’s Special Report on 1.5°C of Warming (some of which can be found on XR’s own website), and many participated in mass civil disobedience with Extinction Rebellion. These forceful public actions in support of radical, science-based change show that many climate scientists are more outspoken than Bendell gives credit for.\n\nLastly, Deep Adaptation claims that none of the major institutions involved in climate research or activism have “an obvious institutional self-interest in articulating the probability or inevitability of social collapse.” This is a compelling argument in that institutions such as the World Wildlife Foundation (WWF), which must demonstrate their impact to donors, might be structurally disincentivized from admitting the full scope of the crisis for fear of seeming ineffective — although we are not aware of conclusive evidence either way, and WWF has made many dire warnings about the precipitous decline of wildlife in the past. While many people no doubt appreciate the call for bolder engagement by academics, it is unclear how institutional incentives prevent academics from making dire predictions. Some of Bendell’s own sources, like Wadhams, gained prominence in exactly this way: by making bold but weakly supported claims about, for example, Arctic ice sheet collapse.\n\nThe precautionary principle, that lack of full scientific certainty is not required before forestalling a threat, is often invoked around discussions around societal collapse. But whilst caution and urgent action on climate are certainly justified, responses should still be based on scientifically-plausible scenarios. This is the point at which the expertise of scientists is vital; they are the ones who can tell you whether or not a sudden catastrophic release of methane or human extinction due to a chain of nuclear meltdowns is actually plausible.","type":"content","url":"/blog/2020/deep-pseudoscience#patterns-of-misinformation","position":25},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Damaging the movement","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl3","url":"/blog/2020/deep-pseudoscience#damaging-the-movement","position":26},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Damaging the movement","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"This version is almost identical to the long-form article in openDemocracy, but contains a few extra footnotes which had to be dropped.daptation is undeniably an emotionally compelling work, especially when one does not dig too deep into its details and references. Maybe this explains the sense felt throughout some parts of XR that the paper has become an integral part of our motivation; that without the Deep Adaptation agenda, we would have less reason to take radical actions and make radical demands of our governments.\n\nIn fact the opposite is true: besides being based on flawed science, Deep Adaptation actively harms us, our movement, and the people we are fighting for.\n\nMost obviously, if societal collapse were truly inevitable, it would make no sense to practice mass civil disobedience against governments that would shortly fall apart. Instead, we in the climate movement should view societal collapse as a distinct possibility among a number of long-term outcomes — and work as hard as possible to prevent it by transforming our societies. The possibility of averting this disaster in fact justifies a greater extent of nonviolent rebellion than we have accomplished thus far: our governments are failing to protect us even though they can. If they couldn’t, rebellion would be both pointless and unjustified.\n\nA defeatist outlook also removes the agency from future acts of harm. A narrative that destruction is inevitable justifies continued destruction, but ignores the human choices which cause it. For example, if we assume that the Amazon rainforest is doomed to die back, we might ignore the horrible injustices that brought it to this point, or be less inclined to support the indigenous and front-line communities still fighting to defend their land. If Native ways of life were truly doomed by natural forces then it would make more sense to support those people in relocating than in fighting for lost land. But that’s not the case. Instead, extractivism remains the larger threat to Indigenous and marginalized people, and catastrophic levels of further extractivism are not a physical inevitability.\n\nDeep Adaptation hurts Extinction Rebellion and the broader climate movement in a number of other ways:\n\ndaptation is undeniably an emotionally compelling work, especially when one does not dig too deep into its details and references. Maybe this explains the sense felt throughout some parts of XR that the paper has become an integral part of our motivation; that without the Deep Adaptation agenda, we would have less reason to take radical actions and make radical demands of our governments.\n\nIn fact the opposite is true: besides being based on flawed science, Deep Adaptation actively harms us, our movement, and the people we are fighting for.\n\nMost obviously, if societal collapse were truly inevitable, it would make no sense to practice mass civil disobedience against governments that would shortly fall apart. Instead, we in the climate movement should view societal collapse as a distinct possibility among a number of long-term outcomes — and work as hard as possible to prevent it by transforming our societies. The possibility of averting this disaster in fact justifies a greater extent of nonviolent rebellion than we have accomplished thus far: our governments are failing to protect us even though they can. If they couldn’t, rebellion would be both pointless and unjustified.\n\nA defeatist outlook also removes the agency from future acts of harm. A narrative that destruction is inevitable justifies continued destruction, but ignores the human choices which cause it. For example, if we assume that the Amazon rainforest is doomed to die back, we might ignore the horrible injustices that brought it to this point, or be less inclined to support the indigenous and front-line communities still fighting to defend their land. If Native ways of life were truly doomed by natural forces then it would make more sense to support those people in relocating than in fighting for lost land. But that’s not the case. Instead, extractivism remains the larger threat to Indigenous and marginalized people, and catastrophic levels of further extractivism are not a physical inevitability.\n\nDeep Adaptation hurts Extinction Rebellion and the broader climate movement in a number of other ways:\n\nIt demotivates us.\n\nThe belief that near-term societal collapse is inevitable takes a serious toll on the mental health of many people. That’s bad in itself, and it’s also the wrong way to bring people into our movement. In fact, a recent study surveying 50,000 people found that “individuals who believe that climate change is unstoppable were less likely to engage in behaviours or support policies to address climate change.” Telling the full truth about the climate emergency is at this point a radical and extremely powerful act; telling the fatalist tale of Deep Adaptation is not. Deep Adaptation encourages a kind of paralysis when it claims that “there is no ‘effective’ response” to the crisis. Again, that’s just wrong: we know that there are many effective responses, and in order to make them happen (and happen fairly) we need democratic political power which can overcome the people and corporations opposed to action on climate change. Building that power is the main goal of the climate movement.\n\nIt is worth noting that for some people Deep Adaptation has the opposite effect. Rather than paralyzing, it resolves a painful cognitive dissonance that comes from seeing the yawning gap between our actual response to climate change and the actions we should take. In the face of this gap, Deep Adaptation says “you’re right, everyone else is catastrophically wrong, and you’d best accept the consequences.” This narrative can be a relief, despite the grief it entails, because it brings a simplifying clarity to our decisions. But, again, it’s simply wrong. Regardless of whether or not we subconsciously want to believe Deep Adaptation, actually following Bendell to his logical conclusions entails a split from reality. If it feels right, it feels right in much the same way a cult feels right to its members: by depriving them of information.\n\nIt delegitimizes us.\n\nNeedless to say, doomist scientific malpractice is also especially damaging for any movement whose first demand is to “Tell the Truth.” Using sloppy science hands ammunition to the denialist opposition, allowing them to claim that things aren’t nearly as bad as we say and dismiss our demands. By framing arguments like those presented in Deep Adaptation as the sole alleged justification for Extinction Rebellion’s protests, deniers and delayists can dismiss the entire movement as “alarmist.” Of course, most of the opposition will continue to argue in bad faith no matter what — but there is a second reason not to go beyond the bounds of reasonable justification: we lose support. Clumsily attacking the scientific consensus and quoting junk science makes it harder for major figures in science to lend their explicit support, denying XR a valuable source of legitimacy (and even legal support) for a fight we know is justified. This reasoning applies just as well for the broader climate movement, whose opposition has spent years sowing mistrust in science.\n\nIt obscures our long-term vision and planning.\n\nBelieving that the end is nigh undermines the kind of long-term planning that will be crucial to XR’s continued success. We’ve already seen this effect in practice: one of the small group of XR protestors who, against the wishes of the wider movement, made the poor strategic decision to disrupt the London underground in Canning Town reportedly referenced the debunked, doom-laden predictions of Guy McPherson as justification. Others have thoroughly critiqued how actions blind to structures of oppression severely damage the movement by setting different struggles against each other, rather than stressing how issues of climate intersect with issues of race, class and gender.\n\nA belief in near-term collapse or extinction engenders a depth of desperation which makes the kind of long-term planning we need in order to live through the climate crisis redundant. Even if the climate movement succeeds in instigating a rapid post-carbon transition, we and many other movements will have much more work to do over the next century to ensure that this transition actually happens, and happens justly. That work will require planning on multi-year or decadal scales, and ensuring that institutions like governments, banks, and universities — not to mention entire industries — contribute to the structural transformation that needs to occur. Simply passing legislation does not guarantee success; neither will mass protests, on their own, bring about the kind of wide-reaching changes to infrastructure and daily practices that we need. And that work will remain important whether or not we manage to stay below 2°C of warming. An unscientific belief in near-term societal collapse undermines this vital, far-sighted dedication.\n\nIt ignores real aspects of a potential collapse.\n\nFurther, Deep Adaptation’s vague framing of “collapse” ignores important aspects of severe societal disruption. It’s not a binary; there can be varying degrees of collapse, and different kinds of collapse. At every step, it always makes sense to keep fighting, both to prevent further damages and to recover from past ones. There is no better demonstration of a persistent fight than the ongoing resistance of indigenous peoples to colonialism and environmental destruction. Deep Adaptation takes a different kind of inspiration from indigenous struggles: Bendell references the abilities of Native leaders to come up with new forms of hope as they were confined to reservations — what Deep Adaptation calls “their new lifestyle” (15). There are myriad problems in this framing of the attempted destruction of Indigenous societies: most prominently, it ignores the genocidal intent of the US government, the willful and violent land grabs of settlers, and the resulting rampant despair in Indigenous communities. We would do far better to look for guidance on the climate crisis from indigenous resistance to colonial rule and extraction than in strategies for passive resistance. Indigenous communities have been fighting the climate and ecological fight for generations; it’s past time that settler communities step up and join that fight.\n\nSecondly, DA ignores the fact that if some form of climate-spurred collapse occurs, it will be imposed by a set of people and institutions who will likely dodge its effects, at least for a time. These are the oil majors, networks of libertarian organizations, and other industries all bent on obstructing both public understanding of climate breakdown and any real social response. Their tactics include sowing doubt about climate science, influencing public opinion against climate policy and tying the reality of climate change to contested partisan ideas. Trade groups like the American Petroleum Institute pour millions of dollars into making fossil fuels seem essential for American greatness. Together with think tanks like the Heritage Foundation and the Global Warming Policy Foundation, these organizations mobilized to defeat and weaken international climate agreements like the Kyoto Protocol and Paris Agreement. Academics and journalists have documented years of activity by this “climate change countermovement,” and it continues to fight to prolong the use of fossil fuels today. Discussing the failures of the climate movement without a serious focus on this countermovement is like blaming an army’s soldiers for losing a battle in which it was outnumbered ten-to-one.\n\nIt is incompatible with environmental and social justice.\n\nGross inequality and injustice are fundamental to climate change. The vast majority of emissions have bolstered the livelihoods of a small number of middle-class and, most of all, extremely wealthy Westerners. Meanwhile the majority of impacts will hit places like African nations or India, which emits at most a seventh as much CO2 per person as the US. Considerations of racial, social, and environmental justice therefore need to be a central part of any response, because not discussing them is still taking a stance.\n\nExtinction Rebellion has been criticised many times for a weak position on environmental justice. No such consideration is found in Deep Adaptation. (None of the words “justice”, “equality”, “racial”, or “colonial” appear once in 13,181 words of text.) The audience for Deep Adaptation is clearly Western, with collapse described as “a situation where the publishers of this journal would no longer exist” (pg. 13). This means there are only two ways to interpret the unwritten equity implications of DA’s framing of collapse, both of which directly conflict with a justice-centric response.\n\nIf it is understood that collapse in Western nations would only come alongside much worse outcomes in developing ones, then the passive response advocated by Deep Adaptation is tantamount to lifeboat ethics. Work on adaptation and resilience are extremely important, but they are most important in the places which will be hit hardest. Bendell’s narrative of inevitable collapse conveniently ignores the debt owed by wealthy nations to exploited ones, regardless.\n\nIf instead we are meant to understand that no-one is safe from the spectre of societal collapse, and that we are therefore all doomed together and equally, then this is the polar opposite of a justice-oriented approach. Aside from being completely inconsistent with climate science, this framing is harmful in the same way that “All Lives Matter” as a response to “Black Lives Matter” is harmful. It willfully ignores both the fact that the crisis will disproportionately impact marginalised groups and the continuing role of racially-charged colonial-era power structures in a system which has failed to address that crisis.\n\nIt distracts from what is most important.\n\nThe narrative that collapse is now inevitable also distracts readers from the most important responses to the climate crisis. We need to remove socio-political power from the Exxons, Kochs, and Trumps who are actively exacerbating the climate crisis and work towards defining and enacting systemic changes throughout the rest of society. If we psychologically abandon all the complex institutions and structures we live in and rely on (as Deep Adaptation advocates) then we will not see reasons to repurpose them. But that’s exactly what we need to do.","type":"content","url":"/blog/2020/deep-pseudoscience#damaging-the-movement","position":27},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Moving past post-scientism","lvl3":"Damaging the movement","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl4","url":"/blog/2020/deep-pseudoscience#moving-past-post-scientism","position":28},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl4":"Moving past post-scientism","lvl3":"Damaging the movement","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"So how can we in the climate movement do better?\n\nWe should publicly disavow the message that near-term collapse is inevitable, or that climate-induced total human extinction is plausible. There is uncertainty, but not so much that one can claim anything will happen. Stepping back from previous unsupported statements would help show that although the movement is broad, they are still fundamentally motivated by the truth.\n\nWe should be much stricter and more careful with science messaging. Major spokespeople should be citing science appropriately, and defer to expertise on the physical science and direct impacts. When slip-ups are inevitably made, statements should be corrected based on scientific feedback instead of doubling down. Representatives should understand the difference between junk science and the valid reasons to be worried about spiralling long-term societal impacts.\n\nExtinction Rebellion, \n\n350.org, and other groups now have an international network of sympathetic scientists and other experts to call upon. The movement is in a unique position to confront political and cultural incumbents across the UK with the emergency message, delivered by those with the necessary understanding and authority. We must embrace this network, and use it.\n\nIn the longer term XR could organise public debates on the science of the emergency. Inviting discussion by authority figures in science would show that the movement is open to criticism and adaptation, while still shifting the window of public debate towards the necessary urgency. It would also help highlight the mainstream media’s failure so far to communicate the scale of the crisis.\n\nWe are right to sound the alarm, and to maximise our impact we need to ensure that our message is grounded in sound science. Greta Thunberg exhorts us to “Unite behind the science”, because the truth is bad enough.","type":"content","url":"/blog/2020/deep-pseudoscience#moving-past-post-scientism","position":29},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Our choice is not new","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"type":"lvl3","url":"/blog/2020/deep-pseudoscience#our-choice-is-not-new","position":30},{"hierarchy":{"lvl1":"The faulty science, doomism, and flawed conclusions of Deep Adaptation","lvl3":"Our choice is not new","lvl2":"The faulty science, doomism, and flawed conclusions of Deep Adaptation"},"content":"There is a Greek myth about Persephone, the vegetation goddess abducted to the Underworld by Hades, god of Death. Fearing that he would lose his stolen bride, the god planned to bind Persephone to the Underworld by forcing her to eat its food.\n\nPersephone fasted until her hunger compelled her to eat. When Persephone finally begged her captor for food, he presented her with three choices. On the right he placed the ambrosia of the underworld, a sweet food laced with melancholy that leads those who eat it to resent the living and embrace death. On the left he placed a vessel filled with nectar from his own garden, which dulls painful memories of the world above. In the middle he placed a bitter pomegranate, whose seeds awaken one to the horror of death and provoke a restless longing for life.\n\nHades knew that Persephone would find the ambrosia and nectar all but irresistible. Once she ate either her fate would be sealed. But Persephone chose to eat the pomegranate, accepting the accompanying anguish in exchange for the chance to return home.\n\nThe environmental movement faces a similar choice. On the one hand we feel justifiably alienated from a society which appears damned by its own excess. Some of us abandon it as irredeemable. On the other hand, seeing the enormous scale of the obstacles we face, our world constricts. Forgetting the possibility of revolutionary change, we content ourselves with small concessions and political maneuvering.\n\nHades’ nectar is the post-political environmentalism of ineffectual carbon markets and greenwashing: a seductive barbiturate. Ending the fossil fuel industry’s dominance could by some measures require the largest expropriation of private wealth in history; facing odds like that it can seem foolish, if not disastrous, to gamble on the chance of revolutionary change. When we drink the nectar our lives become simpler, our ends narrower.\n\nDeep Adaptation is ambrosia, and like all ambrosia the kernel of truth within Deep Adaptation only makes it more enticing. Things are far worse than newspapers, cable news, politicians, and so forth commonly let on. We face a more awful reality with every round of failed COP negotiations. There are real reasons for despair.\n\nYet despite all the talk in Deep Adaptation of accepting the harsh reality of social collapse, the idea has a sickly-sweet attraction: believing that social collapse is inevitable provides a level of certitude and clarity which makes many choices simpler. But we must remember that the choice to believe in an inevitable collapse is itself a luxury, a form of escapism only available to those with the time and resources to plan for its consequences.\n\nChoosing the pomegranate means accepting the truth with all the pain it brings. The truth is that most of the world has been systematically lied to, betrayed, and exploited to prop up a failing system of fossil-fueled growth which will not go easily. We know who is to blame. We know that we need to reform the government from the city to the international level to meet the demands of global and intergenerational justice. This knowledge is far from comfortable because it requires real work: not just nonviolent disobedience, but educating and training others, and gaining political power. As an argument and a philosophy Deep Adaptation ultimately requires none of these things. By rejecting Deep Adaptation we recommit ourselves to understanding how we can live well within planetary limits, imparting that knowledge to others, and excising the rot and paralysis from our politics.\n\nBy Tom Nicholas (\n\n), Galen Hall, and Colleen Schmidt.\n\nThomas Nicholas is a PhD student in computational plasma physics at the University of York, and member of Extinction Rebellion Scientists. Galen Hall is a researcher at the Climate and Development Lab at Brown University. Colleen Schmidt is a recent graduate of Columbia university, where her research focused on plant ecology.\n\n(Thank you to Professor Julia Steinberger, Megan Ruttan Walker, Dr Chris Wymant and Dr Alison Green for input, and to Dr David McKay, Professor Richard Betts, Professor Rich Pancost, Dr James Dyke, Dr Scott Archer-Nicholls and many other scientists for checking the claims in this article.)\n\n(For more detail on what climate tipping points do and do not imply for the planet, the science outreach project \n\nclimatetippingpoints​.info is an excellent resource. It is run by two postdoctoral researchers in climate science based at Stockholm University, and was seed-funded by a project of the Engineering Physical Science Research Council - the body which funds most physical science research in the UK. It gives detailed descriptions of the science of methane clathrates and arctic albedo, including fact-checking of incorrect claims made in Deep Adaptation and elsewhere.)","type":"content","url":"/blog/2020/deep-pseudoscience#our-choice-is-not-new","position":31},{"hierarchy":{"lvl1":"Oxford University Divestment Explained"},"type":"lvl1","url":"/blog/2020/oxford-divests","position":0},{"hierarchy":{"lvl1":"Oxford University Divestment Explained"},"content":"Oxford University’s academic congregation has passed a landmark divestment motion.\nYou can read the student newspaper \n\ncoverage here.\n\nI wasn’t directly involved in this achievement, but I do personally know a lot of the key people who worked hard to make this happen, and I want to clarify a few things:\n\nIt’s not greenwashing\n\nThe motion requires\n\nrestriction on all direct investment in any fossil fuel exploration and extraction companies and an immediate restriction on new investments in funds which invest primarily in fossil fuel extraction companies (including coal, oil and gas, exploration and extraction, as an addition to Oxford University’s existing restriction on thermal coal and tar sands).\n\nThere will also be oversight to ensure it actually happens.\nA role will be created on the Investment Committee for oversight by someone who\n\nshall have recent and relevant expertise in investment management, as well as recent and relevant expertise in climate-conscious investment.\n\nThe careful consideration of the wording to close common loopholes, and structurally-aware oversight measures to ensure follow-up, should hopefully mean the motion is followed as intended.\n\nSeveral Colleges have yet to divest\n\nThe University of Oxford has a complex financial structure, with individual “Colleges” being akin to financially-independent Halls of Residence, as well as the main university having its own finances.\nThis announcement is that the main university has divested by instructing their investment management division to follow the changes in the motion that was passed.\n\nThis is huge because the central university is extremely rich (~£3bn is being divested), and about half the colleges allow their funds to be controlled by the central University Endowment Fund.\n\nHowever, it’s not a complete victory yet, because several of the colleges have not yet divested their own endowments (most of which are tens or hundreds of millions).\n(It’s an example of how disproportionately powerful an institution Oxford is that most of its “poorer” colleges still have larger endowments than many entire universities.)\nMultiple colleges have divested over several years, but the decentralised system makes this a Kafkaesque slog for student campaigners to get them to divest one by one.\n\nThis wasn’t just the recent oil price drop\n\nCampaigning on this has been going on for years, several colleges divested a few years ago, and this particular motion was organised and proposed (and expected to pass) last term, before the major oil price drop due to Coronavirus.\nObviously it won’t have hurt that oil is now a worse investment, but it would probably have passed anyway thanks to all the activism and organising that went on.\n\nIn fact we can quantify how much money the student campaign has moved out of fossil fuels in the lead up to this announcement.\nThis year alone Oxford pulled out of £30 million in fossil fuel investments, bringing the overall percentage of funds invested (indirectly) in fossil fuels from 1.7% to 0.6%.\nOver the past 10 years while the campaign has kept pressure on the endowment managers they have brought the percentage from over 7% of the 3 billion to 0.6%.\nThat’s more than £200 million in fossil fuel divestment that had already happened before this announcement!\nThe remaining 0.5% percent of the endowment is expected to continue to fall with these new measures, in particular because the engagement criteria is so strict.\n\nIt’s more than just direct investments\n\nThis is all referring to money invested through multiple managed funds, none of these investments are directly into e.g. Shell.\nThe managed funds are allocated across the economy (as is common in investing to try and spread risk), including some fraction in fossil fuels.\n\nThese subtleties mean that some universities’ “divestments” are much less complete than what Oxford has just announced, because those universities only prohibited direct investments into fossil fuels.\n\nSome key climate academics pulled their weight\n\nOne of the breakthroughs for the student divestment campaign was getting some very influential Oxford academics on board pushing for divestment, both publicly and internally.\nThe two notable ones are Myles Allen (climate physicist and lead author of the 2018 IPCC 1.5C report) and Cameron Hepburn (Environmental Economics Professor).\n\nIt’s also a victory for student direct action\n\nA small and extremely dedicated student campaign has been pushing for divestment for years, and was a key part of this.\nYou can read the Oxford Climate Justice Campaign’s \n\npress release here.\n\nA notable action recently was the occupation of the central quadrangle of St John’s college (who unfortunately still haven’t divested).\nA group (made of students but also supported by the local XR group) literally camped on the grass inside the college for a week, and even brought a \n\nbig wooden boat (the RSS David Attenborough), which brought much attention and embarrassment to the college for not divesting.\n\n“Engagement” with fossil fuel companies vs “Divestment”\n\nSome divestment motions are basically absolutist statements of intent, which explicitly assume that fossil fuel companies cannot exist in any form in a net-zero emissions world.\nHowever some of the \n\n(carefully-worded) language in the motion is about “engaging” with fossil fuel companies.\nThis means if a company can demonstrate serious progress towards becoming carbon-neutral then the university wouldn’t consider them to be requiring of divestment.\nThe only (non-greenwashing) way a fossil fuel company could become net-zero in reality is through large-scale deployment geologic Carbon Capture and Storage, and this engagement clause is designed to encourage development of those technologies.\nThe motion isn’t supposed to allow space for these companies to simply pay for lots of ineffectual tree-planting offsets, for example.\nIt’s worth emphasising (and well-understood by the motion’s authors) that not a single fossil fuel company has a credible hard net zero target yet, let alone strategy.\nThis topic is worth several articles in itself, but suffice to say that those two academics mentioned in particular have complex and expert views on the role of fossil fuel companies in the future and a lot of the more detailed thought on this point reflects their involvement.\n\nThe real value of the move is hard to quantify\n\nWhile divesting £3bn is not to be sniffed at, this is possibly even more important as a signal.\nThe mere word Oxford is an international symbol of learning and expertise, and if the University of Oxford thinks fossil fuels should be ditched, then that sends a strong signal that it’s probably about time.\nA lot of climate-related research happens at Oxford, and this motions helps strengthen the arguments of the academics who are crying out for the implications of their research to be acted upon.\nFurthermore, if Oxford University leveraging its academic expertise and industry connections leads (through “engagement”) to some fossil fuel company breaking rank and seriously developing CCS technology, the impact of that would be measured in Gigatons of CO2.\nThese kind of downstream socio-economic impacts are harder to predict, but potentially where the real value is.","type":"content","url":"/blog/2020/oxford-divests","position":1},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree"},"type":"lvl1","url":"/blog/2022/easy-ipcc-datatree","position":0},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree"},"content":"","type":"content","url":"/blog/2022/easy-ipcc-datatree","position":1},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl2","url":"/blog/2022/easy-ipcc-datatree#easy-ipcc-part-1-multi-model-datatree","position":2},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"By Tom Nicholas (\n\n) and Julius Busecke (\n\n)\n\n(Originally posted on the \n\nPangeo Medium blog)\n\nIn this series of blog posts we aim to quickly reproduce a panel from one of the \n\nkey figures (fig. 9.3a) of the \n\nIPCC AR6 Report from the raw climate model simulation data, using Pangeo cloud-native tools. This figure is critical: it literally shows our best estimate of the future state of the climate given certain choices by humanity.\n\nThis figure shows both the historical and projected global average temperature of the ocean’s surface, as used in different \n\nGlobal Climate Models. The projections are for possible trajectories of greenhouse gas emissions based on various socio-economic pathways (\n\nNeill et al. 2017).\n\nHowever, the process of creating it is potentially very cumbersome: it involves downloading large files, idiosyncratic code to produce intermediate datafiles, and lots more code to finally plot the results. The open sharing of \n\ncode used for the latest IPCC report inspired us to try to make this process easier for everyone.\n\nThe numerical model output produced under the \n\nCoupled Model Intercomparison Project 6 (CMIP6) output is publicly available (distributed by the \n\nEarth System Grid Federation), so the scientific community should be able to reproduce this plot from scratch, and then build upon it as the underlying science advances.\n\nDue to recent advances in software and the migration of the CMIP6 data archive into the cloud, we are now able to produce this panel rapidly from raw data using easily available public cloud computing.\n\nWorking with many CMIP6 datasets at once can be cumbersome because each model has different size and coordinates. In this blogpost we highlight the new \n\nxarray-datatree package, which helps organize the many datasets in CMIP6 into a single tree-like data structure.\n\nThere have been other attempts to \n\nanalyze, \n\norganize, and \n\nvisualize multi-model experiments, but here we prioritize modularity, flexibility, extensibility, and using domain-agnostic tools. - []() very important graph\n- Currently very difficult to make from raw data\n- The data is openly available, so we should be able to make this plot from scratch (and modify it along the way)\n- We can do this process better with new tools ()\n- Part of the problem is organizing many related datasets with different sizes and coordinates\n- Datatree (+ others) makes this easier \n\nWe were able to reproduce parts of this graph in a \n\nlive demo at \n\nScipy 2022 using cloud-hosted data, but felt we could do it much more succinctly with datatree! - Describe the graph shown in detail (mention Julius' scipy presentation) ","type":"content","url":"/blog/2022/easy-ipcc-datatree#easy-ipcc-part-1-multi-model-datatree","position":3},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Getting the data","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl3","url":"/blog/2022/easy-ipcc-datatree#getting-the-data","position":4},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Getting the data","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"A key component of this workflow is having the large CMIP6 datasets available to open near-instantly from a public cloud store. The \n\nPangeo/ESGF Cloud Data Working Group maintains \n\nAnalysis-Ready Cloud-Optimized versions of a large part of the CMIP6 archive as \n\nZarr stores as public datasets both on \n\nGoogle Cloud Storage and \n\nAmazon S3. (This alone is a huge advancement for open science, but just a part of what we’re showing today.)\n\nFor the sake of demonstration we will load a small subset of sea surface temperature data for three models . We’ll load both the historical baseline, as well as the SSP1.26 (“best case”) and SSP5.85 (“worst case”) future socio-economic pathways: a total of 9 datasets. (We’ll tackle scaling up to all the available data in a future blog post.) - Can get it from cloud zarr storage using pangeo stack (already vastly easier than in the past)\n- **Code to open it** import xarray as xr\nxr.set_options(keep_attrs=True)\nimport gcsfs\nfrom xmip.preprocessing import rename_cmip6\n\ncmip_stores = {\n'IPSL-CM6A-LR/historical':  'gs://cmip6/CMIP6/CMIP/IPSL/IPSL-CM6A-LR/historical/r4i1p1f1/Omon/tos/gn/v20180803/',\n'MPI-ESM1-2-LR/historical': 'gs://cmip6/CMIP6/CMIP/MPI-M/MPI-ESM1-2-LR/historical/r4i1p1f1/Omon/tos/gn/v20190710/',\n'CESM2/historical':         'gs://cmip6/CMIP6/CMIP/NCAR/CESM2/historical/r4i1p1f1/Omon/tos/gn/v20190308/',\n'IPSL-CM6A-LR/ssp126':      'gs://cmip6/CMIP6/ScenarioMIP/IPSL/IPSL-CM6A-LR/ssp126/r4i1p1f1/Omon/tos/gn/v20191121/',\n'IPSL-CM6A-LR/ssp585':      'gs://cmip6/CMIP6/ScenarioMIP/IPSL/IPSL-CM6A-LR/ssp585/r4i1p1f1/Omon/tos/gn/v20191122/',\n'MPI-ESM1-2-LR/ssp126':     'gs://cmip6/CMIP6/ScenarioMIP/MPI-M/MPI-ESM1-2-LR/ssp126/r4i1p1f1/Omon/tos/gn/v20190710/',\n'MPI-ESM1-2-LR/ssp585':     'gs://cmip6/CMIP6/ScenarioMIP/MPI-M/MPI-ESM1-2-LR/ssp585/r4i1p1f1/Omon/tos/gn/v20190710/',\n'CESM2/ssp126':             'gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2/ssp126/r4i1p1f1/Omon/tos/gn/v20200528/',\n'CESM2/ssp585':             'gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2/ssp585/r4i1p1f1/Omon/tos/gn/v20200528/'\n}\n\ndatasets = {\n    name: rename_cmip6(xr.open_dataset(path, engine=\"zarr\")).load()\n    for name, path in cmip_stores.items()\n}\n\nCMIP vocabulary explainer:\nThe words “model” and “experiment” have a specific meaning in CMIP lingo. Within CMIP there are many different datasets produced by various modeling centers around the world. Each of these centers has one or more model setups (source_id in official CMIP language), which are used to produce multiple simulations. For example the simulations for IPSL-CM6A-LR are all produced by the \n\nInstitut Pierre-Simon Laplace in France.\nDifferent simulations are run under certain protocols which prescribe conditions (e.g. greenhouse gas forcings), and these are called experiments. The experiments we’ve loaded here are 'historical', 'ssp126' & 'ssp585'.\n\nThese nine datasets are all similar, e.g. they have the same dimension names (freshly cleaned thanks to another package of ours - \n\nxMIP) and variable names, but they have different sizes along dimensions: horizontal dimensions differ due to varying resolution and time dimensions differ because the historical baseline run is longer than the scenarios by a varying amount.print(datasets[\"CESM2/ssp585\"].sizes)\nprint(datasets[\"IPSL-CM6A-LR/ssp585\"].sizes)Frozen({'y': 384, 'x': 320, 'vertex': 4, 'time': 1032, 'bnds': 2})\nFrozen({'y': 332, 'x': 362, 'vertex': 4, 'time': 1032, 'bnds': 2})\n\nFor instance the \"x\" and \"y\" dimensions here (roughly corresponding to latitude and longitude) do not match between the CESM2 model and the IPSL model. Hence they cannot be combined into a single N-dimensional array or a single \n\nxarray.Dataset! - Data is tricky because can't be combined into a N-D array (without regridding)\n- Means xarray.Dataset alone is inconvenient ","type":"content","url":"/blog/2022/easy-ipcc-datatree#getting-the-data","position":5},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Introducing datatree","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl3","url":"/blog/2022/easy-ipcc-datatree#introducing-datatree","position":6},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Introducing datatree","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"The CMIP6 datasets are clearly related, however. They have a hierarchical order - grouped by model and then by experiment. Each dataset also contains very similar variables. Wouldn’t it be nice to treat them all as one related set of data, and analyze them together?\n\nThe \n\nxarray-datatree package enables this. It arranges these related datasets into a Datatree object, allowing them to be manipulated together. A DataTree object can be thought of as a recursive dictionary of xarray.Dataset objects, with additional methods for performing computations on the entire tree at once. You can also think of it as an in-memory representation of an entire netCDF file, including all netCDF groups. Because it is in-memory your analysis does not require saving intermediate files to disk.\n\nNote: The pip-installable package xarray-datatree provides the class DataTree, importable from the module datatree. From here on we will refer interchangeably to these as just “datatree”. - But what if trees\n- **Code to put our data into a tree** from datatree import DataTree\n\ndt = DataTree.from_dict(datasets)\n\nprint(f\"Size of data in tree = {dt.nbytes / 1e9 :.2f} GB\")Size of data in tree = 4.89 GBprint(dt)DataTree('None', parent=None)\n├── DataTree('IPSL-CM6A-LR')\n│   ├── DataTree('historical')\n│   │       Dimensions:      (y: 332, x: 362, vertex: 4, time: 1980, bnds: 2)\n│   │       Coordinates:\n│   │         * time         (time) object 1850-01-16 12:00:00 ... 2014-12-16 12:00:00\n│   │       Dimensions without coordinates: y, x, vertex, bnds\n│   │       Data variables:\n│   │           area         (y, x) float32 16.0 16.0 16.0 ... 1.55e+08 3.18e+07 3.18e+07\n│   │           lat_bounds   (y, x, vertex) float32 -84.21 -84.21 -84.21 ... 50.11 49.98\n│   │           lon_bounds   (y, x, vertex) float32 72.5 72.5 72.5 72.5 ... 73.0 72.95 73.0\n│   │           lat          (y, x) float32 -84.21 -84.21 -84.21 ... 50.23 50.01 50.01\n│   │           lon          (y, x) float32 72.5 73.5 74.5 75.5 ... 73.05 73.04 73.0 72.99\n│   │           time_bounds  (time, bnds) object 1850-01-01 00:00:00 ... 2015-01-01 00:00:00\n│   │           tos          (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n│   │       Attributes: (12/54)\n│   │           CMIP6_CV_version:       cv=6.2.3.5-2-g63b123e\n│   │           Conventions:            CF-1.7 CMIP-6.2\n│   │           EXPID:                  historical\n│   │           NCO:                    \"4.6.0\"\n│   │           activity_id:            CMIP\n│   │           branch_method:          standard\n│   │           ...                     ...\n│   │           tracking_id:            hdl:21.14100/dc824b97-b288-4ec0-adde-8dbcfa3b0095\n│   │           variable_id:            tos\n│   │           variant_label:          r4i1p1f1\n│   │           status:                 2019-11-09;created;by nhn2@columbia.edu\n│   │           netcdf_tracking_ids:    hdl:21.14100/dc824b97-b288-4ec0-adde-8dbcfa3b0095\n│   │           version_id:             v20180803\n│   ├── DataTree('ssp126')\n│   │       Dimensions:      (y: 332, x: 362, vertex: 4, time: 1032, bnds: 2)\n│   │       Coordinates:\n│   │         * time         (time) object 2015-01-16 12:00:00 ... 2100-12-16 12:00:00\n│   │       Dimensions without coordinates: y, x, vertex, bnds\n│   │       Data variables:\n│   │           area         (y, x) float32 16.0 16.0 16.0 ... 1.55e+08 3.18e+07 3.18e+07\n│   │           lat_bounds   (y, x, vertex) float32 -84.21 -84.21 -84.21 ... 50.11 49.98\n│   │           lon_bounds   (y, x, vertex) float32 72.5 72.5 72.5 72.5 ... 73.0 72.95 73.0\n│   │           lat          (y, x) float32 -84.21 -84.21 -84.21 ... 50.23 50.01 50.01\n│   │           lon          (y, x) float32 72.5 73.5 74.5 75.5 ... 73.05 73.04 73.0 72.99\n│   │           time_bounds  (time, bnds) object 2015-01-01 00:00:00 ... 2101-01-01 00:00:00\n│   │           tos          (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n...\n\nWe created the whole tree in one go from our dictionary of nine datasets, using the keys of the dict to organize the resulting tree. DataTree objects are structured similarly to a UNIX filesystem, so a key in the dict such as 'IPSL-CM6A-LR/historical' will create a node called IPSL-CM6A-LR and another child node below it called 'historical'. This approach means the DataTree.from_dict method can create trees of arbitrary complexity from flat dictionaries.\n\nIf instead we wanted to create the tree manually, you can do that by setting the .children attributes of each node explicitly, assigning other tree objects to build up a composite result.\n\nYou can see that the resulting tree structure is grouped by model first, and then by the experiment. (This choice is somewhat arbitrary and we could have chosen to group first by experiment and then model.)\n\nWhile we printed the string representation of the tree for this blog post, if you’re doing this in a jupyter notebook you will actually automatically get an interactive HTML representation of the tree, where each node is collapsible.\n\nWe say that we have created one “tree”, where each “node” within that tree (optionally) contains the contents of exactly one xarray.Dataset, and also has a node name, a “parent” node, and can have any number of “child” nodes. For more information on the data model datatree uses, see \n\nthe documentation.\n\nWe can actually access nodes in the tree using path-like syntax too, for exampleprint(dt[\"/CESM2/ssp585\"])DataTree('ssp585', parent=\"CESM2\")\n    Dimensions:      (y: 384, x: 320, vertex: 4, time: 1032, bnds: 2)\n    Coordinates:\n      * y            (y) int32 1 2 3 4 5 6 7 8 9 ... 377 378 379 380 381 382 383 384\n      * x            (x) int32 1 2 3 4 5 6 7 8 9 ... 313 314 315 316 317 318 319 320\n      * time         (time) object 2015-01-15 13:00:00 ... 2100-12-15 12:00:00\n    Dimensions without coordinates: vertex, bnds\n    Data variables:\n        lat          (y, x) float64 -79.22 -79.22 -79.22 -79.22 ... 72.2 72.19 72.19\n        lat_bounds   (y, x, vertex) float32 -79.49 -79.49 -78.95 ... 72.41 72.41\n        lon          (y, x) float64 320.6 321.7 322.8 323.9 ... 318.9 319.4 319.8\n        lon_bounds   (y, x, vertex) float32 320.0 321.1 321.1 ... 320.0 320.0 319.6\n        time_bounds  (time, bnds) object 2015-01-01 02:00:00.000003 ... 2101-01-0...\n        tos          (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n    Attributes: (12/48)\n        Conventions:            CF-1.7 CMIP-6.2\n        activity_id:            ScenarioMIP\n        branch_method:          standard\n        branch_time_in_child:   735110.0\n        branch_time_in_parent:  735110.0\n        case_id:                1735\n        ...                     ...\n        tracking_id:            hdl:21.14100/68b741c9-b8f8-479d-b48c-6853b1c71e56...\n        variable_id:            tos\n        variant_info:           CMIP6 SSP5-8.5 experiments (2015-2100) with CAM6,...\n        variant_label:          r4i1p1f1\n        netcdf_tracking_ids:    hdl:21.14100/68b741c9-b8f8-479d-b48c-6853b1c71e56...\n        version_id:             v20200528\n\nYou can see that every node is itself another DataTree object, i.e. the tree is a recursive data structure.\n\nThis structure not only allows the user full flexibility in how to set up and manipulate the tree, but it also behaves like a filesystem that people are already very familiar with.\n\nBut datatrees are not just a neat way to organize xarray datasets; they also allows us to operate on datasets in an intuitive way. - Explain how the tree structure is set up like a file path. We did this in the setup step by giving appropriate keys!\n- Talk about how that's nice and neat ","type":"content","url":"/blog/2022/easy-ipcc-datatree#introducing-datatree","position":7},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Timeseries","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl3","url":"/blog/2022/easy-ipcc-datatree#timeseries","position":8},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Timeseries","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"In this case the basic quantity we want to compute is a global mean of the ocean surface temperature. For this we can use the built-in .mean() method. - Explain `.mean()` method (mapping over each node) timeseries = dt.mean(dim=[\"x\", \"y\"])\n\nThis works exactly like the familiar xarray method, and averages the temperature values over the horizontal dimensions for each dataset on any level of the tree. :exploding_head:\n\nAll arguments are passed down, so this .mean will work so long as the data in each node of the tree has a \"x\" and a \"y\" dimension to average over.\n\nLets confirm our results by plotting a single dataset from the new tree of timeseries: - **Constructing + Plotting a timeseries for each individual dataset** timeseries['/IPSL-CM6A-LR/ssp585'].ds['tos'].plot()\n\nAnd indeed, we get a nice timeseries. Once you select a node, you get back an xarray.Dataset using .ds and can do all the good stuff you already know (like plotting).\n\nFor comparison, here is the relevant part of \n\nJulius’ code from the Scipy 2022 live demo that did not use datatree:timeseries_hist_datasets = []\ntimeseries_ssp126_datasets = []\ntimeseries_ssp245_datasets = []\ntimeseries_ssp370_datasets = []\ntimeseries_ssp585_datasets = []\n\nfor k,ds in data_timeseries.items():\n    # Separate experiments\n    out = ds.convert_calendar('standard')\n    out = out.sel(time=slice('1850', '2100'))# cut extended runs\n    out = out.assign_coords(source_id=ds.source_id)\n    if ds.experiment_id == 'historical':\n        # CMIP\n        if len(out.time)==1980:\n            timeseries_hist_datasets.append(out)\n        else:\n            print(f\"found {len(ds.time)} for {k}\")\n    else:\n        #scenarioMIP\n        if len(out.time)!=1032:\n            print(f\"found {len(out.time)} for {k}\")\n            # print(ds.time)\n        else:\n            if ds.experiment_id == 'ssp126':\n                timeseries_ssp126_datasets.append(out)\n            elif ds.experiment_id == 'ssp245':\n                timeseries_ssp245_datasets.append(out)\n            elif ds.experiment_id == 'ssp370':\n                timeseries_ssp370_datasets.append(out)\n            elif ds.experiment_id == 'ssp585':\n                timeseries_ssp585_datasets.append(out)            \n\nconcat_kwargs = dict(\n    dim='source_id',\n    join='override',\n    compat='override',\n    coords='minimal'\n) \ntimeseries_hist = xr.concat(timeseries_hist_datasets, **concat_kwargs)\ntimeseries_ssp126 = xr.concat(timeseries_ssp126_datasets, **concat_kwargs)\ntimeseries_ssp245 = xr.concat(timeseries_ssp245_datasets, **concat_kwargs)\ntimeseries_ssp370 = xr.concat(timeseries_ssp370_datasets, **concat_kwargs)\ntimeseries_ssp585 = xr.concat(timeseries_ssp585_datasets, **concat_kwargs)\n\n😬 Definitely not as nice as a one-liner...\n\nIt’s worth understanding what happened under the hood when calling .mean() on the tree. We can reproduce the behavior of .mean() by passing a simple function to the built-in .map_over_subtree() method. - **Show `map_over_subtree` verbose example** def mean_over_space(ds):\n    return ds.mean(dim=[\"x\", \"y\"])\n\ndt.map_over_subtree(mean_over_space)\n\nThe function mean_over_space that we supplied gets applied to the data in each and every node in the tree automatically. This will return the exact same result as above, but with this method users can map arbitrary functions over each dataset in the tree, as long as they consume and produce xarray.Datasets. You can even \n\nmap functions with multiple inputs and outputs, but that is an advanced use case which we will use at the end of this post.\n\nYou can already see that this enables very intuitive mapping of custom functions without lengthy writing of loops.","type":"content","url":"/blog/2022/easy-ipcc-datatree#timeseries","position":9},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Saving output","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl3","url":"/blog/2022/easy-ipcc-datatree#saving-output","position":10},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Saving output","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"Before we go further, we might want to save our aggregated timeseries data to use later. DataTree supports saving to and loading from both netCDF files and Zarr stores.from datatree import open_datatree\n\ntimeseries.to_zarr('cmip_timeseries')  # or netcdf, with any group structure\n\nroundtrip = open_datatree('cmip_timeseries', engine=\"zarr\")\nprint(roundtrip)DataTree('None', parent=None)\n├── DataTree('CESM2')\n│   ├── DataTree('historical')\n│   │       Dimensions:      (vertex: 4, time: 1980, bnds: 2)\n│   │       Coordinates:\n│   │         * time         (time) object 1850-01-15 12:59:59.999997 ... 2014-12-15 12:0...\n│   │       Dimensions without coordinates: vertex, bnds\n│   │       Data variables:\n│   │           lat          float64 ...\n│   │           lat_bounds   (vertex) float32 ...\n│   │           lon          float64 ...\n│   │           lon_bounds   (vertex) float32 ...\n│   │           time_bounds  (time, bnds) object ...\n│   │           tos          (time) float32 ...\n...\n\nNodes are saved as netCDF or Zarr groups, meaning that you can now work smoothly with xarray and multi-group files, a \n\nlong-term sticking point for many xarray users. Being able to now work with such files without leaving xarray or python helps retains backwards compatibility and portability.","type":"content","url":"/blog/2022/easy-ipcc-datatree#saving-output","position":11},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Calculating anomalies","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl3","url":"/blog/2022/easy-ipcc-datatree#calculating-anomalies","position":12},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Calculating anomalies","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"Note on Climate Anomalies: Coupled climate models often exhibit ‘biases’, which means that the ocean temperatures at the starting point for e.g. the historical and future experiments will differ from model to model. For the purpose of this scientific analysis however we are most interested in the relative change from that starting point when a forcing (e.g. increased greenhouse gases) is applied. Conventionally we compute the anomaly relative to a given reference value. In this case the reference value is the global ocean temperature averaged over the years 1950-1980 in the corresponding historical experiment. Therefore we want to compute and subtract this reference value for each model separately so that each model-specific bias is removed. - Explain anomalies, base period, model biases  - Demonstrate on non-tree  - **Find the per-model deviation over the tree** \n\nIn practice we need to iterate over each model, pick the historical experiment, select the base period, and average it. We then subtract that resulting value from all experiments of the corresponding model and rearrange the resulting anomaly timeseries into a new tree.anomaly = DataTree()\nfor model_name, model in timeseries.children.items():\n    \n    # model-specific base period as an xarray.Dataset\n    base_period = model[\"historical\"].ds.sel(time=slice('1950','1980')).mean('time')\n    \n    anomaly[model_name] = model - base_period   # subtree - Dataset\n\nWe have used a few features of datatree here. We loop over the .children attribute of the DataTree, which iterates through each sub-node in turn in a dictionary-like manner.\n\nWe find the reference value (base_period) by extracting the data in a node as an xarray.Dataset via .ds, giving us access to all of xarray’s normal API for computations.\n\nOur tree of results (anomaly) is constructed by creating an empty tree and then adding branches to this new tree whilst we loop through models.\n\nIn the final line of this for loop we have subtracted an xarray.Dataset from a DataTree - this operation works node-wise, i.e. the dataset is subtracted from every data-containing node in the tree. - (Could be neater - future work to do tree broadcasting) \n\nNote: In general, we could imagine having rules for operations involving any number of trees of any node structure, analogous to numpy array broadcasting. This would make the above anomaly calculation much more succinct. However at the moment, datatree only supports tree @ dataset-like operations and tree @ tree-like operations, where the trees must have the same node structure.","type":"content","url":"/blog/2022/easy-ipcc-datatree#calculating-anomalies","position":13},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Plotting","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl3","url":"/blog/2022/easy-ipcc-datatree#plotting","position":14},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Plotting","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"Alright, so the final step here is to actually plot the data. We can use the map_over_subtree approach to plot each individual simulation onto the same matplotlib axes: - **Create plot** import matplotlib.pyplot as plt\n\nfrom datatree import map_over_subtree\n\nfig, ax = plt.subplots()\n\n\n@map_over_subtree\ndef plot_temp(ds, original_ds):\n    \n    if ds:\n        label = f'{original_ds.attrs[\"source_id\"]} - {original_ds.attrs[\"experiment_id\"]}'\n        ds['tos'].rolling(time=2*12).mean().plot(ax=ax, label=label)\n\n    return ds\n\n   \nplot_temp(anomaly, timeseries)\nax.legend()\n\nFor this we created a matplotlib figure and axis and then used the built-in xarray plotting capability to add each timeseries to the existing matplotlib.Axes object.\n\nInstead of using the .map_over_subtree method on the DataTree class, we instead used map_over_subtree as a function decorator, which promoted plot_temp to a function that will act over all nodes of any given tree. This decorator is happy to map over multiple trees simultaneously, which we used to extract the metadata for labelling each timeseries.\n\nNote: The way we extracted this metadata for plotting was a little awkward - we can imagine improving this in future versions of datatree.\n\nThe fact that we operate on an xarray dataset also allows us to convieniently smooth the timeseries (by a 2 year rolling mean) to remove the seasonal cycle.\n\nLooking at the figure produced above, we’ve successfully replicated the basic features of the IPCC plot that we wanted! And it took us very few lines of code overall, especially compared to how it’s often done. In future posts we will improve this figure, adding more data and maybe making it interactive.\n\n(Eagle-eyed climate scientist readers will have noticed that our figure is not actually quite correct - we forgot to correctly weight our temperature averages by the areas of the grid cells in each model! We will deal with this subtlety and other more complex custom aggregations in a future blog post.)","type":"content","url":"/blog/2022/easy-ipcc-datatree#plotting","position":15},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Takeaways","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl3","url":"/blog/2022/easy-ipcc-datatree#takeaways","position":16},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Takeaways","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"Datatree is a general tool. Whilst here we used it on CMIP6 data, we learnt about capabilities that might be useful in other projects. We saw that datatree can:\n\nOrganize heterogeneous data\n\nSeamlessly extend xarray’s functionality\n\nHandle I/O with nested groups\n\nApply operations over whole trees\n\nPerform more complex and custom operations\n\nRetain fine-grained control\n\nReduce overall lines of code","type":"content","url":"/blog/2022/easy-ipcc-datatree#takeaways","position":17},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Future","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"type":"lvl3","url":"/blog/2022/easy-ipcc-datatree#future","position":18},{"hierarchy":{"lvl1":"Easy IPCC part 1: Multi-Model Datatree","lvl3":"Future","lvl2":"Easy IPCC Part 1: Multi-Model Datatree"},"content":"This blogpost is the first in a series. We plan to write more to discuss:\n\nUsing \n\nintake with datatree, where the trees could act like in-memory catalogs,\n\nSuporting more complex aggregation operations, such as using .weighted to weight an operation on one tree with weights from another tree,\n\nScaling out to a much larger set of CMIP6 data with dask,\n\nQuick plotting of data in a whole tree, maybe even making it interactive.","type":"content","url":"/blog/2022/easy-ipcc-datatree#future","position":19},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint"},"type":"lvl1","url":"/blog/2022/pint-xarray","position":0},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint"},"content":"(This post was originally published on the \n\nxarray blog.)\n\nTLDR: Pint-Xarray supports unit-aware operations by wrapping \n\npint arrays, so your code can automatically track the physical units that your data represents:distance = xr.DataArray(10).pint.quantify(\"metres\")\ntime = xr.DataArray(4).pint.quantify(\"seconds\")\n\ndistance / timeOut:\n<xarray.DataArray ()>\n<Quantity(2.5, 'meter / second')>","type":"content","url":"/blog/2022/pint-xarray","position":1},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Units are integral to science"},"type":"lvl2","url":"/blog/2022/pint-xarray#units-are-integral-to-science","position":2},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Units are integral to science"},"content":"All quantities in science have units, whether explicitly or implicitly. (And even dimensionless quantities like ratios still technically have units.)\n\nGetting our units right is finicky, and can very easily go unnoticed in our code.\n\nEven worse, the consequences of getting units wrong can be huge!\n\nThe most famous example of a units error has to be NASA’s $125 million \n\nMars Climate Orbiter, which in 1999 burned up in the Martian atmosphere instead of successfully entering orbit around Mars.\nA trajectory course correction had gone wrong, and the error was eventually traced back to a units mismatch: the engineers at Lockheed Martin expressed impulse in \n\npound-force seconds, whereas the engineers at JPL assumed the impulse value their part of the software received was in SI newton seconds.\n\n\n\nNewspaper cartoon depicting the incongruence in the units used by NASA and Lockheed Martin scientists that led to the Mars Climate Orbiter disaster.\n\nWe should take stories like this seriously: If we can automatically track units we can potentially eliminate a whole class of possible errors in our scientific work...","type":"content","url":"/blog/2022/pint-xarray#units-are-integral-to-science","position":3},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Pint tracks units"},"type":"lvl2","url":"/blog/2022/pint-xarray#pint-tracks-units","position":4},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Pint tracks units"},"content":"There are a few packages for handling units in python (notably \n\nunyt and \n\nastropy.units), but for technical reasons we began units integration in Xarray with \n\npint.\nThese various packages work by providing a numerical array type that acts similarly to a NumPy array, and is intended to plug in and replace the raw NumPy array (a so-called “duck array type”).\n\nPint provides the Quantity object, which is a normal numpy array combined with a pint.Unit:q = np.array([6, 7]) * pint.Unit('metres')\nprint(repr(q))Out:\n<Quantity([6 7], 'meter')>\n\nPint Quantities act like NumPy arrays, except that the units are carried around with the arrays, propagated through operations, and checked during operations involving multiple quantities.","type":"content","url":"/blog/2022/pint-xarray#pint-tracks-units","position":5},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Xarray now wraps Pint"},"type":"lvl2","url":"/blog/2022/pint-xarray#xarray-now-wraps-pint","position":6},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Xarray now wraps Pint"},"content":"Thanks to the \n\ntireless work of Xarray core developer \n\nJustus Magin, you can now enjoy this automatic unit-handling in Xarray!\n\nOnce you create a unit-aware Xarray object (see below for how) you can see the units of the data variables displayed as part of the printable representation.\nYou also immediately get the key benefits of Pint:\n\nUnits are propagated through arithmetic, and new quantities are built using the units of the inputs:distance = xr.DataArray(10).pint.quantify(\"metres\")\ntime = xr.DataArray(4).pint.quantify(\"seconds\")\n\ndistance / timeOut:\n<xarray.DataArray ()>\n<Quantity(2.5, 'meter / second')>\n\nDimensionally inconsistent units are caught automatically:apples = xr.DataArray(10).pint.quantify(\"kg\")\noranges = xr.DataArray(200).pint.quantify(\"cm^3\")\n\napples + orangesOut:\nDimensionalityError: Cannot convert from 'kilogram' ([mass]) to 'centimeter ** 3' ([length] ** 3)\n\nUnit conversions become simple:walk = xr.DataArray(500).pint.quantify('miles')\n\nwalk.pint.to('parsecs')Out:\n<xarray.DataArray ()>\n<Quantity(2.6077643524162074e-11, 'parsec')>\n\nWith these features, you can build code that automatically propagates units and converts them where necessary to stay consistent.\nFor example, the problem of the NASA orbiter could have been prevented by explicitly converting to the correct units at the startdef jpl_trajectory_code(impulse):\n\n    # Defensively check units first\n    impulse = impulse.pint.to(\"Newton * seconds\")\n\n    # This function we called here will only compute the correct result if supplied input in units of Newton-seconds,\n    # but that's fine because we already converted the values to be in the correct units!\n    propagated_position = some_rocket_science(impulse)\n\n    return propagated_position\n\nNote: We are adding \n\nnew features to make specifying the units of parameters of existing library functions more slick.\n\nIn the abstract, tracking units like this is useful in the same way that labelling dimensions with Xarray is useful: it helps us avoid errors by relieving us of the burden of remembering arbitrary information about our data.","type":"content","url":"/blog/2022/pint-xarray#xarray-now-wraps-pint","position":7},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Quantifying with pint-xarray"},"type":"lvl2","url":"/blog/2022/pint-xarray#quantifying-with-pint-xarray","position":8},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Quantifying with pint-xarray"},"content":"The easiest way to create a unit-aware Xarray object is to use the helper package we made: \n\npint-xarray.\nOnce you import pint_xarray you can access unit-related functionality via .pint on any DataArray or Dataset (this works via \n\nXarray’s accessor interface).\n\nAbove we have seen examples of quantifying explicitly, where we specify the units in the call to .quantify().\nWe can do this for multiple variables too, and we can also pass pint.Unit instances:ds = xr.Dataset({'a': 2, 'b': 10})\n\nds.pint.quantify({'a': 'kg',\n                  'b': pint.Unit('moles')})Out:\n<xarray.Dataset>\nDimensions:  ()\nData variables:\n    a        int64 [kg] 2\n    b        int64 [mol] 10\n\nAlternatively, we can quantify from the object’s .attrs, automatically reading the metadata which xarray objects carry around.\nIf nothing is passed to .quantify(), it will attempt to parse the .attrs['units'] entry for each data variable.\n\nThis means that for scientific datasets which are stored as files with units in their attributes (which netCDF and Zarr can do for example), using Pint with Xarray becomes as simple as:import pint_xarray\n\nds = open_dataset(filepath).pint.quantify()","type":"content","url":"/blog/2022/pint-xarray#quantifying-with-pint-xarray","position":9},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Dequantifying"},"type":"lvl2","url":"/blog/2022/pint-xarray#dequantifying","position":10},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Dequantifying"},"content":"To convert our pint arrays back into NumPy arrays, we can use .dequantify.\nThis will strip the units from the arrays and replace them into the .attrs['units'] of each variable.\nThis is useful when we want to save our data back to a file, as it means that the current units will be preserved in the attributes of a netCDF file (or Zarr store etc.), as long as we just do ds.pint.dequantify().to_netcdf(...).","type":"content","url":"/blog/2022/pint-xarray#dequantifying","position":11},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Dask integration"},"type":"lvl2","url":"/blog/2022/pint-xarray#dask-integration","position":12},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Dask integration"},"content":"So Xarray can wrap Dask arrays, and now it can wrap Pint quantities… Can we use both together? Yes!\n\nYou can get a unit-aware, Dask-backed array either by .pint.quantify()-ing a chunked array, or you can .pint.chunk() a quantified array.\n(If you have Dask installed, then open_dataset(f, chunks={}).pint.quantify() will already give you a Dask-backed, quantified array.)\nFrom there you can .compute() the Dask-backed objects as normal, and the units will be retained.\n\n(Under the hood we now have an xarray.DataArray wrapping a pint.Quantity, which wraps a dask.array.Array, which wraps a numpy.ndarray.\nThis “multi-nested duck array” approach can be generalised to include other array libraries (e.g. scipy.sparse), but requires \n\ncoordination between the maintainers of the libraries involved.)","type":"content","url":"/blog/2022/pint-xarray#dask-integration","position":13},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Unit-aware indexes"},"type":"lvl2","url":"/blog/2022/pint-xarray#unit-aware-indexes","position":14},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Unit-aware indexes"},"content":"We would love to be able to promote Xarray indexes to Pint Quantities, as that would allow you to select data subsets in a unit-aware manner like:da = xr.DataArray(name='a', data=[0, 1, 2], dims='x', coords={'x': [1000, 2000, 3000]})\nda = da.pint.quantify({'a': 'Pa', 'x': 'm'})\n\nda.pint.sel(x=2 * 'km')\n\nUnfortunately this will not possible until the ongoing work to extend Xarray to support \n\nexplicit indexes is complete.\n\nIn the meantime pint-xarray offers a workaround. If you tell .quantify the units you wish an index to have, it will store those in .attrs[\"units\"] instead.time = xr.DataArray([0.1, 0.2, 0.3], dims='time')\ndistance = xr.DataArray(name='distance',\n                        data=[10, 20, 25],\n                        dims=['time'],\n                        coords={'time': time})\ndistance = distance.pint.quantify({'distance': 'metres',\n                                   'time': 'seconds'})\nprint(distance.coords['time'].attrs)Out:\n{'units': <Unit('second')>}\n\nThis allows us to provide conveniently wrapped versions of common xarray methods like .sel, so that you can still select subsets of data in a unit-aware fashion like this:distance.pint.sel(time=200 * pint.Unit('milliseconds'))Out:\n<xarray.DataArray 'distance' ()>\n<Quantity(20, 'meter')>\nCoordinates:\n    time     float64 200.0\n\nObserve how the .pint.sel operation has first converted 200 milliseconds to 0.2 seconds, before finding the distance value that occurs at a time position of 0.2 seconds.\n\nThis wrapping is currently necessary for any operation which needs to be aware of the units of a dimension coordinate of the dataarray, or any xarray operation which relies on an external library (such as calling scipy in .integrate).","type":"content","url":"/blog/2022/pint-xarray#unit-aware-indexes","position":15},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"CF-compliant units for geosciences with cf-xarray"},"type":"lvl2","url":"/blog/2022/pint-xarray#cf-compliant-units-for-geosciences-with-cf-xarray","position":16},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"CF-compliant units for geosciences with cf-xarray"},"content":"Different fields tend to have different niche conventions about how certain units are defined.\nBy default, Pint doesn’t understand all the unusual units and conventions we use in geosciences.\nBut \n\nPint is customisable, and with the help of \n\ncf-xarray we can teach it about these geoscience-specific units.\n\nIf we import cf_xarray.units (before import pint_xarray) then we can quantify example climate data from the \n\nPangeo Project’s CMIP6 catalog:import xarray as xr\nimport cf_xarray.units\nimport pint_xarray\n\nds = xr.open_dataset('gs://cmip6/CMIP6/CMIP/NCAR/CESM2-FV2/historical/r2i1p1f1/Amon/sfcWind/gn/v20200226/', engine='zarr')\nds = ds.pint.quantify()\n\nsquared_wind = ds['sfcWind'] ** 2\nsquared_wind.pint.unitsOut:\n<Unit('meter ** 2 / second ** 2')>\n\nHere (thanks to cf_xarray) pint has successfully interpreted the CF-style units 'm s-1', then automatically changed them when we squared the wind speed.","type":"content","url":"/blog/2022/pint-xarray#cf-compliant-units-for-geosciences-with-cf-xarray","position":17},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Plotting"},"type":"lvl2","url":"/blog/2022/pint-xarray#plotting","position":18},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Plotting"},"content":"We can complete our real-world example by plotting the data in its new units:import cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\np = squared_wind.isel(time=\"2014-01\").plot(\n    subplot_kws=dict(projection=ccrs.Orthographic(-80, 35), facecolor=\"gray\"),\n    transform=ccrs.PlateCarree(),\n)\np.axes.set_global()\np.axes.coastlines()\nplt.show()\n\nwhere xarray.plot has detected the Pint units automatically.","type":"content","url":"/blog/2022/pint-xarray#plotting","position":19},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Conclusion"},"type":"lvl2","url":"/blog/2022/pint-xarray#conclusion","position":20},{"hierarchy":{"lvl1":"Unit-aware arithmetic in Xarray, via pint","lvl2":"Conclusion"},"content":"Please have a go! You will need xarray (v2022.03.0+), pint (0.18+), and pint-xarray (0.3+).\n\nPlease also tell us about any bugs you find, or documentation suggestions you have on the \n\nXarray or \n\npint-xarray issue trackers.\nIf you have usage questions you can raise them there, on the \n\nXarray discussions page, or on the \n\nPangeo Discourse forum.\n\nThe work here to allow Xarray to wrap Pint objects is part of a \n\nbroader effort to generalise Xarray to handle a wide variety of data types (so-called “duck array wrapping”).\nAlong with the incoming \n\nsupport for flexible indexes, we are excited for all the new features that this will enable for Xarray users!","type":"content","url":"/blog/2022/pint-xarray#conclusion","position":21},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray"},"type":"lvl1","url":"/blog/2023/cubed-xarray","position":0},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray"},"content":"_TLDR: Xarray can now wrap a range of parallel computation backends, including \n\nCubed, a distributed serverless framework designed to limit memory usage.","type":"content","url":"/blog/2023/cubed-xarray","position":1},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Introduction"},"type":"lvl2","url":"/blog/2023/cubed-xarray#introduction","position":2},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Introduction"},"content":"One of Xarray’s most powerful features is parallel computing on larger-than-memory datasets through its \n\nintegration with Dask.\nWe love Dask - it’s an incredibly powerful tool, a big part of Xarray’s appeal, and central to the scalability of the \n\nPangeo stack.\n\nHowever, the vision of seamlessly processing any array workload at arbitrary scale is incredibly challenging, and its good to explore alternative implementations that might complement Dask’s approach.\n\nCubed is one such alternative - a serverless framework for array processing developed by Tom White.\nTom comes to the Xarray community via working on the statistical genetics toolkit, \n\nsgkit, and brings with him \n\nconsiderable experience with distributed systems.\nWe will discuss how Cubed works, what its novel design can offer, and demo using it on a realistic problem.\n\nWe will also explain how Xarray has been \n\ngeneralized to support wrapping Cubed, opening the door to yet more alternative implementations of chunked parallel array computing in Xarray.","type":"content","url":"/blog/2023/cubed-xarray#introduction","position":3},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"The challenge of managing memory usage"},"type":"lvl2","url":"/blog/2023/cubed-xarray#the-challenge-of-managing-memory-usage","position":4},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"The challenge of managing memory usage"},"content":"Managing RAM usage in big computations is very hard.\nIn the wider world there are many different frameworks for performing distributed computations, but most of them can’t make strict guarantees about limiting memory usage.\nIn practice this means that configuring workers or adjusting details of the computation plan to avoid accidentally exceeding memory limits is often the bottleneck for getting real-world computations to run successfully.\n\nThe Dask developers have been making great progress on this problem, releasing big \n\nchanges to the distributed scheduler, and recently a new approach to \n\nrechunking using a P2P algorithm.\n\nWe’ve been working on a different approach: \n\nCubed’s framework is specifically designed from the ground up to avoid this memory problem.","type":"content","url":"/blog/2023/cubed-xarray#the-challenge-of-managing-memory-usage","position":5},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"What is Cubed?"},"type":"lvl2","url":"/blog/2023/cubed-xarray#what-is-cubed","position":6},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"What is Cubed?"},"content":"Cubed is a library for processing N-dimensional distributed arrays using a bounded-memory serverless model.\nWhat does that mean?\nLet’s take these terms one at a time:\n\nN-dimensional arrays\n\nCubed provides a cubed.Array class which \n\nimplements the \n\nPython Array API Standard.\nLike numpy/cupy/dask arrays etc., Cubed arrays understand basic N-dimensional operations such as indexing, aggregations, and linear algebra.\nNote that Cubed does not understand data structures other than arrays, for example DataFrames.\n\nDistributed\n\nCubed Arrays are divided into chunks, allowing the work of processing your dataset to be split up and distributed across many machines.\n\nAs well as exposing a .chunks attribute in exactly the same way as Dask, cubed provides functions for applying chunk-aware operations such as map_blocks, blockwise, and apply_gufunc.\nCubed owes another huge debt to Dask here for developing and exposing these useful abstractions.\n\nBounded-memory\n\nCubed aims to perform operations without exceeding a pre-set bound on the total RAM usage of the computation.\nBy building all common chunked array operations out of basic operations with known memory requirements, Cubed guarantees that the entire computation executes in a series of bounded-memory steps.\n\nCubed implements all chunked array operations in terms of just two “primitive operations”: \n\nblockwise and \n\nrechunk.\nBlockwise applies an in-memory function across multiple blocks of multiple inputs, and can be used to express a range of tensor operations.\nRechunk simply changes the chunking pattern of an array without altering its shape.\n\nFIGURE: Dependency tree for chunked array operations within Cubed, \n\nfrom cubed’s documentation.\nNotice that all array API operations (white) can be implemented for chunked arrays in terms of “core ops” (orange) and eventually just two “primitive ops” - blockwise and rechunk.\n\nBoth blockwise and rechunk can be performed using algorithms which only require loading one chunk from each input array into memory at any time.\nIn the case of rechunk the \n\nalgorithm comes from the Pangeo Rechunker package, whose model was a direct inspiration for Cubed.\nAs each chunk is of known size, the RAM usage for processing each chunk can be reliably estimated in advance.\n\nTherefore the expected RAM usage of any Cubed computation can be projected in advance, and compared against the known system resources available (specified via the \n\ncubed.Spec object).\n\nServerless\n\nServerless computing is a model in which some service runs a users tasks for them on an on-demand basis, allowing the user to avoid worrying about the details of how in fact their tasks are being run on real servers somewhere.\n\nCubed breaks down every array operation into a set of independent tasks, each of which reads one chunk from cloud storage and operates on it. These tasks are a good fit for a serverless cloud computing model.\n\nHowever, unlike on a cluster the serverless functions cannot communicate with each other directly over the network.\nCubed instead writes an intermediate result to persistent storage between each array operation, saving a history of steps via Zarr.\nThis approach is a generalisation of the way Rechunker’s algorithm writes and reads data to an intermediate Zarr store as it executes.\n(Cubed actually implements some optimizations to avoid writing every intermediate array to disk, but \n\nmore are possible.)\n\nThe benefit of serverless for analytics jobs is that instead of the user creating, managing, and shutting down a cluster manually (which lives either locally or on one or more remote servers), they simply specify the tasks they want performed and hand those tasks off to some paid-for serverless cloud computing service, such as Google Cloud Functions or AWS Lambda.\nThe tradeoff is that this approach is much more I/O intensive, and serverless nodes are generally more expensive than cluster nodes.\n\nCubed interfaces with a range of serverless cloud services and uses them to execute the chunk-level operations required to process the whole array.\nThis also provides chunk-level parallelism - all the tasks for creating one array are embarrasingly parallel so can be run simultaneously.\nIn other words, if your Cubed operation is writing its result to a Zarr store with 1000 chunks, Cubed will compute that step with 1000 parallel processes simultaneously by default.\n\nFIGURE: There are a large number of commercial cloud services which offer serverless computing services.\nAll of the services pictured here are supported by the \n\nLithops framework, which is only one of many tools Cubed can use to execute computations.\nSo far, using the Lithops executor, \n\nCubed has been tested on both AWS Lambda and Google Cloud Functions.\n(It has also been run on \n\nAWS using Modal and \n\nDataflow using the Beam executor.)","type":"content","url":"/blog/2023/cubed-xarray#what-is-cubed","position":7},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Sounds cool, how do I try it?"},"type":"lvl2","url":"/blog/2023/cubed-xarray#sounds-cool-how-do-i-try-it","position":8},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Sounds cool, how do I try it?"},"content":"You can try using Cubed arrays directly or you can use them seamlessly through Xarray, in the same way you use Dask.\n\nFirst conda install -c conda-forge cubed-xarray, which will install Cubed, the latest version of Xarray (v2023.5.0), and the glue package \n\ncubed-xarray.\n\nTo run Cubed in serverless mode you need to \n\nset up integration with a cloud service.\nAlternatively you can just run Cubed locally, but you won’t see many of the benefits (local execution is the default if you don’t specify an executor below).\n\nBefore creating a Cubed array you need a Spec, which tells Cubed (a) how much RAM you think a process should need to compute any one chunk, and (b) where to store all the temporary files it will create as it executes.\nThis location can be a local directory or a cloud bucket.from cubed import Spec\n\nspec = Spec(work_dir='tmp', allowed_mem='1GB')\n\nNow you can create a cubed-backed Xarray object in the same way you would create a dask-backed one:ds = open_dataset(\n    'data.zarr', \n    chunked_array_type='cubed',\n    from_array_kwargs={'spec': spec})\n    chunks={},\n)\n\nThe only difference is the addition of the new chunked_array_type kwarg (which defaults to 'dask'), and the new from_array_kwargs dict, which allows passing arbitrary kwargs down to the constructor that creates the underlying chunked array.\nThe chunks kwarg plays exactly the same role as it does when creating Dask arrays. (This also works with the .chunk methods on Dataset/DataArray, using the same keyword arguments.)\n\nNow you specify your analysis using normal Xarray code, right up until you need to compute the result.\nIn a similar way to how Dask’s .compute method accepts a scheduler argument, Cubed’s .compute accepts an executor argument.\nYou need to specify which executor to use by importing it, and configuration options for the cloud service would also be passed at this point.from cubed.runtime.executors.lithops import LithopsDagExecutor\n\nds.compute(executor=LithopsDagExecutor())","type":"content","url":"/blog/2023/cubed-xarray#sounds-cool-how-do-i-try-it","position":9},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"How does this integration work?"},"type":"lvl2","url":"/blog/2023/cubed-xarray#how-does-this-integration-work","position":10},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"How does this integration work?"},"content":"Xarray has been coupled to Dask \n\nsince \n\n2015. How did we generalize it?\n\nMuch of the integration works immediately because Cubed implements the \n\nPython Array API Standard, and Xarray can \n\nwrap any numpy-like array (see the blog post on \n\nwrapping pint.Quantity arrays).\n\nChunked arrays implement additional attributes (e.g. .chunks) and methods (e.g. .rechunk), but also require special functions (e.g. blockwise, map_blocks) for mapping in-memory numpy functions over different chunks.\nThese key functions are useful abstractions invented within Dask, and \n\nCubed implements its own versions of each of them.\n\nChunked array implementations specify how to dispatch computations to the correct implementation of these functions by subclassing a new ChunkManagerEntrypoint base class.\nAny array library implementing an array-like API and defining chunk-aware functions can hook in.\nUsing entrypoints means the possible backends can be automatically discovered, which is why we don’t need to explicitly import anything from the cubed-xarray package.","type":"content","url":"/blog/2023/cubed-xarray#how-does-this-integration-work","position":11},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Demo: The “Quadratic Means” problem"},"type":"lvl2","url":"/blog/2023/cubed-xarray#demo-the-quadratic-means-problem","position":12},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Demo: The “Quadratic Means” problem"},"content":"(\n\nFull notebook here)\n\nTo succinctly demonstrate the relative advantages of Cubed, we’ll choose a workload which is representative of what happens when cluster-based frameworks struggle with memory management.\n\nThe “Quadratic Means” problem is a simplified version of calculating the cross-product mean from the climatological anomalies of two variables.\nIt requires taking products of two chunked scalar variables U and V and then aggregating the result along the chunked dimension.\nIt is a good example of problem that is simple to express, but creates a dask graph complex enough to trigger suboptimal performance of the distributed scheduler.","type":"content","url":"/blog/2023/cubed-xarray#demo-the-quadratic-means-problem","position":13},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl3":"Dask results","lvl2":"Demo: The “Quadratic Means” problem"},"type":"lvl3","url":"/blog/2023/cubed-xarray#dask-results","position":14},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl3":"Dask results","lvl2":"Demo: The “Quadratic Means” problem"},"content":"\n\nFIGURE: Dask’s memory usage for a simplified version of the “quadratic means” workload, at a number of different scales.\nNotice the RAM usage increasing proportially with dataset size, until the workers spill to disk for the largest dataset (1.5TB).\nIf the workload were much larger, or the cluster much smaller, this would have crashed. This was run on 20 16GB RAM workers using Coiled.\n\nWe ran this workload on Coiled, for datasets of increasing size, using the latest version of dask.distributed, both with and without P2P rechunking.\nWe see that as the problem size increases, the peak memory required by the dask cluster increases too.\nOnce the peak memory required is larger than the RAM available to the workers, they continue loading data, but spill it to disk (spilled memory not shown in the plot).\nIf the total memory available is smaller than the dataset size, the cluster will crash (here we used 20 workers to ensure there was enough disk space to complete the 1.5TB workload).\n\nThis type of failure occurs sometimes for real array workloads when using dask, but is a symptom of the general difficulty of memory management when using any cluster-based framework.\n(The performance of on this particular case could almost certainly be improved by changes to dask’s distributed scheduler, and indeed \n\ndask.distributed has gotten hugely better at memory management recently.\nHere though we are just using this case as a specific example of a type of failure that is very difficult to solve in general.)","type":"content","url":"/blog/2023/cubed-xarray#dask-results","position":15},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl3":"Cubed results","lvl2":"Demo: The “Quadratic Means” problem"},"type":"lvl3","url":"/blog/2023/cubed-xarray#cubed-results","position":16},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl3":"Cubed results","lvl2":"Demo: The “Quadratic Means” problem"},"content":"Trying this problem with Cubed (using the Lithops executor and Google Cloud Functions), we find that Cubed successfully completed the workload at all scales, including the 1.5TB workload.\nRemarkably, it did so using only 1.3GB of RAM per serverless container, staying below the threshold of 2GB we set in the cubed.Spec.\n\nIf we take just the largest version of this workload (1.5TB), we can visualize the relationship between the various types of memory Cubed accounts for over the course of the computation.\n\nFIGURE: Cubed’s actual memory usage vs projected memory usage, for the largest workload (1.5TB). Cubed successfully processed 1.5TB of data whilst only using a maximum of 1.3GB of RAM per serverless container, staying well below the given limit of 2GB of RAM.\nThis was run using Lithops on Google Cloud Functions, which provisioned thousands of serverless containers each with 2048MB of RAM.\n\nYou can see again that the \n\nprojected memory usage is below the allowed memory usage (else Cubed would have raised an exception before the job even started running), and the actual peak memory used was lower still. We’ve also plotted the reserved memory, which is a parameter intended to account for the memory usage of the executor itself (i.e. Lithops here), and was estimated by measuring beforehand using \n\ncubed.measure_reserved_memory().\n\nOne obvious tradeoff for this memory stability is that Cubed took a lot longer to complete - roughly 4x longer then dask for the 1.5TB workload (45m 22s vs 11m 26s). We will come back to discuss this shortly.\n\nFinally it’s interesting to look at Cubed’s equivalent of the task graph. To calculate one array (the product <UV>_t from the quadratic means problem), Cubed’s “Plan” for processing 1.5TB of data looks like this:\n\nFIGURE: Cubed Plan (i.e. task graph) for processing 1.5TB of array data.\nThis graph corresponds to a single xarray variable (UV) in the “quadratic means” workload. Each node corresponds to an intermediate Zarr array that gets written to disk as the computation progresses, separated by either rechunk (rc) or blockwise (bw) steps.\nNotice that the number of nodes in the graph is not proportional to the number of chunks in the array.\n\nThe task graph is at the array-level, meaning that it does not grow proportionally to the number of chunks.\nThe final linear series of steps does get longer with problem scale, but only logarithmically, as it represents the number of steps in a \n\ntree-like reduction necessary to reduce that quantity of data.\n(This Plan is somewhat similar to Dask’s \n\n“High Level Graph”, but array-native, and designed to avoid the problem of over-burdening a scheduler.)","type":"content","url":"/blog/2023/cubed-xarray#cubed-results","position":17},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Pros and cons of Cubed’s model"},"type":"lvl2","url":"/blog/2023/cubed-xarray#pros-and-cons-of-cubeds-model","position":18},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Pros and cons of Cubed’s model"},"content":"Cubed uses a completely different paradigm to Dask (and other frameworks), and so has various advantages and disadvantages. Let’s discuss the obvious disadvantages first.","type":"content","url":"/blog/2023/cubed-xarray#pros-and-cons-of-cubeds-model","position":19},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl3":"Disadvantages","lvl2":"Pros and cons of Cubed’s model"},"type":"lvl3","url":"/blog/2023/cubed-xarray#disadvantages","position":20},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl3":"Disadvantages","lvl2":"Pros and cons of Cubed’s model"},"content":"Writing to persistent storage is slow\nIn general writing and reading to persistent storage (disk or object store) is slow, and doing it repeatedly even more so.\nWhilst there is scope for considerable \n\noptimization within Cubed, the model of communicating between processes by writing to disk will likely always be slower for many problems than communicating using RAM like dask does.\nOne idea for mitigating this might be to use a very fast storage technology like Redis to store intermediate results.\n\nSpinning up cloud services can be slow\nThere is also a time cost to spinning up the containers in which each task is performed, which can vary considerably between cloud services.\n\nHigher monetary cost\nWe estimate that Cubed cost about an order of magnitude more to run than Dask on Coiled did for this 1.5TB workload (order 10 vs order 1).\nSpecific numbers and lengthier discussion are given in the \n\naccompanying notebook.\nThis is not very suprising, but the lesson should also be that (like performance) estimating cloud costs is complicated, workload-specific, provider-specific, and contextual.\nIt is also worth noting that costs for both could be lowered.\nFuture optimizations to Cubed could reduce the number of intermediate Zarr steps required, and in a serverless paradigm you cannot accidentally be charged for a cluster you leave running idle.\nOn the other hand \n\nCoiled can use ARM chips, as well as \n\nspot instances, both of which could lower its cost further.\n\nStill a prototype\nCubed is still young, and whilst fairly feature-complete it has not been rigorously tested on very large datasets, or on several of the cloud services it could theoretically run on.\nThere will be bugs, or things which are simply not yet implemented.\nThe \n\ncubed-xarray integration is similarly experimental, and any \n\npresent incompatibilities will likely cause silent coercion to numpy arrays.\nContributions to Cubed to are welcome!\n\nOnly array processing\nCubed is far less general than other parallel processing frameworks like Dask, Spark, or Apache Beam.\nIt only supports array processing, and cannot support other data structures like Dataframes, \n\ncustom collections, or \n\narbitrary computation graphs.","type":"content","url":"/blog/2023/cubed-xarray#disadvantages","position":21},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl3":"Advantages","lvl2":"Pros and cons of Cubed’s model"},"type":"lvl3","url":"/blog/2023/cubed-xarray#advantages","position":22},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl3":"Advantages","lvl2":"Pros and cons of Cubed’s model"},"content":"Bounded-memory\nWe’ve already seen how this works, but being able to complete a large-scale job that previously crashed or had to be manually split up would be an extremely useful feature on its own.\n\nResumable\nSaving each intermediate result to disk means the progress of the computation is saved even if the cluster goes down or power is lost.\nCubed allows you to resume a stalled computation in a new process to finish a long job.\n\nNo cluster to manage\nA serverless design means that the user does not having to deploy and manage a cluster at all.\nArguably conceptually simpler, this model also means less boilerplate code, no error-prone deployment step, and only paying for computation you actually do, not for the time the cluster is up.\n\nHorizontal scalability\nCubed is designed with “\n\nhorizontal scaling” in mind, meaning bringing many machines to bear on the problem at once.\n(As opposed to “vertical scaling” - utilizing a bigger machine.)\nZarr can support as many concurrent writes as there are chunks, and the serverless framework should be able to smoothly launch and manage almost any number of parallel workers.\nThis has yet to be tested at truly massive scale however.\nCubed also avoids another potential scaling bottleneck of having too many individual tasks for a single scheduler process to manage.\nThe computation is represented as an array-level graph, so the graph size doesn’t scale with the number of chunks, and there is no complex scheduler to over-burden.\n\nVarious runtimes\nCubed can execute its computation plans using a range of executors, opening up a big space of possibilities.\nAs well as running locally, Cubed can convert its plans into \n\nBeam Pipelines and run on Google Cloud Dataflow, use \n\nLithops to dispatch to a range of serverless providers, or use the \n\nModal client to start-up containers in literally a second.\nIt should even be possible to write a \n\nDask executor for Cubed, similar to what exists in the Rechunker package.\nThis might go some way to giving the benefits of Cubed but with lower cost and speed.\nOne could also imagine \n\nrunning Cubed on another type of cluster.\n\nOverall one could think of different parallel array processing libraries as complementary, best-suited to different problems or to running on different systems.","type":"content","url":"/blog/2023/cubed-xarray#advantages","position":23},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Range of options"},"type":"lvl2","url":"/blog/2023/cubed-xarray#range-of-options","position":24},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Range of options"},"content":"There is a saying that there are \n\nonly three important numbers in programming: 0, 1, and infinity.\nHaving gone from 1 to 2 options of parallel backends, can we go to N?\n\nThat is the situation in the database analytics world.\nThere SQL (Structured Query Language) queries form a common high-level way of expressing user intent, but the actual execution of a query on a database can be performed using a huge range of technologies, known as \n\nQuery Engines.\nThese projects compete to determine which can get the best performance on \n\nvarious standardised benchmarks.\nIn the example above Xarray is playing a similar role to SQL as a high-level query language, allowing users to easily try out different array backends depending on their problem.\n\nHere we’ve focused on Cubed, but this work opens the door to using other alternative distributed array processing frameworks such as \n\nRamba, \n\nArkouda, \n\nAesara, or even others yet to be developed...\nWe’re discussing this dream in the \n\nPangeo Distributed Arrays Working Group, so if this inspires you then please join us!","type":"content","url":"/blog/2023/cubed-xarray#range-of-options","position":25},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Conclusion"},"type":"lvl2","url":"/blog/2023/cubed-xarray#conclusion","position":26},{"hierarchy":{"lvl1":"Cubed: Bounded-memory serverless array processing in xarray","lvl2":"Conclusion"},"content":"Xarray can now wrap multiple chunked array backends.\n\nCubed is a serverless prototype which you can try out today.\n\nWe hope this integration work leads to a range of new parallel backend options for users.\n\nPlease come and help out on the \n\nCubed repository, and join the \n\nDistributed Arrays Working Group!","type":"content","url":"/blog/2023/cubed-xarray#conclusion","position":27},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration"},"type":"lvl1","url":"/blog/2023/dask-distributed","position":0},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration"},"content":"By Tom Nicholas and Ryan Abernathey\n\n(Originally posted on the \n\nPangeo Medium blog.)","type":"content","url":"/blog/2023/dask-distributed","position":1},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl2":"Summary"},"type":"lvl2","url":"/blog/2023/dask-distributed#summary","position":2},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl2":"Summary"},"content":"Pangeo aimed to make big data analysis easy for scientists\n\nBut awkwardly dask.distributed would fail at large scales…\n\nIt’s now fixed, hooray for scientist-developer collaboration!","type":"content","url":"/blog/2023/dask-distributed#summary","position":3},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl2":"The Grand Vision"},"type":"lvl2","url":"/blog/2023/dask-distributed#the-grand-vision","position":4},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl2":"The Grand Vision"},"content":"The \n\nClimate Data Science Lab (funded by the Gordon and Betty Moore Foundation) was founded to create a positive feedback loop between practicing climate scientists and developers of open-source scientific software.\nMotivated by the explosion in the size of scientific datasets needing analysis, the idea was to improve the software tools available in the \n\nPangeo stack by creating a group with people who could both use the software to do scientific research and write the necessary code.\n\nFor some software packages, this approach of having scientist-developer contributors has worked pretty well.\nIn particular Xarray and Zarr have both benefited hugely from having a tight loop between users and developers, or even a core development team containing users who are practicing scientists.","type":"content","url":"/blog/2023/dask-distributed#the-grand-vision","position":5},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl2":"Dask as a linchpin"},"type":"lvl2","url":"/blog/2023/dask-distributed#dask-as-a-linchpin","position":6},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl2":"Dask as a linchpin"},"content":"Central to the \n\nPangeo vision was the ability for users to easily scale up analysis from data small enough to fit on a single machine to workloads requiring distributed systems, analyzing many terabytes or even petabytes of data.\nAs something of a scientist myself, this would be transformative — all we really want out of computers is to write down our analysis method clearly, and then simply apply it to all of our available data.\n\nEnter Dask, a parallel computing library which plugs seamlessly into Xarray and has a distributed scheduler capable of of computation across many nodes.\nDask was mission critical — as the Pangeo website says:\n\n“Dask is the key to the scalability of the Pangeo platform”.","type":"content","url":"/blog/2023/dask-distributed#dask-as-a-linchpin","position":7},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Perennial performance problems","lvl2":"Dask as a linchpin"},"type":"lvl3","url":"/blog/2023/dask-distributed#perennial-performance-problems","position":8},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Perennial performance problems","lvl2":"Dask as a linchpin"},"content":"Unfortunately, as the Pangeo community of computational scientists has pushed this tool stack to larger and larger scales, we have sometimes found that dask often did not perform the way we hoped.\nIn theory, we could prototype an analysis on small data, then hand it off to dask, whose distributed execution engine would apply it smoothly in parallel to much larger datasets.\nIf the analysis algorithm is straightforward to parallelize (e.g. applying the same function to every independent time slice of a dataset) then it should simply work, applying the same algorithm chunk-by-chunk in a streaming-like fashion.\nTo do bigger science you would just have to fork out more cloud computing credits.\n\nIn reality, what sometimes happens is that analysis workflows begin to stall at large scales.\nIn particular, the live dask task dashboard would show the dreaded orange bars, indicating that the memory of dask workers was becoming saturated, and they were being forced to spill data to disk.\nThe spilling would then massively slow down task execution on those workers, which often caused your computation to slow to an endless crawl, and in the worst case kill your workers.\n\nThis scalability issue became a \n\nwell-known problem in the pangeo community, to the point where we had common recommendations for coping strategies, employing such advanced solutions as \n\n“just get more RAM”, or \n\n“break the job up before feeding to the scheduler”.\n\nThe issue was of course \n\nreported upstream, but the discussion languished over multiple years, for two big reasons.\nFirstly, the problem with the distributed scheduler’s performance was very hard to diagnose, especially for the scientists encountering it, none of whom were dask core developers.\nFrustratingly, there seemed to be a gap between the theoretical performance of the scheduler on artificial test cases and the real-world performance on actual scientific workloads.\nInvoking the mantra of “Can you please simplify this to a minimum failing example?” didn’t help, as it was very hard for non-expert dask users to “dumb down” their full analysis problems.\n\nSecondly, \n\nearly attempts to fix the problem did not get at the root cause.\nThere were multiple scheduling inefficiencies known to the Dask devs which might be relevant, but after these were fixed the performance \n\nonly improved marginally in many of the Pangeo use cases.","type":"content","url":"/blog/2023/dask-distributed#perennial-performance-problems","position":9},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Attempting to analyse ocean data with xGCM and dask","lvl2":"Dask as a linchpin"},"type":"lvl3","url":"/blog/2023/dask-distributed#attempting-to-analyse-ocean-data-with-xgcm-and-dask","position":10},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Attempting to analyse ocean data with xGCM and dask","lvl2":"Dask as a linchpin"},"content":"I was one of the scientists frustrated by being unable to understand why Dask would work so beautifully on subsets of my dataset, but failed when I ran the same analysis at full scale.\nMy analysis problem was to use the \n\nxGCM package to apply vector calculus operations in a \n\ngrid-aware manner to a very large oceanographic simulation \n\ndataset called \n\nLLC4320.\nThe inability to analyse this particular Petabyte-scale dataset was actually one of the original \n\nmotivations that led to Pangeo, and my project was supposed to show that with the power of this full-stack solution we could now do oceanographic science at a scale not previously possible.\n\nDask encodes computations into a directed acyclic graph (DAG) of tasks to be executed by the scheduler, and in this respect our computation was not particularly unusual.\nWe needed to open a large number of independent chunks of data (the ocean state at different times), apply a function to each (e.g. take the divergence of the surface velocity of the ocean), then save each of these chunks back out.\nHowever it had some subtleties that would prove important.\nIn particular xGCM’s new \n\n“grid ufuncs” feature has an option to apply boundary conditions by padding one side of an array before applying the vector calculus operation.\nWhat this meant for dask is that the task graph was slightly more complex than a typical test case (e.g. compared to using \n\nxarray’s apply_ufunc), with small side chains that would get merged back in during the course of the computation.\n\nSuspecting that these side chains were confusing the scheduler somehow, we tried various suggestions to eliminate these peripheral tasks, trying to simplify the topology of the overall graph, such as \n\ntask fusion, \n\ninlining, and \n\nscattering.\nAlas none of these fixes banished the orange bars.\n\nA common feature of all the failing workloads appeared to be the loading in of more data than expected.\nEven for almost “embarrasingly-parallel” computation graphs, we could see that many \n\nmore data-opening tasks were being performed than data-analysing tasks, meaning that dask was over-eagerly loading in more data than it needed, overwhelming the workers.","type":"content","url":"/blog/2023/dask-distributed#attempting-to-analyse-ocean-data-with-xgcm-and-dask","position":11},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"A fix is found!","lvl2":"Dask as a linchpin"},"type":"lvl3","url":"/blog/2023/dask-distributed#a-fix-is-found","position":12},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"A fix is found!","lvl2":"Dask as a linchpin"},"content":"I \n\nreported this issue upstream, resurrecting a \n\nprevious discussion, along with a half-baked suggestion of how we might fix it in the scheduler.\nI included some of our task graphs, and \n\nGabe Joseph of Coiled suggested a quick way to try out my naive suggestion.\n\nA breakthrough came when, after iterating back and forth, we decided to take our efforts to the \n\nCoiled Slack, where Gabe could have me run test versions of dask.distributed on their \n\ncloud platform.\nWe realised that the xGCM analysis could reproduce the over-eager memory consumption problem even in small cases - now we had a minimum failing example!\nIt appeared that the side chains were enough of an added complexity to the task graph to make expose a race condition in the scheduling process, which allowed more data-loading tasks to run than necessary.\nThis explained how artifical test examples could perform smoothly whilst users still reported failures in real-world cases!\n\nNow armed with the understanding to reproduce the problem, as well as motivating cases for fixing it, Gabe went off to \n\nsolve it comprehensively.\nI can’t thank him and the other dask developers enough for the effort they put in here.\nThey made a \n\nset of test problems, including \n\none based on my xGCM case, and created an opt-in flag so that the \n\nPangeo users could try out their solution in the wild.\n(You can read an in-depth explanation of exactly what the problem with “root task overproduction” was and how it was solved in \n\nGabe’s post on the Coiled blog.)\n\nVarious community members were now able to test it, including pangeo software-developing scientists with a interest in pushing dask performance as far as we can, such as \n\nmyself, \n\nDeepak Cherian, and \n\nJulius Busecke.\nWe found that this improved task allocation algorithm worked so much better on essentially all of our varied types of geospatial workloads, and the opt-in flag has now become the default in dask distributed version 2022.11.0!","type":"content","url":"/blog/2023/dask-distributed#a-fix-is-found","position":13},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Impact","lvl2":"Dask as a linchpin"},"type":"lvl3","url":"/blog/2023/dask-distributed#impact","position":14},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Impact","lvl2":"Dask as a linchpin"},"content":"The difference in performance is massive for affected workloads - not only did it banish the dreaded orange bars by lowering the peak memory usage, but computations often finished faster too!\nThe benefits apply across a wide range of workloads, not just those that motivated solving the problem in the first place.\n\nI think it’s fair to say that this fix represents a “leap in productivity across the entire field” [of pangeo-using scientists], by speeding up or enabling computations that were either a pain to run before or flat-out impossible.\nThis fulfils one of the \n\noriginal stated aims of the Climate Data Science lab, substantiating the idea that collaborations between practicising scientists / research software engineers (Pangeo) and industry software developers (Coiled) can lead to big benefits for both.\nIn fact the benefits spill over into other fields of science too, as the Xarray + Dask combo is domain-agnostic.","type":"content","url":"/blog/2023/dask-distributed#impact","position":15},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Lessons","lvl2":"Dask as a linchpin"},"type":"lvl3","url":"/blog/2023/dask-distributed#lessons","position":16},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Lessons","lvl2":"Dask as a linchpin"},"content":"What can we learn from this?\n\nUsers found real problems\n\nFirstly, it emphasises the importance of listening to users.\n\n\nMatt Rocklin mentioned this recently, explaining why the dask team try to prioritise solving problems that are consistently reported by users, rather than jumping on easy-to-identify shiny new ways to speed up parts of the code that might not actually be a bottleneck.\n\nScientist-dev “road-testers” identified the core issue\n\nSecondly, scientist-developers like myself were able to identify problems that affected average scientist-users, but average users could not have fixed (or even identified).\nClearly it’s worthwhile to fund a few “Research Software Engineers” whose focus is improving software for scientists rather than only publishing scientific papers.\n(Another example of this in the Pangeo sphere is Deepak Cherian’s work on \n\nimproving groupby operations in dask.)\nThe key point here is that if the devs are also users, they are motivated to focus on hunting down the problems that all users face.\n\nMinimum reproducible example extremely important, but also hard in a full-stack context\n\nWithout a \n\nminimal, complete, reproducible example this problem did not get solved.\nHowever, it took a long time for such an example to emerge for a piece of software as complex as dask.\nThe problem was too subtle to be captured by a simple toy example, but real example workflows were generally too complex to clearly point to where the problem was coming from.\n\nI’m not entirely sure what the solution is here, but perhaps galleries of examples of intermediate complexity could be more easily adapted into failing problems than starting from either toy problems or real-world cases.\n\nIt’s also arguably more important that a failing example is reproducible than it is minimal, but that’s still challenging in a real scientific context.\nWe should be aiming to make all work on Pangeo cloud esily reproducible anyway, whether it’s polished final results or fail cases.\nThis means making it simpler to access data regardless of format (especially requester-pays buckets), on a common cloud compute platform, and full control over software environments to install experimental versions of packages.\n\nAutomated performance testing and full-stack tests crucial\n\nAnother aspect of the problem was users seeing poor performance but not having a rigorous way to report it. Every example was somewhat anecdotal, home-brewed, awkward to recreate, and not necessarily convincingly benchmarked.\nA set of automated full-stack performance tests (i.e. “integration testing”) might have flagged this problem and narrowed it down much earlier.\nThis idea has been \n\nproposed for the pangeo stack before, but not yet fully implemented.\nWe could also work with Coiled to get more Pangeo use cases into their automated performance tests.\n\nAcademia-industry collaboration around open source can be a very productive pattern\n\nThe Pangeo community has always strived to integrate people from academia, government agencies, and private industry.\nIn particular, we have always written into our grants consulting and services from open-source companies like \n\nAnaconda, \n\nB-Open, \n\nQuansight, and \n\nCoiled (just to name a few).\nThese arrangements have given our scientist-developers access to specialized software engineering expertise that is hard to hire for internally.\nConversely, the analysis problems that scientists face tend to really push the boundaries of what is possible, providing interesting and useful challenges to motivate software development.\nThis relationship worked well in this case, and we think it can be replicated more broadly.","type":"content","url":"/blog/2023/dask-distributed#lessons","position":17},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Next steps for this stack?","lvl2":"Dask as a linchpin"},"type":"lvl3","url":"/blog/2023/dask-distributed#next-steps-for-this-stack","position":18},{"hierarchy":{"lvl1":"Dask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration","lvl3":"Next steps for this stack?","lvl2":"Dask as a linchpin"},"content":"When working with big tabular data in the cloud, an analyst has a wide range of software choices, from data warehouses like Snowflake and BigQuery to open-source query engines like Spark, Presto, Trino, Dask Dataframe, DuckDB, etc.\nThe prevalance of SQL in this world means that users can pretty easily take the same code (SQL queries) and run it on different back-ends.\nThis friendly competition has driven a lot of innovation and performance improvements.\n\nFor big arrays, for a long time Dask’s Array module has been the only game in town. While we love Dask, we think it’s also great to see alternative array-oriented “query engines” coming online--it’s a sign that scientific computing in the cloud is maturing. In particular, we’re excited about:\n\nXarray Beam, a framework based around \n\nApache Beam,\n\nCubed, a serverless framework inspired by \n\nRechunker,\n\nRamba, which parallelizes numba-compiled functions using \n\nRay,\n\nArkouda, which wraps a parallel implementation in the domain-specific language \n\nChapel.\n\nHopfeully some of these new projects can explore new algorithms and architectures that can drive further improvements in Dask, resulting in faster time-to-science and more efficient use of computing resources.\n\nIf you’re interested in exploring the nascent area of distributed array computing in the cloud, we invite you to join the \n\nPangeo Working group on Distributed Arrays.\n\nAcknowledgements: Thanks for Gabe for useful comments on a draft of this post, as well as to Gabe, Florian Jetter, and the rest of the dask dev team for the fix!","type":"content","url":"/blog/2023/dask-distributed#next-steps-for-this-stack","position":19},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures"},"type":"lvl1","url":"/blog/2024/datatree","position":0},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures"},"content":"(Originally posted on the \n\nXarray blog.)","type":"content","url":"/blog/2024/datatree","position":1},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"tl;dr"},"type":"lvl2","url":"/blog/2024/datatree#tl-dr","position":2},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"tl;dr"},"content":"xarray.DataTree has been \n\nreleased in \n\nv2024.10.0, and the prototype \n\nxarray-contrib/datatree repository archived, after collaboration between the xarray team and the \n\nNASA ESDIS project. 🤝","type":"content","url":"/blog/2024/datatree#tl-dr","position":3},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Why trees?"},"type":"lvl2","url":"/blog/2024/datatree#why-trees","position":4},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Why trees?"},"content":"The DataTree concept allows for organizing heterogeneous collections of scientific data in the same way that a nested directory structure facilitates organizing large numbers of files on disk. It does so in a way that preserves common structure between data in the collections, such as aligned arrays and common coordinates.\n\nFor those familiar with netCDF4/Zarr groups, a DataTree can also be thought of as an in-memory representation of a file’s group structure. Xarray users have been \n\nasking for a way to handle multiple netCDF4 groups \n\nsince at least 2016!\n\nDataTree enables xarray to be used for various new use cases, including:\n\nClimate model intercomparisons,\n\nMulti-scale image pyramids, e.g. in \n\ngenomics,\n\nOrganising heterogenous data, such as satellite observations and model simulations.\n\nSimple and convenient access to entire hierarchical files.","type":"content","url":"/blog/2024/datatree#why-trees","position":5},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"What is a DataTree exactly?"},"type":"lvl2","url":"/blog/2024/datatree#what-is-a-datatree-exactly","position":6},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"What is a DataTree exactly?"},"content":"The new high-level container class xarray.DataTree acts like a tree of linked xarray.Dataset objects, with alignment enforced between arrays in parent and child nodes, but not between those in sibling nodes. It can be written to and opened from formats containing multiple groups, such as netCDF4 files and Zarr stores.\n\nFor more details please see the \n\nhigh-level description, the \n\ndedicated page on hierarchical data, and the \n\nsection on IO with groups in the xarray documentation.","type":"content","url":"/blog/2024/datatree#what-is-a-datatree-exactly","position":7},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Deprecation"},"type":"lvl2","url":"/blog/2024/datatree#deprecation","position":8},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Deprecation"},"content":"If you previously had used the datatree.DataTree prototype in the \n\nxarray-contrib/datatree repository, that has now been archived and will no longer be supported. Instead we encourage you to migrate to the implementation of DataTree that you can import from xarray, following the \n\nmigration guide.","type":"content","url":"/blog/2024/datatree#deprecation","position":9},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Big moves"},"type":"lvl2","url":"/blog/2024/datatree#big-moves","position":10},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Big moves"},"content":"This was a big feature addition! For a \n\ndecade there have been 3 core public xarray data structures, now there are 4: \n\nVariable, \n\nDataArray, \n\nDataset, and now \n\nDataTree.\n\nDatatree represents arguably one of the largest new features added to xarray in 10 years - the migration of the existing prototype alone added >10k lines of code across \n\n80 pull requests, and the resulting datatree implementation now contains contributions from at least 25 people.\n\nWe also had to resolve some really \n\ngnarly design questions to make it work in a way we were happy with.","type":"content","url":"/blog/2024/datatree#big-moves","position":11},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"How did this happen?"},"type":"lvl2","url":"/blog/2024/datatree#how-did-this-happen","position":12},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"How did this happen?"},"content":"DataTree didn’t get implemented overnight - it was a multi-year effort that took place in a number of steps, and there are some lessons to be learned from the story.\n\nIn March 2021, the xarray team submitted a \n\nfunding proposal to the \n\nChan-Zuckerberg Initiative to develop “TreeDataset”, citing bioscience use cases such as \n\nmicroscopy image pyramids. Unfortunately whilst we’ve been lucky to \n\nreceive CZI funding before, on this occasion we didn’t win money to work on the datatree idea.\n\nIn the absence of dedicated funding for datatree, Tom then used some time whilst at the \n\nClimate Data Science Lab at Columbia University to take a initial stab at the design in August 2021 - writing the first implementation on an overnight Amtrak! This simple prototype was released as a separate package in the \n\nxarray-contrib/datatree repository, and steadily gained a small community of intrepid users. It was driven partly by the use case of \n\nclimate model intercomparison datasets.\n\nA separate repository was chosen for speed of iteration, and to be able to more easily \n\nmake changes without worrying as much about \n\nbackwards compatibility as code in xarray’s main repo does. However the separate repo meant that the prototype datatree library was not fully integrated with xarray’s main codebase, limiting possible features and requiring fragile dependencies on private xarray internals.\n\nThe prototype then sat there for 2 years, until the NASA ESDIS team approached the xarray core team in August 2023. ESDIS devs wanted the ability to work with entire hierarchical files, and had experimented with the prototype version of datatree, but they wanted datatree functionality to be migrated upstream into xarray’s main repository so there would be more guarantees of long-term API stability and support.\n\nAmazingly NASA were able to offer the time of 3 engineers: Owen (NASA EOSDIS Evolution and Development 3 (EED-3) contract), Matt (NASA National Snow and Ice Data Center Distributed Active Archive Center (NSIDC)), and Eni (Goddard Earth Sciences Data and Information Services Center (GES DISC)). So starting in late 2023 the NASA trio worked on migrating the prototype datatree into xarray upstream, with regular supervision from Tom, Justus, and Stephan (existing xarray core team).\n\nThis second stage of development allowed us to reduce the bus factor on the datatree code, sanity check the original approach, and it gave us a chance to make some significant improvements to the design without backwards-compatibility concerns (for example enabling the \n\nnew “coordinate inheritance” feature).","type":"content","url":"/blog/2024/datatree#how-did-this-happen","position":13},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Lessons for future collaborations"},"type":"lvl2","url":"/blog/2024/datatree#lessons-for-future-collaborations","position":14},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Lessons for future collaborations"},"content":"This development story is different from the more typical scientific grant funding model - how did that work out for us?\n\nThe scientific grant model for funding software expects you to present a full idea in a proposal, wait 6-12 months to hopefully get funding for it, then implement the whole thing during the grant period. In contrast datatree evolved over a gradual process of moving from ideas to hacky prototype to robust implementation, with big time gaps for user feedback and experimentation. The migration was completed by developer-users who actually wanted the feature, rather than grant awardees working in service of a separate and maybe-only-theoretical userbase.\n\nOverall while the migration effort took longer than anticipated we think it worked out quite well!","type":"content","url":"/blog/2024/datatree#lessons-for-future-collaborations","position":15},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl3":"Pros","lvl2":"Lessons for future collaborations"},"type":"lvl3","url":"/blog/2024/datatree#pros","position":16},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl3":"Pros","lvl2":"Lessons for future collaborations"},"content":"Zero overhead - the existing xarray team did not to have to write a proposal to get developer time, and there was literally zero paperwork inflicted (on them at least).\n\nCertainty of funding - writing grant proposals is a lottery, so the time invested up front doesn’t even come with any certainty of funding. Collaborating with another org has a much higher chance of actually leading to more money being available for developer time.\n\nTime efficient - an xarray core dev spending 10% of their time advising someone who is less familiar with the codebase but has more time is an efficient use of relative expertise.\n\nBus factor - the new contributors reduced the bus factor on the datatree code dramatically.\n\nUser-driven Development - it makes sense to have actual interested user communities involved in development.\n\nStakeholder representation - after officially adding Owen, Matt and Eni to the \n\nxarray core team, the NASA ESDIS project has some direct representation in, insider understanding of, and stake in continuing to support the xarray project.","type":"content","url":"/blog/2024/datatree#pros","position":17},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl3":"Cons","lvl2":"Lessons for future collaborations"},"type":"lvl3","url":"/blog/2024/datatree#cons","position":18},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl3":"Cons","lvl2":"Lessons for future collaborations"},"content":"Not everyone got direct funding - It’s less ideal that Tom, Justus, and Stephan didn’t get direct funding for their supervisory work. In future it might be better to have one of the paid people at the contributing org already be a core xarray team member, or perhaps find some way to pay them as a consultant.\n\nTricky to accurately scope - The duration of required work was tricky to estimate in advance, and we didn’t want to “just ship it”. We hold the xarray project to high standards and backwards compatibility promises so we want to ensure that any publicly released features don’t compromise on quality.\n\nThis contributing model is more similar to how open-source software has historically been supported by industry, but perhaps because xarray is primarily developed and used by the scientific community we tend to default to more grant-based funding models.\n\nOverall we think this type of collaboration could work again in future! So if there is an xarray or xarray-adjacent feature your organisation would like to see, please reach out to us.","type":"content","url":"/blog/2024/datatree#cons","position":19},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Go try out DataTree"},"type":"lvl2","url":"/blog/2024/datatree#go-try-out-datatree","position":20},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Go try out DataTree"},"content":"Please try datatree out! The hierarchical structure is potentially useful to any xarray users who work with more than one dataset at a time. Simply do from xarray import DataTree or call \n\nopen_datatree(...) on a netCDF4 file / Zarr store containing multiple groups.\n\nBe aware that as xarray.DataTree is still new there will likely be some bugs lurking or places that performance could be improved, as well as as-yet \n\nunimplemented features (as there always are)!","type":"content","url":"/blog/2024/datatree#go-try-out-datatree","position":21},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Thanks"},"type":"lvl2","url":"/blog/2024/datatree#thanks","position":22},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Thanks"},"content":"A number of other people also \n\ncontributed to datatree in various ways - particular shoutout to \n\nAlfonso Ladino and \n\nEtienne Schalk for their dedicated attendance at many of the \n\nweekly migration meetings!","type":"content","url":"/blog/2024/datatree#thanks","position":23},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Funding Acknowledgements"},"type":"lvl2","url":"/blog/2024/datatree#funding-acknowledgements","position":24},{"hierarchy":{"lvl1":"Xarray x NASA: xarray.DataTree for hierarchical data structures","lvl2":"Funding Acknowledgements"},"content":"Owen, Eni, and Matt were able to contribute development time as part of the \n\nNASA ESDIS project.\n\nTom was supported first by the Gordon and Betty Moore foundation as part of Ryan Abernathey’s \n\nClimate Data Science Lab at Columbia University, then later by various funders for a fraction of his time through \n\n[C]Worthy.","type":"content","url":"/blog/2024/datatree#funding-acknowledgements","position":25},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?"},"type":"lvl1","url":"/blog/2025/cloud-optimized-scientific-data","position":0},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?"},"content":"Why naively lifting scientific data to the cloud falls flat.\n\n(Originally posted on the \n\nEarthmover blog)","type":"content","url":"/blog/2025/cloud-optimized-scientific-data","position":1},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Scientific formats predate the cloud"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#scientific-formats-predate-the-cloud","position":2},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Scientific formats predate the cloud"},"content":"There are exabytes of scientific data out in the wild, with more being generated every year. At Earthmover we believe the best place for it to reside is in the cloud, in object storage. Cloud platforms are the state-of-the-art for managing tabular business data, and scientific data teams deserve similarly powerful tools. The \n\nPangeo project has shown how the cloud can work for \n\narray data too, enabling efficient storage of enormous scientific datasets, scalable data-proximate compute, and \n\nsharing public data far and wide.\n\nUnfortunately, most scientific array file formats (such as NetCDF, HDF5, TIFF, and GRIB) predate the cloud, so were never designed with access from cloud object storage in mind. We will demonstrate why naively moving netCDF files to the cloud results in deathly slow performance, and show you how Zarr, and other “cloud-optimized” data formats, solve this problem.\n\nThis post is for you if you’re responsible for managing an archive of scientific data and are curious about what a cloud migration would look like. It’s also for you if you’re just interested in learning more about how these formats work! If you’re not a developer or data engineer, you should still be able to follow along.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#scientific-formats-predate-the-cloud","position":3},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Object storage is not a filesystem"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#object-storage-is-not-a-filesystem","position":4},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Object storage is not a filesystem"},"content":"Cloud object storage (such as Amazon S3) and filesystems (like the one on your laptop) are both data storage systems, but are crucially different. We need to understand those differences to see why cloud-optimizing data is important.\n\nFiles are containers for data (each a series of bytes) that’s physically on your computer and managed by your Operating System. They are close at hand and fast to scan through. Imagine cooking in your own kitchen - you need to open the fridge to access ingredients (opening a file), it’s quick to look through what you have (fast to scan through the file), you need to put everything away and close the fridge door when you’re done (closing the file), and if someone else is blocking the fridge you have to wait until they are done (files are locked whilst another process edits them). This ingredient storage system is optimized for immediate use by one person at a time, to quickly examine a small number of close-at-hand items.\n\nObject storage is similar in that each object is also a container for a series of bytes, but the way you access that data is different. There is no concept of “opening files”. Instead, all you can do is say “I would like to get this part of that object, please”, or “I would like to put some bytes into this location, please”. You interact with object storage only through these much simpler operations, such as GET, PUT, LIST, or DELETE, all sent over the network as HTTP requests.\n\nReading data from object storage is more like ordering ingredients directly from Costco. Instead of accessing ingredients directly, you submit specific orders (the GET requests). Each individual item takes time to deliver (called “latency”), but you can place many orders simultaneously, and the same distributor can fulfill orders for many customers at once. However, for this to work you need to know what is available and what you want to order in advance. This system is designed for scale rather than individual efficiency.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#object-storage-is-not-a-filesystem","position":5},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Lousy latency"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#lousy-latency","position":6},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Lousy latency"},"content":"The most important difference for this post is the “latency”, the delay between asking for a piece of data and receiving the first few bytes of that data. (Or between placing your Costco order and receiving it).\n\nLatency with object storage is much higher than for a local filesystem. When you read a piece of data from part of a file sitting on your laptop, moving the pointer to the correct location in the file to begin reading the first few bytes into RAM takes ~1 millisecond. But to get a piece of data from object storage using a GET request takes a \n\nfew 10’s of milliseconds from the moment you send the request over the network to the moment you receive the first byte back. Spoiler: this will be a problem.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#lousy-latency","position":7},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Describe yourself in 8 bytes"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#describe-yourself-in-8-bytes","position":8},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Describe yourself in 8 bytes"},"content":"Formats like netCDF are amazing because they are self-describing. The information required to understand the data in the file (known as the metadata) is present in the file itself, so you can simply read that one netCDF file to learn that it contains, for example, two arrays named “temperature” and “precipitation”, each of which is arranged as a 100x100x10 grid of 32-bit float values. Without this metadata you would either have had to know a priori what was in there, or just guess what it meant. Self-describing data formats were a major step forward for reproducible science and sanity in general.\n\nBut formats like netCDF/HDF5 are commonly sub-optimal for object storage because the descriptions of the contents are spread throughout the file. The tiny part that tells you that the first array is called “temperature” and the other tiny part that tells you that the second array is called “precipitation” are not adjacent; they are separated from one another by all of the thousands of actual temperature numeric data values. That means to find out a few kBs of information (all the metadata), you may have to comb through the entire file, which could be MBs or even GBs in size. Imagine if you had to walk through the entire Costco store just to find out what departments they have.\n\n\n\nA schematic of the layout of metadata within non-cloud-optimized HDF5/netCDF4 files. The metadata is scattered throughout the whole file, separated by large distances by the raw data chunks. From \n\nBarrett et. al. 2024.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#describe-yourself-in-8-bytes","position":9},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Death by a thousand GETs"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#death-by-a-thousand-gets","position":10},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Death by a thousand GETs"},"content":"Presented with a URL pointing to a mystery file sitting in object storage, you must begin by issuing one GET request for the first few bytes (called the \n\n“magic” bytes), and wait for that to come back to learn that this is, in fact, a netCDF file. Then you can issue another GET request for some other set of bytes to learn that this file contains an array called “temperature” of size 1GB. Only once that comes back do you know how many bytes to jump forward for another GET request to learn that the next array is called “precipitation”.\n\nThis is not a problem for a local filesystem. Your pointer starts at the start of the file, and using the filesystem’s seek operation to move it to a new location is very speedy (~0.1 milliseconds on an SSD), so scanning through the entire file by moving the pointer forwards lots of times in a row is no big deal.\n\nBut to read a lone netCDF file in object storage you have to issue many GET requests, one after another, each of which adds another round-trip’s worth of latency. As we learned above, latency for object storage is very high, so this rapidly adds up.\n\nLet’s demonstrate. \n\nThis demo shows what happens when you naively try to open a 1GB (3GB uncompressed) netCDF file in a public S3 bucket using Xarray, and print all the metadata you might care about (e.g. array names and sizes), by reading it as if it were a local file. Each filesystem read call is translated into one HTTP GET request, with no special treatment like caching or pre-fetching.\n\nAs expected, the performance is atrocious - it takes the better part of a minute just to read the metadata, making 502 separate GET requests just to download a grand total of 267KB. That’s an effective bandwidth of only 7 KB/s, far slower than even 1990’s-era dial-up internet.\n\n(Note there are many ways this could be optimized, such as by caching, pre-fetching, or \n\nrepacking metadata. Even using the netCDF4-python library out-of-the-box would perform somewhat better. But the point is that if you do not or cannot alter the files, and you naively treat object storage as if it’s a local filesystem, you will get abysmal performance.)\n\nYou could instead ask to download the entire file up front, which means downloading all the data when you really only wanted to see the metadata to know what’s inside. But then you are downloading millions or billions of times more data than you know you will need.\n\n\n\nZarr cartoon about how having to download all the data before looking at any of it is an antiquated paradigm.\n\nEven worse - this is all for one file. Most datasets of interest consist of thousands of netCDF files, making these problems a thousand times worse! It can potentially take hours just to open such a dataset on the cloud. This sucks, and is ultimately all a consequence of the data not being “cloud-optimized”.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#death-by-a-thousand-gets","position":11},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"A series of fat pipes"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#a-series-of-fat-pipes","position":12},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"A series of fat pipes"},"content":"At this point you are probably wondering, “if the latency of the cloud is so high then what is so good about it?” The answer is that what object storage lacks in latency it more than makes up for in throughput.\n\nIf the internet is famously \n\n“a series of tubes”, then the cloud is a series of fat pipes. Inside a data center, the object storage is connected to the compute resources via connections of enormous bandwidth. Whilst it might take a moment between you asking for the tap to be turned on and water starting to flow, once it begins you will quickly be drenched by a massive firehose of data coming through that fat pipe.\n\nThe total amount of data transferred divided by the total time it took to transfer it is called throughput. The throughput of object storage can be through the roof - the \n\nPyWren paper demonstrated linear scaling to aggregate read throughputs of 80GB/s on S3, with no signs of slowing at even larger scales. This kind of performance is comparable to that of the expensive filesystems of \n\ntop supercomputers, but on far cheaper commodity hardware instead of specialized HPC hardware, and available to anyone.\n\nHow do we achieve this massive throughput? The problem we saw with the metadata is not that we’re doing a lot of GET operations – the cloud is actually great at that – it’s rather that we’re doing all those GETs sequentially, one after the other. If we only needed to issue one GET request we would only have to wait once, and if we were able to somehow immediately issue all the GET requests we were ever going to need, the power of object storage is that it can send all the results back to us simultaneously, thereby taking full advantage of the fat pipe. (In the Costco analogy, you could submit an order for a literal truckload of apples.)","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#a-series-of-fat-pipes","position":13},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Idea: Separate the metadata"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#idea-separate-the-metadata","position":14},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Idea: Separate the metadata"},"content":"So to benefit from object storage, traditional netCDF isn’t going to work - we have to change something. The first problem to solve is needing to do all those sequential GETs just to retrieve a very small amount of metadata.\n\nIf that metadata were instead separated out into a known location (for example, a separate accompanying “sidecar” file), we could simply issue one GET request for the entire contents of that small metadata file. Then we would know everything about the structure of the rest of the data after just one round trip’s worth of latency, which is the fastest we could possibly have learnt about anything in object storage. (Costco now has a catalog.)\n\nSeparating the metadata from the data is also what allows scalability - more data no longer implies searching for longer to find the metadata, as it’s always separated into a known location.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#idea-separate-the-metadata","position":15},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Idea: Split the data into chunks"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#idea-split-the-data-into-chunks","position":16},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Idea: Split the data into chunks"},"content":"Once we know what’s in the dataset, the other problem is how to get only the parts of the data we actually want, while taking advantage of the massive throughput object storage offers.\n\nIf we split each data array into many small chunks, then we can download only the subset of data we actually want to use. For example, all chunks that make up the most recent satellite image of a certain location. If each chunk is a separate file, then we can easily request them independently and in parallel, potentially all at once through our fat pipe.\n\n(If you’re really paying attention you might ask: “If small is good why don’t we just make the chunks as small as possible?” One reason is that we also want to compress each chunk to save storage space. There is a trade-off between larger chunks, which compress more efficiently, and smaller chunks, which allow you to make more targeted requests.)\n\nNetCDF and HDF5 \n\nalready use chunking internally within their file formats. However, the information about the chunks (another form of metadata) is relatively slow to access on object storage, for the reasons described above, meaning that it isn’t trivial to just grab the chunks we want. You also need to split the chunks out if you want to partially update an existing dataset in the cloud, because object storage doesn’t allow you to edit files, only replace them in their entirety.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#idea-split-the-data-into-chunks","position":17},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Congratulations! That’s Zarr."},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#congratulations-thats-zarr","position":18},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Congratulations! That’s Zarr."},"content":"So let’s invent a new file format designed to work beautifully with object storage, with separate files for the metadata and splitting the data into chunks. Oh wait - that already exists: it’s called Zarr.\n\nThis isn’t facetious - Zarr really is faster because of these two main reasons. In fact, you can clearly see it in this diagram of the internal structure of an example Zarr store.\n\n\n\nDiagram of the structure of a Zarr v3 store, showing how the metadata is separated into dedicated files, and the data is split into chunks.\n\nAll the metadata for a particular array are in one zarr.json file, which can be retrieved with a single GET request. The actual data arrays are split into grids of chunk files, any set of which can be retrieved simultaneously using one GET request per chunk.\n\nBack in 2018, there were \n\nvarious ideas for overcoming cloud latency, but it’s cloud-optimized formats such as Zarr, \n\nCloud-Optimized GeoTIFF (a.k.a. COG), and even \n\ncloud-optimized HDF5 that have taken off within climate, weather, microscopy, and other scientific data communities.\n\nNote that none of these ideas were invented by Zarr. Commercial cloud object storage is almost 20 years old(!) and cloud-optimized tabular formats such as Avro and Parquet were developed to take full advantage soon afterwards. (Parquet was a direct inspiration for Zarr.) To understand why we can’t just use tabular formats for scientific data read our recent \n\nfundamentals post on tensors vs tables.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#congratulations-thats-zarr","position":19},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Consolidating ~~power~~ metadata"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#consolidating-power-metadata","position":20},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Consolidating ~~power~~ metadata"},"content":"However, this still isn’t perfect. Zarr’s native format on object storage stores metadata for each array in a dedicated file, i.e. it has one zarr.json file to go get per array. Some datasets have hundreds of different arrays, so it would be considerably more efficient still if all the metadata for all the arrays were somehow in one json file.\n\nThat’s what Zarr’s “consolidated metadata” feature is: one weird trick to minimize the number of GET requests, by consolidating all the metadata into one zarr.json file at the root of the Zarr store. Consolidated metadata is arguably a bit of a hack, but because Xarray actually writes this by default, many existing Zarr stores have it already.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#consolidating-power-metadata","position":21},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Icechunk: Consolidated metadata + version control"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#icechunk-consolidated-metadata-version-control","position":22},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Icechunk: Consolidated metadata + version control"},"content":"Earthmover’s open-source format \n\nIcechunk takes consolidated metadata and runs with it. Like a conventional Zarr store, Icechunk also stores all the metadata and chunks as separate objects, but Icechunk also has a system for tracking version history of all those objects, providing git-like version-control features for your data. You can think of it as tracking successive versions of consolidated metadata, each of which describes the state of your entire Zarr store at one point in its history. For more details, read the \n\nIcechunk launch blog post, or watch the \n\nwebinar on version control with icechunk.\n\nAs the metadata is always consolidated, Icechunk also minimizes the number of GET requests required to open the dataset. We ran a simple benchmark to confirm this.\n\n\n\nBenchmarking results for calling xarray.open_dataset on a single file (1GB compressed, 3GB uncompressed) of \n\nNOAA GFS data, stored in different formats. Code was run from a laptop in New York, so the time reflects latency to reach the data in AWS us-east-1. Code to reproduce the results is \n\navailable on Github.\n\nOur benchmark compares the number of HTTP requests issued, and the overall time taken to call xarray.open_dataset on 1GB’s worth of array data stored in S3. It includes the netCDF demo from earlier, compared against the same data stored as native Zarr, Zarr with consolidated metadata, and as icechunk.\n\nWhile Zarr with consolidated metadata minimizes the number of HTTP GET requests required overall, Icechunk has similar speed advantages, and all the power of transactions and version control!\n\n(For the more technical amongst you see the footnotes about this benchmark at the end of the article.)","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#icechunk-consolidated-metadata-version-control","position":23},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"High throughput means low cost"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#high-throughput-means-low-cost","position":24},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"High throughput means low cost"},"content":"That’s latency. For throughput, thanks to optimizations in \n\nZarr-Python 3 and \n\nIcechunk, our open-source software stack is now approaching the \n\ntheoretical limits of maximum possible throughput. So when streaming data from object storage to cloud compute within the same datacentre, you can use the full bandwidth of those fat pipes.\n\nThis translates to big cost savings. Multi-dimensional data analysis workloads are often IO-bound, meaning that the CPUs you’re paying for spend most of their time waiting for data to come in, not actually computing on it. So the \n\n100x speedup we see for NASA data using Zarr and Icechunk compared to using non-cloud-optimized netCDF translates to 100x less time you’re paying for CPUs to sit around, and so 100x lower cloud computing bills!","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#high-throughput-means-low-cost","position":25},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"No free lunch"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#no-free-lunch","position":26},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"No free lunch"},"content":"Whilst using cloud-optimized data in the cloud is generally a no-brainer, there are some downsides to the cloud approach.\n\nObject storage always has high time-to-first-byte. If reacting to new data as fast as physically possible really matters to you (e.g., because you’re doing high-frequency trading), then you should not use object storage at all. High-frequency trading is an example of a \n\n“transactional” workload because the objective is to make a decision and write a small amount of information back as fast as possible (to make the trade). Object storage is better suited for “analytical” workloads, in which a larger amount of data is ingested for important but not subsecond-urgent querying. These analytical workloads are plenty common within scientific and geospatial domains.\n\nUsing a new file format generally requires copying the original data. That requires both more storage space to hold the copy as well as compute to perform the copying. But what if we didn’t have to copy all the data? Look out for a future blog post about “Virtual Zarr”...","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#no-free-lunch","position":27},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Towards the great scientific supercomputer in the sky"},"type":"lvl2","url":"/blog/2025/cloud-optimized-scientific-data#towards-the-great-scientific-supercomputer-in-the-sky","position":28},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl2":"Towards the great scientific supercomputer in the sky"},"content":"Hopefully now next time you hear “cloud-optimized data” you will understand that it’s really just about separating the metadata from the chunks, and fetching things efficiently.\n\nBut our vision for working with scientific data in the cloud requires more than just cloud-optimizing files and dumping them into cloud buckets. Ideally we want to forget about files entirely and just live in a world of seamless cloud data lakes. But that will have to wait until next time!","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#towards-the-great-scientific-supercomputer-in-the-sky","position":29},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl3":"Footnotes:","lvl2":"Towards the great scientific supercomputer in the sky"},"type":"lvl3","url":"/blog/2025/cloud-optimized-scientific-data#footnotes","position":30},{"hierarchy":{"lvl1":"Fundamentals: What is Cloud-Optimized Scientific Data?","lvl3":"Footnotes:","lvl2":"Towards the great scientific supercomputer in the sky"},"content":"When \n\nRyan and Joe first integrated Zarr with Xarray in 2017, it wasn’t even possible to open netCDF in the cloud directly without downloading the entire file.\n\nRecent improvements to opening netCDF directly, such as \n\n“cloud-optimized HDF5”, are using these same ideas, and so are formats such as COG and Parquet.\n\nZarr’s performance on this chart used to be a lot worse - big \n\nimprovements were made to the Zarr-Python library by the open-source community.\n\nThe reason the number of requests and total time don’t just scale in lockstep is because if you issue a lot of requests all at once it only takes one round-trip’s worth of latency to get them all back. That’s basically what Zarr without consolidated metadata is doing.\n\nFor a larger subset of GFS containing more files, netCDF would take even longer as the metadata of every file would need to be read, while Zarr and Icechunk would require approximately the same amount of time. So Zarr & Icechunk are the only options here that can scale to accessing truly massive datasets.","type":"content","url":"/blog/2025/cloud-optimized-scientific-data#footnotes","position":31},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data"},"type":"lvl1","url":"/blog/2025/science-needs-a-social-network","position":0},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data"},"content":"","type":"content","url":"/blog/2025/science-needs-a-social-network","position":1},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl2":"Science needs a social network for sharing big data"},"type":"lvl2","url":"/blog/2025/science-needs-a-social-network#science-needs-a-social-network-for-sharing-big-data","position":2},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl2":"Science needs a social network for sharing big data"},"content":"Tom Nicholas, 18th Jan 2025\n\nContext: This post was originally published just as a \n\nHackMD doc.\nOn the back of that post I gave to give a talk & hosted a discussion about this at the \n\nPangeo Showcase (\n\nrecording here). If you are interested in helping build the thing please \n\nfill out this form!\n\ntl;dr: Science is missing a crucial data sharing network. We have the technology - here’s what we should build.","type":"content","url":"/blog/2025/science-needs-a-social-network#science-needs-a-social-network-for-sharing-big-data","position":3},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Data sharing sucks","lvl2":"Science needs a social network for sharing big data"},"type":"lvl3","url":"/blog/2025/science-needs-a-social-network#data-sharing-sucks","position":4},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Data sharing sucks","lvl2":"Science needs a social network for sharing big data"},"content":"Sharing large scientific datasets is a pain in the ass, and finding them is even worse. There are myriad portals and tools and APIs and bucket URLs and crappy catalogs that are split across arbitrary geographic and institutional lines, which are often slow or have gone down, or will only give you part of the data or in a suboptimal pattern or in a format you’ve never heard of. Crucial datasets aren’t easy to find by Googling, and because you don’t know where they live (or even if what you want exists anywhere) you don’t know which institutions’ catalog to comb through. That’s if anyone even bothered to put the data online, rather than just fobbing you off with “available upon request” and a gesture towards an out-of-date email address.\n\nImagine instead simply being able to visit one website, search for any scientific dataset from any institution in the world, preview it, then stream the data out at high speed, in the format you prefer. A true “scientific data commons”.","type":"content","url":"/blog/2025/science-needs-a-social-network#data-sharing-sucks","position":5},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Scientists love GitHub","lvl2":"Science needs a social network for sharing big data"},"type":"lvl3","url":"/blog/2025/science-needs-a-social-network#scientists-love-github","position":6},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Scientists love GitHub","lvl2":"Science needs a social network for sharing big data"},"content":"For sharing code, this commons already exists: it’s called \n\nGitHub. “Just put it on GitHub” has become the “Open Science” community’s mantra. As well as being the de facto way to share code underlying individual scientific papers, GitHub has enabled open collaboration on impactful scientific software projects (including true inter-disciplinary collaboration between otherwise unrelated fields). Despite being privately-owned, we regularly champion its use under the \"Open science” banner, as it is an enormous boon for open science worldwide.","type":"content","url":"/blog/2025/science-needs-a-social-network#scientists-love-github","position":7},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"GitHub is a centralized social network","lvl2":"Science needs a social network for sharing big data"},"type":"lvl3","url":"/blog/2025/science-needs-a-social-network#github-is-a-centralized-social-network","position":8},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"GitHub is a centralized social network","lvl2":"Science needs a social network for sharing big data"},"content":"GitHub is a version-controlled code hosting site with easy local backups, but it’s also arguably a social network - upon launch in 2008 their strapline was \n\n“Social Code Hosting”. You can like, comment, subscribe, and follow users, repositories, and organisations.\n\nNote that whilst the source code they host is distributed (via backing up by everyone that clones a repository) their social network is completely centralized. All the issues, comments, followers etc. live in GitHub’s servers, are not automatically backed up by any other organisation, and are only acessible through the GitHub web platform or official API.","type":"content","url":"/blog/2025/science-needs-a-social-network#github-is-a-centralized-social-network","position":9},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"GitHub for data?","lvl2":"Science needs a social network for sharing big data"},"type":"lvl3","url":"/blog/2025/science-needs-a-social-network#github-for-data","position":10},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"GitHub for data?","lvl2":"Science needs a social network for sharing big data"},"content":"Most modern scientific works are really composed of up to 3 artifacts: the manuscript, data, and code. Science is currently often done by disseminating the manuscript alone via a journal, but efficient sharing of all three is necessary to have any hope of reproducibility of scientific results.\n\nManuscript sharing networks exist but are hamstrung by publicly-funded content being brutally paywalled by parasitic for-profit organisations (otherwise known as “Scientific Publishers”). But that shitshow deserves discussion in a separate post.\n\nGitHub is designed specifically for sharing code - using it to share data is really misusing the platform. The fact that people do still share (small) data this way regularly shows there is an unfullfilled need.\n\nWhat we still need is a global social network for sharing big scientific datasets, or a “GitHub for data”.","type":"content","url":"/blog/2025/science-needs-a-social-network#github-for-data","position":11},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Goal of “FAIR” scientific data","lvl2":"Science needs a social network for sharing big data"},"type":"lvl3","url":"/blog/2025/science-needs-a-social-network#goal-of-fair-scientific-data","position":12},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Goal of “FAIR” scientific data","lvl2":"Science needs a social network for sharing big data"},"content":"To imagine something new, let’s keep in mind the oft-cited (but rarely achieved) \n\n“FAIR data principles”. These state that scientific data assets should be Findable, Accessible, Interoperable, and Reproducible.","type":"content","url":"/blog/2025/science-needs-a-social-network#goal-of-fair-scientific-data","position":13},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"What would the ideal data-sharing service look like?","lvl2":"Science needs a social network for sharing big data"},"type":"lvl3","url":"/blog/2025/science-needs-a-social-network#what-would-the-ideal-data-sharing-service-look-like","position":14},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"What would the ideal data-sharing service look like?","lvl2":"Science needs a social network for sharing big data"},"content":"The ideal data-sharing service would have a number of important properties:\n\nUnified\n\nA catalog of all scientific data in the world should have global search and connections across all projects, regardless of instutition or academic subfield.\n\nThis feature of GitHub is what allows true interdisciplinary collaboration within \n\nfoundational software projects. Any other approach causes fragmentation and artifical boundaries between hazily-defined fields of science, and wastes effort through re-inventing unnecessarily bespoke solutions for generic data engineering problems.\n\nOpen to anyone\n\nAnyone should be able to create a public catalog entry for their dataset on the platform.\n\nThis doesn’t mean it necessarily has to be free to upload data to the catalog, but it should be possible for anyone who can pay a reasonable hosting fee to do so (like the commercial cloud, dropbox, or website hosting). This means that no institutional login should be required, no requirement for a .edu email address, no requirement to be in a specific country, etc.\n\nFree to browse\n\nAnyone should be able to use the platform to browse and discover public datasets, without them being paywalled.\n\nDownloading the actual data doesn’t need to be free (and shouldn’t be - bandwidth for big data is expensive so downloading it unnecessarily should be disincentivised), but viewing enough metadata to decide if you want to get the actual contents should be free. This is especially important for crucial public data such as real-time satellite imagery of wildfires - which infuriatingly is not necessarily findable \n\neven by experts.\n\nScalable to massive datasets\n\nThe platform should be able to handle the biggest datasets in science.\n\nIn many fields the largest datasets are some of the most important. That’s especially true in physical sciences such as climate science, meteorology, neuroscience, and genomics. For example \n\nERA5 is effectively our best guess as to the Earth’s weather over the last 80 years, and as such it’s the starting point for a huge amount of climate / meteorology / hazard & energy forecasting research. Unfortunately it’s about 5 Peta-Bytes in size, so could only be hosted by a system that can handle arbitrarily large datasets.\n\nAs the \n\nPangeo project in big data geoscience has shown, this can’t be an afterthought, because sharing big data requires fundamentally different architectures (i.e. \n\ncloud-native data repositories). Luckily placing data in S3-compatible public-facing cloud storage also solves the problem of ensuring open accessibility and scalable computing resources for data analysis. Agencies like NASA are already catching on to this whole cloud idea, but not to the full potential that this infrastructure technology enables.\n\nGlobally available\n\nThe same data should be available to anyone in the world.\n\nCloud storage allows large datasets to be available to anyone within a \n\ncloud region, but the technical challenge is guarantee that copies of data stored in different cloud regions are consistent with one another. One approach is to do this at the level of the storage itself (e.g. \n\nTigris), but another is to have automatic methods of \n\nguaranteeing consistency between copies.\n\nCommon data models\n\nThe vast majority of scientific data can be represented via one of 2 or 3 common data models. Particularly Tabular data (rows and columns, think spreadsheets), and Multi-dimensional Array (or “Tensor”) data (grids of values, think latitude-longitude). Many fields of physical science are naturally described via Multi-dimensional Arrays, because the universe we live in is 4-dimensional. For example climate/weather simulations, microscope imagery, but also more abstract dimensions such as DNA base pairs in genomics.\n\nRecognising this (still extremely general) structure allows for plugging in to all the existing tools in the world designed to process, analyse, and visualize data. For example, adopting common tabular data formats allows the use of off-the-shelf \n\ndata processing technologies, so is really required to achieve the FAIR principle of \n\nInteroperability. This idea is so useful that platforms for managing Tabular data specifically for e-commerce businesses are themselves worth \n\nbillions of dollars.\n\nIn the world of Tabular data \n\ncommon models are well-established, and in the world of array data the \n\nZarr model is general enough to abstract over many existing scientific array data formats, often \n\nwithout even altering the original data format.\n\nVersion-controlled\n\nThe key value-add of git is the way it tracks every change to your source code. A robust version control system is crucial to have any hope of reproducibility, and that’s true also for data. Luckily open-source systems such as \n\nApache Iceberg (Tabular) and \n\nEarthmover’s recent release of \n\nIcechunk (Arrays) both allow users to rollback to old versions of data to reproduce results.\n\nWithout this crucial building block previous attempts to create an updateable version-controlled catalog have often become mired in all the complexity needed to make up for the lack of intrinsically version-controlled data models.\n\nData ownership\n\nUsing the platform should not require you to relinquish legal or technical control over your data. (This is how the the scientific publishing industry really screwed us all over.)\n\nThis requires openly-licensed data formats (which Iceberg and Icechunk both are), as well as the option for data to reside on your own infrastructure. The platform then merely catalogs references to data that actually still lives on other systems. The cloud works well for this - data can live in one bucket but be referenced from anywhere, and data consumers don’t necessarily have to know the difference.\n\nCitable dependency networks\n\nDatasets in the catalog should be citable, and it should be possible to see which datasets are derived from which others.\n\nThe \n\nDOI system makes this technically pretty straightforward, but it’s sociologically crucial for scientists to be able gain career credit for non-manuscript contributions (such as widely-used datasets and open-source code). The fact this kind of credit currently is both difficult to obtain and undervalued by tenure committees is a major source of misaligned career incentives in science.\n\nSubscribable\n\nMany scientific datasets are not just derived from upstream data, they should actually be updated every time the upstream data is updated.\n\nA \n\nPublisher-Subscriber model (think RSS blog feeds) would allow recomputing derived data upon updates to upstream data. For example, imagine recomputing a metric of wildfire risk whenever new satellite imagery becomes available.\n\nSocial\n\nScience progresses through debate, and so the data catalog should include spaces for debate.\n\nThis could take the form of public comments, or GitHub-like issue raising. Version-control also theoretically allows public submission of data corrections (i.e. Pull Requests that update data). Whilst this brings up the spectre of Moderation, at least moderation here would be federated in that each data project’s owners would have the power and incentive to moderate their own space. This approach scales with the number of datasets, and seems to work relatively well for GitHub.\n\nScientists also have professional social networks, and users being able to follow specific scientists, datasets, or data providers would simply formalize and streamline data-sharing networks which already exist informally.\n\nSustainably-fundable\n\nSomething of this scope would cost someone \n\nmillions of dollars every year to build and maintain. Ideally the project would generate this funding itself.\n\nThere’s no way around that figure - it requires hosting infrastructure (servers) and staff (expensive software engineers) - decentralizing the project would just split the same cost among more institutions. If you want it to actually work anywhere near as well as GitHub it would cost even more than that, to pay for expensive industry-grade UX designers and site reliability engineers and so on.\n\nThe funding mechanism also needs to scale up proportionally to the size of the user base - and it’s important that the project is actually incentivised to provide a good service. GitHub handles these problems by being a private company offering a useful service to businesses, with a free tier for public repositories but a paid tier for private ones.\n\nRight to Exit\n\nIt’s important that a global network of scientific knowledge is not beholden to any one institution or company. However it would be hard to build the unified search and dependency/social network features, use common data models, and get economies of scale without some degree of centralization.\n\nThis is a similar problem to social media: users want to be on the same network, accessed via a platform with a high-quality interface. But they also want to be able to leave without losing their personal/professional network if there is an \n\n“Elon Musk Event” threatening trust in the platform. For social networks this \n\n“Right of Exit” is harder to achieve than simply having multiple platform vendors or open data formats, because an alternative service doesn’t automatically come with \n\nyour follower network on it.\n\nOne idea for avoiding being locked-in to a social network is decentralization. Mastodon and Bluesky both claim to be decentralized in a way that safeguards your Right to Exit. Their solution is to separate the network from the platform used to interact with the network, and make the network layer an open protocol. Think of email: while no-one owns the “email network”, there are many platforms you can use to send and receive emails (Outlook, GMail, Thunderbird etc.), and you can switch between them. This is possible because there is a public email protocol which every email provider uses to send messages between them.\n\nDecentralizing an entire social network turns out to be \n\nincredibly challenging. A more tractable idea that could still help alleviate lock-in would be a decentralized catalog protocol. That would at least allow multiple institutions / other companies to host their own data catalogs, and have them integrate fully, with shared global search and dependency networks. To be more concrete, imagine NASA hosts a catalog of Icechunk repositories on the platform IceHub, whilst NOAA hosts some others on a different platform IceBucket. A shared decentralized network using a public protocol would allow the NASA data repositories to subscribe to updates from the NOAA datasets, with bi-directional linking, despite being on different platforms. It would also allow either platform to easily provide a search index that covers all public datasets on both platforms. One naive way to do this is would be to simply have the full public network be stored on both platform’s servers (somewhat like \n\nNNTP), which also means that if IceHub disappeared then IceBucket could expand to serve the same institutions without losing the record of the dataset dependency network.\n\nA major high-quality platform can still exist - this just allows other platforms to potentially also exist, whilst being connected to the same network. Such decentralization combined with open data formats and data ownership should allow the development of such a high-quality platform whilst minimizing risk of data or network lock-in. It would also aid the switch to whatever future scientific infrastructure we realise we need after this platform, assuming we’re all still around by then.\n\nTo be clear, of all the requirements, this last one is the only one that I’m not actually sure is even technically possible. It would be \n\nawesome though, so I would love it if someone who knows more than I about networking protocols could weigh in. \n\nHere’s a place for brainstorming ideas.","type":"content","url":"/blog/2025/science-needs-a-social-network#what-would-the-ideal-data-sharing-service-look-like","position":15},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Surely this exists already?","lvl2":"Science needs a social network for sharing big data"},"type":"lvl3","url":"/blog/2025/science-needs-a-social-network#surely-this-exists-already","position":16},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Surely this exists already?","lvl2":"Science needs a social network for sharing big data"},"content":"Whilst various initiatives (federal, non-profit, and for-profit) aim to solve some of these problems, as far as I am aware nothing currently exists which meets all of the criteria above simultaneously. Let’s briefly discuss where major existing efforts fall short.\n\nNot even trying - the vast majority of scientific data is only available “upon request”, which is to say, not openly available at all. Given that this data was created with public money, and the public would benefit from it being openly accessible, this is unacceptable.\n\nRolling-their-own catalogs and data portals is currently most large institutions’ approach (e.g. \n\n[1] \n\n[2] \n\n[3] \n\n[4] \n\n[5] \n\n[6]), if they bother with a unified data catalog at all. This leads to massive waste through duplication of effort and difficulty finding relevant data outside of that one institution, only to end up with a mess of services which don’t generalize, interoperate, or even necessarily work very well. These existing services also are not open to anyone - they very much care \n\nwho you are - which limits the practice of data-driven science to a \n\nsubset of society. Worse, many institutions such as universities and national labs actively make it harder to access data instead of easier, by requiring strict affiliation and access controls to be permitted behind the walls of their “data fortress”, then making it very hard to free the data even once you have access.\n\nSTAC, which stands for Spatio-Temporal Asset Catalog, is an open specification providing a common structure for describing and cataloging geospatial datasets. STAC is extremely cool, but it isn’t quite what we need. Firstly, the data model is not general enough - it’s intimately tied to \n\ngeospatial-specific concepts. There is plenty of scientific data that is multidimensional arrays but nothing to do with the earth’s surface, e.g. \n\nstatistical genetics data and \n\nmicroscope imaging data. Secondly, because STAC’s core is a static catalog and API specification, not a network protocol, it’s not designed to faciliate dynamic updates or global connectivity. (This is different to STAC’s idea of a \n\n“Dynamic Catalog”, which is better understood as a static catalog created on-demand.) Some groups have built tooling on top to provide a \n\nunified STAC catalog, a \n\nsearchable index and \n\npublish update notifications, but they must step outside the STAC specification to do so and hence lose some of the decentralization. Nevertheless this is all great inspiration, and a general cataloging protocol should play nicely with STAC in an extensible way, as the core spec is a perfect example of the domain-specific metadata conventions that each field of science should organize amongst themselves.\n\nIntake is another open-source data catalog project from within the scientific python ecosystem. An Intake Catalog is in theory a little bit like a STAC catalog, but more general. Unfortunately Intake also tries to solve several other problems at the same time, including data loading, transformation, and processing. The result is that the \n\ncatalog definitions are python-specific, and hence not interoperable.\n\nZenodo and \n\nDataverse are science-oriented hosting services. Whilst great initiatives, they aren’t designed to share big data or to exploit common scientific data models. For example Zenodo has a max dataset size of 50GB, whilst crucial climate science datasets can be many PetaBytes in size - at least 5 orders of magnitude larger! A service which doesn’t take advantage of the fact that the vast majority of scientific data is either multidimensional arrays or tabular (i.e. they treat all data as unstructured) from a user perspective is not really that different from just putting raw binary files into GitHub - i.e. the repository doesn’t understand that your data has any useful common structure. The funding model of these services also appears to be entirely based on public grants. (I must say Dataverse is pretty impressive, and perhaps the closest existing design I’ve seen to being capable of hitting all the technical requirements on this list.)\n\nGitHub being mis-used as a file sharing service does fulfil several of these properties, including being sustainably-funded. But again it’s not Scalable nor does it use Common Data Models, and it can’t really be made so (because GitHub and git itself were designed for something completely different: handling small text files containing source code). It’s also not decentralized - whilst \n\nGitHub’s package dependency graph is publicly available, it arguably does not fulfil the “Credible Exit” criteria according to BlueSky’s definition, because you cannot simply switch over to another provider like GitLab whilst retaining your network connections.\n\nWikipedia is another existing example of a commons, a knowledge commons. It is fully open-source, unified, open, federated, and sustainably-funded. It also hosts data through \n\nWikiData, so could we just put our scientific data there? Unfortunately whilst the size of the work to create and maintain Wikipedia is incredible, the actual amount of data stored is smaller than you might think. Once compressed, the size of all the English text on Wikipedia is a miniscule \n\n24 GB! Even downloading all the images and so on only requires \n\nabout 25TB - that fits on one big external hard drive. Again this is not the required scale of data hosting, which would require a completely different architecture (i.e. a cloud-native one). Also note that the majority of funding comes from \n\nindividual public donations (I donate - you should too). That’s awesome, but doesn’t seem like an approach that would work for a scientific data platform invisible to most of the public.\n\nSnowflake is a commercial product: a proprietary cloud data platform that’s widely used by the private sector. It’s extremely scalable as it’s designed for big (business) data, and sustainably-funded in the sense of being very profitable. However it doesn’t recognise anything other than Tabular data, because almost all business data is Tabular. A platform set up to manage companies’ private data also has little need for a public global data catalog, as unlike scientists, businesses don’t tend to openly share data with one another.\n\nHugging Face Hub is another platform from another private company, but for sharing Machine Learning training datasets and model weights. It works well for that community, but again it is not truly Scalable (because your data has to be uploaded to \n\ntheir storage) nor does it use a Common Data Model (though \n\nit could), and is ML-specific. \n\nDagsHub is very similar, except that it lets you connect your own storage bucket, and has version-control. However, the version-control system assumes unstructured data. It’s also worth noting that the stupid amounts of money being poured into the ML sphere incentivises the founding of multiple companies trying to provide data-sharing services, whereas in science there are far fewer, even though there is no fundamental difference between ML “tensor” model weights and scientific multidimensional array data.\n\nSource Cooperative is a non-profit data-sharing utility that basically wraps cloud storage buckets provided by the \n\nAWS Open data program, so is Free to Browse and Scalable to large datasets. In theory Source could evolve to meet many of the criteria above, but note that one big missing requirement that would be hard to add later is Common Data Models - Source is essentially just a catalog of buckets, each full of potentially inconsistent and unrelated objects. For now it’s still in beta, so has few features, is effectively invite-only, and does not yet have a clear sustainable funding mechanism in place.\n\nGoogle Earth Engine is scalable, and does understand the raster data model (2D arrays of geospatial imagery). However, it’s geoscience-specific, doesn’t respect Data Ownership and is completely Centralized - as all the data lives on Google machines. Crucially it’s also not sustainably-funded - Google runs it at a loss, and have only recently started trying to monetize it. Nevertheless Earth Engine is widely used, meaning that an alarming number of environmental non-profits completely depend on the charity of a big corporation, who have no obvious reason beyond PR to keep funding the service for free, and could \n\npull the plug at literally any time.\n\nEDIT: Every time I show someone new this post they suggest another hosting service that I hadn’t heard of. So far none of them meet all the 13 criteria above, and most of them fall down on Scalability or Common data models.","type":"content","url":"/blog/2025/science-needs-a-social-network#surely-this-exists-already","position":17},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Conclusion","lvl2":"Science needs a social network for sharing big data"},"type":"lvl3","url":"/blog/2025/science-needs-a-social-network#conclusion","position":18},{"hierarchy":{"lvl1":"Science needs a social network for sharing big data","lvl3":"Conclusion","lvl2":"Science needs a social network for sharing big data"},"content":"There’s a major missing link in the way the world shares scientific data. Until we build it we cannot reasonably expect results from data-intensive science to be truly Findable, Accessible, Interoperable, or Reproducible. Though still a major effort, luckily cloud computing and recent advances in key open-source software libraries bring the idea within reach.","type":"content","url":"/blog/2025/science-needs-a-social-network#conclusion","position":19},{"hierarchy":{"lvl1":"Hi, I’m Tom Nicholas 👋"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Hi, I’m Tom Nicholas 👋"},"content":"\n\nA bit about me...\n\n💼 Engineer @ \n\nEarthmover\n\n🌎 Active member of the \n\nPangeo Community\n\n🌕 Core developer of \n\nXarray\n\n🧪 PhD in fusion plasma physics from \n\nUni. York & \n\nCCFE\n\nI also work on several other \n\nopen-source projects for science.\n\nAbout me ℹ️ :::{grid-item-card}\n:link: projects.md\nProjects I've worked on 🔧\n::: \n\nMy blog ✍️","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Hi, I’m Tom Nicholas 👋","lvl2":"Recent blog posts"},"type":"lvl2","url":"/#recent-blog-posts","position":2},{"hierarchy":{"lvl1":"Hi, I’m Tom Nicholas 👋","lvl2":"Recent blog posts"},"content":"Fundamentals: What is Cloud-Optimized Scientific Data?\n\nThe article I wish I could have read back when I first heard of Zarr in 2018. Explains how object storage and conventional filesystems are different, and the key properties that make Zarr work so well in cloud object storage.\n\n\nDate: April 17, 2025 | Author: Tom Nicholas | Tags: cloud, zarr, netcdf, earthmover, open-science\n\nScience needs a social network for sharing big data\n\nImagine being able to visit one website, search for any scientific dataset from any institution in the world, preview it, then stream the data out at high speed, in the format you prefer. We have the technology - here's what we should build.\n\n\nDate: January 18, 2025 | Author: Tom Nicholas | Tags: open-science, frost\n\nXarray x NASA: xarray.DataTree for hierarchical data structures\n\nHow xarray's new DataTree feature came about, and thoughts on how public agencies can support the open-source scientific software that they depend on.\n\n\nDate: December 19, 2024 | Author: Tom Nicholas | Tags: code, python, xarray, open-science\n\nCubed: Bounded-memory serverless array processing in xarray\n\nCubed was designed to address the main problems with Dask, so I integrated it with Xarray. \n\n\nDate: June 01, 2023 | Author: Tom Nicholas | Tags: code, python, xarray, dask, cubed, open-science\n\nDask.distributed and Pangeo: Better performance for everyone thanks to science / software collaboration\n\nDask's distributed scheduler algorithm got a major fix after we tested its' limits on a huge oceanography analysis problem.\n\n\nDate: January 04, 2023 | Author: Tom Nicholas | Tags: code, python, dask, open-science","type":"content","url":"/#recent-blog-posts","position":3},{"hierarchy":{"lvl1":"Talks"},"type":"lvl1","url":"/talks","position":0},{"hierarchy":{"lvl1":"Talks"},"content":"Below are a few highlighted talks that I have given recently.\n\nFROST: Federated Registry Of Scientific Things\n\nPangeo Showcase February 2025\n\nVirtualiZarr: Create virtual zarr stores using xarray syntax\n\nPangeo Showcase March 2024\n\nWhat’s next for Pangeo?\n\nPangeo Showcase December 2023\n\nCubed: Bounded-Memory Serverless Array Processing in Xarray\n\nPangeo Showcase November 2023\n\nXarray-Datatree: Hierarchical Data Structures for Multi-Model Science\n\nPangeo Showcase February 2023\n\nEnabling Petabyte-scale Ocean Data Analytics with xGCM\n\nSciPy Conference 2022","type":"content","url":"/talks","position":1}]}